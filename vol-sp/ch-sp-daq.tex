\chapter{Data Acquisition}
\label{ch:sp-daq}

\fixme{Fix SI units throughout, and use consistent dword
  references. Brett}
\fixme{Add missing references. Georgia}
\fixme{Add a couple of figures on co-processor. Georgia}

\section{Introduction}
\label{sec:fd-daq:introduction}

The \dword{fd} \dword{daq} system is responsible for receiving,
processing, and recording data from the \dword{dune} \dword{fd}.
It provides
timing and synchronization for all \dwords{detmodule} and
subdetectors; receives, synchronizes, compresses, and buffers data
streaming from the subdetectors; extracts information from the data at a
local level to subsequently make local, module, and cross-module data
selection decisions; builds event records %``events''
 from selected space-time volumes 
and relays them to permanent storage; and carries out local data
reduction and filtering of the data as needed.

This chapter provides a description for the design of the \dword{dune}
\dword{fd} \dword{daq} system developed by the \dword{dune} \dword{fd}
\dword{daq} consortium. 
This consortium brings together resources and expertise from CERN,
Colombia, Czech Republic, France, Italy, Japan, the Netherlands, the UK, and the USA. 
Its members bring considerable experience from ICARUS, MicroBooNE,
SBND, and
DUNE prototype \dwords{lartpc}, as well as from \dword{atlas} at the LHC and other major
HEP experiments across the world.

The system is designed to service all \dword{fd} \dword{detmodule} designs interchangeably. 
However, some aspects of the \dword{daq} design described in this chapter are tailored to meet the specific needs of the single-phase detector module technology. 
Adaptations to detector technology are implemented in the upstream part of the DAQ leaving the remainder generic.
The individual modules are serviced essentially independently and only loosely coupled though a cross-module triggering mechanism.

The chapter begins with an overview of the \dword{daq} design (Section~\ref{sec:fd-daq:overview}),
including requirements that the design must meet, and specification of
interfaces between the \dword{daq}  and other \dword{dune} \dword{fd} systems. 
Subsequently, Section~\ref{sec:fd-daq:design}, which comprises the
bulk of this chapter, describes the design of the \dword{fd}
\dword{daq} in greater detail.
Section~\ref{sec:fd-daq:validation} describes design validation efforts
to date, and future design development and validation plans. At the
center of these efforts is the 
\dword{protodune} \dword{daq} system (described in Section~\ref{sec:fd-daq:protodune}), which has served as a demonstrator of several
key aspects of the  \dword{dune}  \dword{daq}  design, and continues to serve as a
platform for further design development and validation toward the
DUNE \dword{daq} design. 
Finally, the chapter finishes with two sections
(Sections~\ref{sec:sp-daq:production} and \ref{sec:sp-daq:organization}) providing details on
the management of the
\dword{daq} project, including schedule to completion of the design, 
production, and installation of the system, as well as cost, resources, and
safety considerations.

\section{Design Overview}
\label{sec:fd-daq:overview}

An overview of the \dword{dune} \dword{fd} \dword{daq} system 
servicing a single \dword{fd}
\dword{detmodule} is
provided in Figure~\ref{fig:fd-daq:layout}. The system is
physically located at the \dword{fd} site, and it is split between the
underground DUNE caverns and the surface level at \dword{surf}. Specifically, the \dword{daq} occupies space and
power both in the underground \dword{cuc} and the above ground \dword{mcr}.
The upstream part of the system, which is responsible for
raw detector data reception, buffering, and pre-processing reside in the \dword{cuc}.
The \dword{daqbes},
which is responsible for
event-building, as well as run control and monitoring, resides on the
surface.
Data flows through the \dword{daq} from 
upstream to the back-end and then to offline. The majority
of raw data processing and buffering is performed underground, in the
upstream \dword{daq}, thus controlling consumption of available data bandwidth  to the surface. A
hierarchical data selection subsystem consumes minimally-processed
information from the upstream \dword{daq}, and through further data processing, 
constructs module-level trigger decisions leading to a trigger command.
The command is executed by a \dword{daqdfo} residing in the \dword{daqbes}
by retrieving the required data from memory buffers maintained by the upstream DAQ.
The results are aggregated across the detector module into a cohesive record and saved to non-volatile storage.
During or after the aggregation, an optional down-selection of the data is possible via
high level filtering.
Finally, the data is  transferred offsite and archived by the DUNE offline group.
All
detector modules and their subcomponents are synchronized and timed against a global,
common clock, provided by the timing and synchronization
subsystem. Cross-module communication as well as communication
to the outside world for data selection purposes is facilitated
through the external trigger interface. The
specifics of design implementation and data flow are described in Section~\ref{sec:fd-daq:design}.

\begin{dunefigure}[DAQ conceptual data flow for one module]{fig:fd-daq:layout}{\dword{daq} conceptual data flow focusing on a single \nominalmodsize module. Included are the upstream \dword{daq} (orange), \dfirst{daqdsn} (blue), \dfirst{daqbes} which includes the \dfirst{daqdfo}, \dfirst{eb} and storage buffer (yellow).
    Further, there is the subsystem for timing and synchronization (gray) and the subsystem for control, configuration and management (brown).
  }
  \includegraphics[width=0.8\textwidth,trim=4cm 3cm 4cm 3cm]{daq-layout-2.pdf}
\end{dunefigure}


\subsection{Requirements and Specifications}
\label{sec:fd-daq:requirements}

The \dword{dune} \dword{fd} \dword{daq} system is designed to meet the
\dword{dune} top-level as well as \dword{daq}-level requirements
summarized in Table~\ref{tab:specs:SP-DAQ}. The \dword{daq}-level requirements are
imposed to ensure that the 
system %is capable of 
can record all necessary information for offline 
analysis of data that is associated with on- and off-beam physics events, as directed
by the \dword{dune} physics mission, and with minimal compromise to
\dword{dune}'s physics sensitivity. The requirements must be met by following the 
specifications provided in the same table.
Those specifications are
associated with trigger functionality, readout,
and operations and are motivated further in the following subsections.

\subsubsection{How DUNE's Physics Mission Drives the DAQ Design}

The DUNE Far Detector has three main physics drivers: neutrino \dword{cpv} and related
long baseline oscillation studies using the high intensity beam provided
by \fnal, off-beam measurements of atmospheric neutrinos and searches
for rare processes such baryon-number-violating decays,
and detection of neutrinos from a \dfirst{snb} occurring within our galaxy. The
\dword{dune} \dword{fd} \dword{daq} system must facilitate data
readout for delivering on these main physics drivers, while keeping
within physical (space, power) and resource constraints for
the system. In particular, the off-beam measurements require the
continuous readout of the detector, and the lack of external triggers for such
events requires real-time or online data processing, and
self-triggering capabilities. Since the
continuous raw data rate of the far detector module, as received by
the \dword{daq} system, reaches multiple
terabits per second, significant data buffering and processing
resources are needed as part of the design.

The \dword{dune} \dword{fd} modules employ two active detector components from which the \dword{daq} system must acquire data: the \dfirst{tpc} and the \dfirst{pds}. The two components access the physics by sensing and collecting signals associated with very different sensing time scales.

Ionization charge measurement by the \dword{tpc} for any given activity in the detector requires a nominal recording of data over a time window of order \SIrange{1}{10}{\milli\second}. 
This time scale is determined by the ionization electron drift speed in \lar and the detector dimension along the drift direction, and is nominally set to \spreadout, corresponding to 2.4$\times$\SI{2.25}{\milli\second}.
The latter (\SI{2.25}{\milli\second}) assumes DUNE's nominal drift electric field of 500~V/cm.
This assures the capture of ionization information from at least a full drift before and after a trigger.
Given the ability to  immediately record a subsequent readout, early commissioning data will be used to evaluate and optimize this nominal readout time.

On the other hand, the \dword{pds} measures argon scintillation light emission, which
occurs and is detected over a timescale of multiple nanoseconds to
microseconds for
any given event and/or subsequent subevent process. Unlike the \dword{tpc},
the \dword{pds} data is zero-suppressed in
the \dword{pds} electronics (see Chapter~\ref{ch:fdsp-pd}); therefore the total raw data volume received by
the \dword{daq} system is expected to be dominated by
the \dword{tpc} data, which is sent out from the front-end electronics as a continuous stream.
 
Figure \ref{fig:daq-rates} provides the expected activity rates in a
single far detector module as a function of true energy associated
with given types of signal.
At low energy ($<$10 MeV), activity is dominated by radiological backgrounds
intrinsic to the detector, and
low-energy solar neutrino interactions. Supernova burst neutrinos,
expected to arrive at a galactic \dword{snb} rate of once per century, 
would span the 10-30 MeV range. At higher energies (generally
above 100 MeV), rates are dominated by cosmic rays, beam neutrino interactions,
and atmospheric neutrino interactions. With the exception of supernova
burst neutrinos, the activity associated with any of these physics
signals is localized in space and particularly in time. Supernova burst
neutrinos on the other hand are characteristically different, as they arrive as multiple
signals of localized activity that extends over the entirety of the
detector and over multiple seconds. %The arrival time and energy
%distribution of
%neutrinos from a \dword{snb} is model-dependent; as such, the DAQ must
%utilize model-independent approach in selecting \dword{snb} activity.

The nature and rates of these signatures necessitates a data selection strategy that handles two
distinct cases: a localized high-energy activity trigger, prompting an event record readout for
activity associated with a minimum of 100 MeV of deposited energy; and an extended
low-energy activity trigger, prompting an event record readout when
multiple localized low-energy activity candidates with low deposited energy each are found over a short (less than 10
seconds) time period and over the entirety of a \nominalmodsize 
module. Because of the high granularity of the detector readout elements, a
hierarchical data selection subsystem is employed to provide data processing and
triggering, and facilitate optional data reduction and filtering. The
\dword{daq}  
system is required to yield $>$99\% efficiency for particles
depositing $>$100 MeV of energy in the detector, for localized
high-energy triggers; it is also required to yield sufficient efficiency for low-energy
deposition in the detector as needed to yield $>$90\% galactic
supernova burst trigger coverage, for extended low-energy triggers. Galactic coverage is defined as
the supernova burst trigger efficiency, weighted by the probability of
a galactic \dword{snb} (which is \dword{snb} distance-dependent).

By offline considerations, the \dword{daq} is required to reduce the data volume for offline permanent storage to  \offsitepbpy.
A far detector composed of four single-phase modules employing a full-detector, \spreadout readout strategy will be limited by this to an average readout rate of \SI{0.3}{\hertz}. 
Strategies will be developed during commissioning and early running that will limit the readout to some subset of the detector module and these are expected to allow an increase in this rate limit by about an order of magnitude.
The instantaneous readout rate can be much higher, for example to accommodate calibrations.
For planning purposes, the DAQ will allot an average rate of \dword{snb} triggers of 1 per month.
Given current understanding of \dword{snb} rates and the $>$90\% expected efficiency for \dword{snb} within \SI{100}{\kilo\parsec}, the vast majority of such triggers will be due to fluctuations of low energy radiological backgrounds and potentially of excess noise.
Such triggers will prompt \snbtime of data from the entire module to be read out.
At this average rate and if saved to offline storage, the \dword{snb} triggers will lead to production of 1.8 PB/year uncompressed from one single-phase module. There is nevertheless no requirement to permanently store fake \dword{snb} data. 

The capability of recording data losslessly is built into the design as a conservative measure; a particular concern is charge collection efficiency in the case of zero suppression.
MicroBooNE is currently investigating the impact of zero suppression on reconstruction efficiency and energy resolution for low-energy events.
Expected data rates from physics signals of interest, which fit the 30 PB yearly generated volume and trigger rate requirements, are summarized in Table~\ref{tab:sp-daq:rates} and detailed in~\citedocdb{9240} and potential bottlenecks were analyzed in~\citedocdb{11461}.

\begin{dunefigure}[Expected physics-related activity
    rates in one FD module]{fig:daq-rates}{Expected physics-related activity
    rates in a single \nominalmodsize module. \label{sec:fd-daq:rates}
}
  \includegraphics[width=0.7\textwidth,clip,trim=6cm 6cm 10cm 2cm]{daq-event-type-rates-vs-energy.pdf}
\end{dunefigure}

\begin{dunetable} [Expected DAQ Yearly Data Rates] {p{0.3\textwidth}p{0.1\textwidth}p{0.5\textwidth}} {tab:sp-daq:rates}{Summary of expected data rates for initial single-module running.
    The rates assume no compression, and are given for a single \SI{10}{\kilo\tonne} module. Trigger primitives (see Section~\ref{sec:sp-daq:design-data-selection}) are not kept permanently; they are temporarily stored for 1-2 months at a time. The same applies to fake \dword{snb} data. Improved readout algorithms will be developed and evaluated with the initial data and are expected to provide about an order of magnitude reduction in data while retaining efficiency.}
Source  & Annual Data Volume & Assumptions \\\toprowrule
Beam interactions & 27 TB & 10 MeV threshold in coincidence with beam
time, including cosmic coincidence; \spreadout readout \\\colhline
Cosmics and atmospheric neutrinos & 10 PB & \spreadout readout \\\colhline
Radiological backgrounds & $<2$ PB & $<1$ per month fake rate for SNB
trigger\\\colhline
Cold electronics calibration & 4 TB & scaled from \dword{pdsp} experience \\\colhline
Radioactive source calibration & 100 TB & $<10$ Hz source rate; single
APA readout; \spreadout readout \\\colhline
Laser calibration & 200 TB & 10$^6$ total laser pulses; half the
TPC channels illuminated per pulse; lossy
compression (zero-suppression) on all channels\\\colhline
Random triggers & 60 TB & 45 per day\\\colhline
Trigger primitives & $13$ PB & Dominated by $^{39}$Ar (50~kHz per APA face); collection
channels only; 20 bytes per trigger primitive \\\colhline
\end{dunetable}

Self-triggering on \dword{snb} activity is a unique challenge for the
DUNE \dword{fd}, and an aspect of the design which has never been demonstrated
in a \dword{lartpc}. The challenge with \dword{snb} triggering is two-fold. 
First, the activity of the individual \dword{snb} neutrino interactions
is expected to be of relatively low energy (\SIrange{10}{30}{\mega\electronvolt}),
sometimes indistinguishable from pile-up of radiological background activity in the
detector.  Triggering on an ensemble of \bigo{100} events expected on
average in the case of a galactic supernova burst is therefore
advantageous; however, since this ensemble of events is expected to occur sparsely over the
entire detector and over an extended period of \bigo{10}\si{s},
sufficient buffering capability must be designed into the system to
capture the corresponding signals. 
Furthermore, to ensure high efficiency in collecting \dword{snb} interactions
that, individually, are below low-energy activity threshold, data from
all channels in the detector will be recorded over an extended and contiguous period of
time \bigo{100}\si{s} around every \dword{snb} trigger.

\input{generated/req-longtable-SP-DAQ} 

\subsubsection{Considerations for Design}
\label{sec:fd-daq:considerations}

The \dword{daq} system is designed as a single, scalable system that can service
all \dword{fd} modules. It is also designed on the principle that the system should be
able to record and store full detector data with zero dead time, applying appropriate data reduction through data selection and compression; and
that it should be evolutionary, taking advantage of the staged
construction for the DUNE \dword{fd}, and thus beginning very conservatively
for the first DUNE \dword{fd} module, and agressively reducing the design
conservatism as further experience is gained with detector
operations. At the same time, it is designed to preserve the possibility of additional capacity
as required. The bulk of processing and buffering of raw detector data is
done underground, in the upstream \dword{daq} part of the system (see Figure~\ref{fig:fd-daq:layout}), in order to
control the consumption of available network bandwidth to surface.

Power, cooling, and space constraints in the \dword{cuc} are limited to \daqpower and \daqracks (out of \cucracks total) for DAQ for all four \dword{fd} modules.
The underground computing required for the DAQ to service the SP detector module is expected to require less than a quarter of the total power and rack space provided for all four detector modules. 
The servers for FELIX, trigger, various services, networking and the timing system and including spares is expected to consume less than \SI{63}{\kilo\watt} and fill less than 300U of rack space.

%%% numbers from Alessandro
% - 85 felix servers , 350 W, 2 APAs/host, 2Us - including spares
% - 80 trigger servers, 300 W, 1 U
% - 25 servers for MLT, CCM, others, 200 W, 1 U
% - 18 network units, 150 W, 1 U
% - Timing system, 1000 W





There are four key challenges for the DUNE \dword{fd} \dword{daq} system: 
\begin{itemize}
\item First, the high overall experiment's uptime goal, requires the \dword{daq} to be designed with stringent reliability, fault tolerance and redundancy criteria, aiming at contributing in a negligible way to the overall downtime.

  The \dword{daq} system is fully configurable, controllable, and operable from remote locations, with authentication and authorisation implemented to allow exclusive control. The \dword{daq} provides monitoring of the quality of the detector data and of its own operational status, as well as automated error detection and recovery capabilities.

\item Secondly, the system must be able to evolve to accommodate newly commissioned sub-components as they are installed into a detector module which is under construction. 
  The DAQ must also continue to service existing modules which have reached their operational status while simultaneously accommodating subsequent detector modules as they are installed and commissioned. 
  To support this ongoing variability the DAQ will support operating as multiple independent instances or partitions.

  Patitioning will be supported also within a single detector module, in order to support e.g. special calibration or debugging runs of parts of the detector, which are incompatible with physics data taking settings, while the rest of the detector remains in physics data taking mode. Partitioning, i.e. allowing multiple instances of the DAQ to operate indipendently with different configurations on different parts of the detector, will also be important during the installation and commissioning phases, to parallelize experts work e.g. for photon detectors and TPC.

\item Thirdly, the \dword{snb}  physics
requirements necessitate large buffering in the upstream \dword{daq} and low fake
supernova burst trigger rates. 

The implementation of a buffer element in the upstream DAQ allows for the formation and capture of delayed, data-driven trigger decisions with no loss of physics information.
The specification for the depth of this buffer is determined in consultation with physics groups and is driven primarily by the need to record up to ten seconds of unbiased data preceding a \dword{snb} trigger.
%% Why says this?  it has nothing to do with buffering, does it?(bv) %%
%This aspect remains to be validated with simulation, to ensure that high coverage (greater than 90\%) for galactic \dwords{snb} is achieved by the \dword{snb} trigger.
Collecting data containing information on other types of interactions and decays do not pose additional requirements on the upstream \dword{daq} buffer since the latency to form the required triggers is expected to be well below 10 seconds.

\item Finally, the \dword{daq} needs to reduce the data volume of data to be permanently stored offline to a maximum of 30 PB/year.
The \dword{daq} system is designed so as to be able to select interesting time windows in which activity was detected, apply lossless compression to data records, as well as filter them to remove unnecessary data regions in an intelligent way, i.e.~without compromising physics performance.

A programmable trigger priority scheme ensures that the readout for the main physics triggers is never or rarely inhibited so as to enable easy determination of the live-time of these triggers. 


\end{itemize}

% \subsection{Delete this subsection, kept now to retain section numbering used by reviewers}
% %\label{sec:sp-daq:parameters}
% \fixme{Delete this section heading (keep content) once we've responded to reviewers comments.}

Table~\ref{tab:sp-daq:parameters} summarizes the important parameters driving the \dword{daq} design. These parameters set the scale of data buffering, processing, and transferring resources which must be built into the design of each \dword{fd} module. 

\begin{dunetable}
[Key DAQ Parameters]
{ll}
{tab:sp-daq:parameters}
{Summary of important parameters driving the DAQ design.}
Parameter & Value \\ \toprowrule
TPC Channel Count per Module & 384,000\\ \colhline
TPC Collection Channel Count per Subdetector (APA) & 960\\ \colhline
TPC Induction Channel Count per Subdetector (APA) & 1,600\\ \colhline
PDS Channel Count per Module & TBD\\ \colhline
PDS Channel Count per Subdetector & TBD\\ \colhline
TPC \dword{adc} Sampling Rate & 2 MHz\\ \colhline
TPC \dword{adc} Dynamic Range& 12 bits\\ \colhline
PDS \dword{adc} Sampling Rate & TBD \\ \colhline
PDS \dword{adc} Dynamic Range & TBD \\ \colhline
PDS \dword{adc} Readout Length & TBD \\ \colhline
Localized Event Record Window & \spreadout \\  \colhline
Extended Event Record Window &  100 s\\  \colhline
Full size of TPC Localized Event Record per Module & 6.22 GB \\  \colhline
Full size of TPC Extended Event Record per Module & 115 TB\\  \colhline
Full size of PDS Localized Event Record per Module & TBD \\  \colhline
Full size of PDS Extended Event Record per Module & TBD \\  \colhline
\end{dunetable}


\subsection{Interfaces}
\label{sec:sp-daq:interfaces}

The \dword{daq} system scope begins at the optical fibers streaming raw digital data from the detector active components
(\dword{tpc} and \dword{pds}), and ends at a wide area network (WAN) interface that
distributes the data from on site at \surf to offline centers off
site. The \dword{daq} also provides common computing and network services for
other \dword{dune} systems, although slow control and safety functions
fall outside \dword{daq} scope.

Consequently, the \dword{dune} \dword{fd} \dword{daq} system interfaces with the \dword{tpc} \dword{ce}, \dword{pds}
readout, computing, \dword{cisc}, and calibration systems of the %\dword{dune}
\dword{fd}, as well as with facilities and underground installation. The
 interface agreements with the \dword{fd} systems 
are listed in Table~\ref{tab:sp-daq:interfaces}, and described
briefly in the following subsections. Interface agreements with
facilities and underground installation are described in Section~\ref{sec:sp-daq:production}.

\fixme{Add summaries in table. Georgia}
\begin{dunetable}
[DAQ System Interface Links]
{p{0.3\textwidth}p{0.3\textwidth}p{0.2\textwidth}}
{tab:sp-daq:interfaces}
{Data Acquisition System Interface Links }
Interfacing System & Description & Reference \\ \toprowrule
TPC CE & Data rate and format, number and type of links, timing, inherent noise & \citedocdb{6742}{v6}\\ \colhline
PDS Readout & Data rate and format, number and type of links, timing &  \citedocdb{6727}{v2} \\ \colhline
Computing & Off-site data transfer rates, methods, data file content, disk buffer, software development and maintenance &  \citedocdb{7123} \\ \colhline
CISC & Information exchange, hardware and software for rack and server monitoring & \citedocdb{6790}{v1} \\ \colhline
Calibration & Constraint on total volume of the calibration data; trigger and timing distribution from the DAQ & \citedocdb{7069} \\ \colhline
Timing and Synchronization & Clients, clock frequency, protocols, transports, accuracy, monitoring &  \citedocdb{11224} \\ \colhline
Facilities & Detector integration, coordination, cables, racks, safety, conventional facilities, lack of impact on cryo and DSS &  \citedocdb{6988}{v1} \\ \colhline
Installation & Prototyping, planning, transport, underground equipment and activity, safety & \citedocdb{7015} \\ \colhline
\end{dunetable}

\begin{description}
\item[\dword{tpc} \dword{ce}] The \dword{daq} and \dword{tpc} \dword{ce} interface is described in
\citedocdb{6742}. The physical interface is at the \dword{cuc}, where optical links from the \dwords{wib} transfer
the raw \dword{tpc} data to the \dword{daq} \dword{fe} readout (\dword{felix}; see
Section~\ref{sec:daq:design-upstream}). This ensures the \dword{daq} is electically decoupled from the detector
cryostat. Ten \SI{10}{Gbps} links are expected per \dword{apa}, and have
been specified as 300m OM4 multi-mode fibers from \dword{sfp}+ at the \dword{wib} to
\dword{minipod} on \dword{felix}. The data format has been specified
to use a custom communication protocol and no
compression.

\item[\dword{pds} Readout] The \dword{daq} and \dword{pds} readout interface is described in
\citedocdb{6727}. It is anticipated to
be of the form of 150  \SI{10}{Gbps} OM4 fibers from one \dword{fd} module. 
This
is similar to the interface to the \dword{tpc} \dword{ce}, except the overall
data volume is lower by an order of magnitude. The data format has been specified to use
compression (zero supression) and a custom communication protocol.

\item[Computing] The \dword{daq} and computing interface is described in \citedocdb{7123}.
 The computing consortium
 is responsible for the online areas of WAN connection between \surf and
\fnal providing on order \SI{100}{Gbps} bandwidth, while the \dword{daq} consortium is responsible for disk buffering
to handle any temporary WAN disconnects and the infrastructure needed
for real-time data quality monitoring.  The computing consortium 
is also
responsible for the offline development and operation of the tools for data
transfers to \fnal. The primary
constraint in defining the \dword{daq} and offline computing interface is the
requirement to produce less than \SI{30}{PB/year} 
into final storage at
\fnal. \dword{daq} and
computing consortia are jointly responsible for data
format definition and data access libraries, as well as real-time data
quality monitoring software. The former is specified in the form of a 
data model documented in \citedocdb{7123}.

\item[CISC] The \dword{daq} and \dword{cisc} interface is described in
\citedocdb{6790}. The \dword{daq} provides a network in the \dword{cuc} for \dword{cisc},  operation information and hardware
monitoring information to \dword{cisc}, and power distribution and
rack status units in \dword{daq} racks. The information from \dword{cisc}
feeds back into the \dword{daq} for run control operations.

\item[Calibration] The \dword{daq} and calibration interface is described in \citedocdb{7069}. Two calibration systems are envisioned for the \dword{fd}: a laser calibration system and a neutron generator. 
  Two way communication between the calibration system and the DAQ is needed.  In the first, the calibration system must notify the trigger decision process when activity has been induced in the detector.  In the second, the DAQ must provide input to the calibration system so that it may avoid inducing activity in the detector during certain periods such as during a \dword{snb} readout time or during a beam spill.
This second communication will be initiated by the data selection system and distributed through the
\dword{daq} timing system.


\item[Timing and Synchronizationm] The timing system of the \dword{dune} \dword{fd} connects with almost all detector systems and with the calibration system and has a uniform interface to each of them. 
  A single interface document, \citedocdb{11224}, describes all these timing interfaces. 

Accuracy of timestamps delivered to  detector endpoints will be $\pm$\SI{500}{\nano\second} with respect to UTC.  Synchronization between any two endpoints in the detector will be better than \SI{10}{\nano\second} on average.   Between detector modules, synchronization will be better than \SI{25}{\nano\second} on average.  
\end{description}

\section{Data Acquisition System Design}
\label{sec:fd-daq:design}

This section begins with an overview of the \dword{daq}
design followed by
descriptions of the subsystem designs and implementation specifics.

\subsection{Overview}
\label{sec:fd-daq:design-overview}

The  \dword{daq} system is composed of six distinct subsystems:
%
(1) upstream \dword{daq} (Section~\ref{sec:daq:design-upstream}),
%
(2) \dfirst{daqdsn} (Section~\ref{sec:sp-daq:design-data-selection}),
%
(3) \dfirst{daqbes} (Section~\ref{sec:fd-daq:design-backend}), 
%
(4) \dfirst{daqccm} (Section~\ref{sec:fd-daq:design-run-control}), and
%
(5) \dfirst{daqtss} (Section~\ref{sec:sp-daq:design-timing}).
%
The physical extent
the first five of the \dword{daq} subsystems
can be specified in reference to Figure~\ref{fig:fd-daq:layout}: the
upstream \dword{daq} and \dword{daqtss}  live underground in the \dword{cuc}; \dword{daqdsn} occupies
both underground and above-ground spaces; \dword{daqbes} is above-ground
and includes data flow orchestration, event building and buffering, before distribution of data
to offline; and \dword{daqccm} extends throughout the entire physical layout of the
system, supported on a private network throughout the \dword{daq} system. Each of these subsystems is described in further
detail in the following subsections. 

Front-end readout is carried out by the upstream \dword{daq}, using custom data receiver and
co-processing \dword{fpga}/CPU hardware, all of which is hosted in of order 100 servers in the \dword{cuc}. A
similar number of additional servers is responsible for the execution
of subsequent software-based low-level processing of ``trigger
primitives''
generated in the upstream \dword{daq} for the purposes of data selection. The collective
low-level information (trigger primitives and ``trigger candidates''
constructed from trigger primitives) is propagated to a central server responsible
for further processing and module-level triggering. The module level
trigger also
interfaces to a second server which is responsible for receiving and
propagating cross-module and external trigger and timing
information. The module level trigger considers trigger candidates and
external trigger inputs in issuing a ``\dword{trigcommand}'' to the \dword{daqbes}
subsystem. The \dword{daqbes} subsystem 
facilitates event building in a few servers and buffering for built
events on non-volatile storage. Upon receiving a \dword{trigcommand}, the \dword{daqbes}
 queries
data from the upstream \dword{daq} buffers and builds that into an ``event
record'', which is temporarily stored as (a number of) files. Event records can be optionally processed in a high-level
filter/data reduction stage, which is part of overall data selection,
 prior to event records being shipped to DUNE offline. Pervasively,
 the  \dfirst{daqccm} provides the data taking orchestration
 (Section~\ref{sec:fd-daq:design-run-control}), and the
 \dfirst{daqtss} provides synchronization and synchronous command distribution
 (Section~\ref{sec:sp-daq:design-timing}). Figure~\ref{fig:daq-conceptual-overview}
 provides a conceptual illustration of the overal \dword{daq} system
 functionality while Figure~\ref{fig:daq-deployment} illustrates the deployment of the system.
 %, while Figure~\ref{fig:daq-design}
%specifies the design implementation. 

\begin{dunefigure}[DAQ system overview]{fig:daq-conceptual-overview}{Conceptual
   Overview of \dword{daq} System Functionality for a single \nominalmodsize module}
  \includegraphics[width=0.9\textwidth,clip,trim=0 0 0 20mm]{daq-toplevel-conceptual-nebot.pdf}
\end{dunefigure}

\begin{dunefigure}[DAQ deployment]{fig:daq-deployment}{\dword{daq} Deployment}
%   \includegraphics[width=0.8\textwidth]{daq-overview.pdf}
\fixme{Someone draw this.  Roland?}
\end{dunefigure}

Key to the implementation of the \dword{daq} design is the requirement that
the system is partitionable. Specifically, the system can operate in
the form of multiple independent \dword{daq} instances, each 
executed across all \dword{daq} subsystems and uniquely mapped among subsystem components. 
More specifically, a given partition may span the entire %up to one
\dword{detmodule} or some subset of it; its extent is configurable at
run start. This ensures continual readout of the
majority of the detector in normal physics data-taking run mode, while
enabling simultaneous calibration or test runs of small portion of the
detector without interruption of normal data-taking. 

\subsection{Upstream DAQ}
\label{sec:daq:design-upstream}
%\fixme{module-generic}

The upstream \dword{daq} provides the first link in the data flow chain of
the \dword{daq} system and is where raw data from detector electronics
is received by the \dword{daq}.
It implements a receiver, buffer, and a portion of low-level data
selection (trigger primitive generation; see Section~\ref{sec:sp-daq:design-data-selection}) as detailed in Figure~\ref{fig:daq:readout}.
It is physically connected to the detector electronics via optical
fiber(s) and buffers and serves data to other \dword{daq} subsystems,
namely the \dword{daqdsn} and the \dword{daqbes}.

\begin{dunefigure}[DUNE upstream DAQ subsystem and connections]{fig:daq:readout}{\dword{dune} upstream \dword{daq} subsystem and its connections.}
  \includegraphics[width=0.8\textwidth]{daq-readout.pdf}
\end{dunefigure}

\begin{dunefigure}[DUNE upstream DAQ subsystem functional blocks.]{fig:daq:readout-blocks}{\dword{dune} upstream
    \dword{daq} subsystem functional blocks.}
  \includegraphics[angle=-90,width=0.8\textwidth]{daq_functional_blocks_upstream.pdf}
\end{dunefigure}

The upstream \dword{daq} system comprises many similar \dword{daqrou}, each
connected to a subset of electronics from a detector module and
interfacing with the \dword{daq} switched network. In the case of the
\dword{tpc}, 75 functionally identical RU are each responsible for the readout of raw data from two
\dwords{apa}. In the case of the \dword{pds}, 5-10 RU are each responsible for the
readout of raw data from a collection of \dword{pds} subdetectors, where each
collection corresponds to an optically isolated region in the
detector. 
Each RU is comprised of a \dword{fec} which is a commodity server that hosts a
collection of custom hardware, 
firmware, and software that collectively form four functional blocks,
which work together 
as illustrated in Figure~\ref{fig:daq:readout-blocks}, and are
itemized below:

\begin{enumerate}
\item Data reception (Section~\ref{sec:fd-daq:upstream-receiver}), facilitated by \dword{felix} cards;
\item Network based I/O (Section~\ref{sec:fd-daq:upstream-io}), facilitated by an embedded or commercial off-the-shelf network interface controller;
\item Data processing (Section~\ref{sec:fd-daq:upstream-proc}), facilitated by \dword{fpga} resources on the \dword{felix}
  card and/or on-host CPU resources, or, in the case of the \dword{tpc} RU only,
  additional \dword{fpga} resources in the form of two dedicated co-processing
  boards (interfacing directly with the \dword{felix} card); 
\item Buffering (Section~\ref{sec:fd-daq:upstream-buf}), facilitated by host RAM and SSD, or, in
  the case of the \dword{tpc} RU only, RAM
  and SSD available on the co-processing boards.
\end{enumerate}

In the baseline design, used also for system costing, each RU is composed of:
\begin{enumerate}
\item one dual socket multicores 2U server, with two \SI{10}{Gbps} and two \SI{1}{Gbps} ethernet ports for redundant data transmission and control, with at least 256 GB of DDR4 RAM, and sufficient PCIe lanes to host 2 TB of SSD disks,
\item two \dword{felix} cards~\cite{atlas-felix}, each with a PCIe 3.0 x16 interface and supporting up to 48 \SI{9.6}{Gbps} bidirectional serial optical links,
\item only in case of the TPC readout, four co-processor cards mounted onto the two \dword{felix} cards, for additional processing power and data buffering.

\end{enumerate}

Each functional block is described below.  In addition, and like all
other \dword{daq} subsystems, the upstream \dword{daq} uses services and implements interfaces that are provided by 
the \dword{daqccm} as described in Section~\ref{sec:fd-daq:design-run-control}.

\subsubsection{Data reception}
\label{sec:fd-daq:upstream-receiver}
The physical interface between the detector electronics and the DAQ are 9.6 Gbps point-to-point serial optical links running a simple (e.g., 8/10 bit encoded) protocol. 
For each \dword{apa} there are ten such links connecting its \dwords{wib} to the DAQ.
To reduce space and power consumption in the \dword{cuc}, high data aggregation is desired.
In the baseline design, the 20 fibers from two \dwords{apa} are aggregate into one \dword{fec} with each \dword{apa} connected to one \dword{felix} FPGA PCIe 3.0 board. 
If technology advances sufficiently a PCIe 4.0 version of \dword{felix} may be produced to increase the data aggregation such that each board would accept 20 links and a total of 40 links per \dword{fec} would be accomodated.
Tests were performed to verify \dword{om3} and \dword{om4} fibers support \SI{10}{Gbps} links over a run of \SI{300}{\meter}.
Higher speed fiber optic links may be employed in order to reduce the number of fibers if run lengths can be reduced.
The \dword{felix} board and firmware was developed initially by and for the ATLAS experiment and is now proposed or already in use by several other experiments including \dword{protodune}.

\subsubsection{Network based I/O}
\label{sec:fd-daq:upstream-io}

The upstream \dword{daq} subsystem provides access to the \dword{daqdsn} and \dword{daqbes} through a commodity switched network as illustrated in Figure~\ref{fig:daq:readout}).
The network communication protocol is as described in Section~\ref{sec:daq:design-ipc}.
The network I/O is handled by the \dwords{daqrou} via software; dedicated hardware or firmware development is not required.

\subsubsection{Data processing}
\label{sec:fd-daq:upstream-proc}

The data processing functional block is tasked with identifying and forming \dwords{trigprimitive}, after a stage of data reorganization and noise filtering for both the \dword{tpc} or \dword{pds}.

\Dwords{trigprimitive} summarize time periods on a channel where the digitized waveform is no longer consistent with noise.
These regions of interest are then used as input to the \dword{daqds} sub-system where they begin the process of forming a trigger decision.

In the baseline design this functional block is implemented in \dword{fpga}.
R\&D studies are ongoing to evaluate whether alternative implementations (in GPUs or CPUs) would bring any advantages in terms of flexibility or cost.

\subsubsection{Buffering}
\label{sec:fd-daq:upstream-buf}

In \dword{dune}, the upstream \dword{daq} system is in charge of buffering all
detector data until the \dword{daqdss} has issued a \dword{trigcommand}
(see Section~\ref{sec:sp-daq:design-data-selection}) and until the
\dword{daqbes} (Section~\ref{sec:fd-daq:design-backend}) has requested
and received the corresponding selected data. 
In addition, in the case of a \dword{snb} trigger, data received after
the issuing of the trigger must be buffered for a longer period to
avoid loss of data due to any possible downstream
bottlenecks. Localized and extended trigger activity are associated
with two rather different time scales and data throughput metrics, and
those collectively dictate the temporary storage technology and scale. 

The time required to form a trigger decision based on localized activity is expected to be on order one second.
Extended triggers present a far more challenging set of buffering requirements.  
Early activity from a \dword{snb} may occur at a rate near that of radiological activity.
Theoretical estimates indicate \snbpretime may be needed for the \dword{snb} interaction rate is significantly enough above backgrounds to warrant a trigger.
In order to locate interactions in this low-rate period, the full data rate must be buffered until a \dword{snb} trigger may be formed.
The throughput and endurance required by this buffer is satisfied by RAM technology such as DDR4.

A second challenge in recording data during a \dword{snb} is to assure essentially 100\% efficiency for collecting the individual, low-energy interactions. 
To achieve this, the full-stream of data is recorded for a time duration that is expected to cover the time envelope of the burst.
Guided by \dword{snb} models, this duration is chosen to be \snbtime.
This requires extracting as much as 115 TB from the \dword{tpc} upstream \dword{daq} across one single-phase detector module.
It is not cost effective to design the \dword{daq} to extract such extended data record along the same path as nominal readout.

The technology and scale of this additional buffering must satisfy several requirements. 
Each storage element must accept the full data rate of the portion of the detector module it services.
The media must have sufficient capacity and allow for sufficient
extraction throughput so that it is unlikely 
to ever be too full to accept another extended data record. 
Furthermore, assuming that, on average, a \dword{snb} trigger condition will be
satisfied once per month, the most optimal technology is
solid-state \dword{nvme} devices, which at the scale required to provide suitable input bandwidth, can provide a capacity to write the data from several extended activity triggers.
Providing only a modest overhead to normal operations, this data can
be extracted from such storage from the \dword{daq} in well under a day.

For both types of activity, the buffering requirements may be reduced by employing lossless compression to the data prior to it entering the buffer.
A factor of two-four compression is expected, based on MicroBooNE and \dword{protodune} experience using a modified Huffman encoding of differential ADC values. 
If expected noise levels are achieved, this compression would provide a factor of four to ten, depending on the detector module technology.
Effort is currently underway to understand the costs and technology involved in exploiting this benefit.

\subsection{Data Selection}
\label{sec:sp-daq:design-data-selection}

The \dfirst{daqdsn} subsystem is a hierarchical, online, primarily
software-based system. It is responsible for immediate and continuous processing of a substantial fraction of the entire input data stream. 
This includes data from \dword{tpc} and \dword{pds} subdetectors.
From that input, as well as external inputs provided, for example, by
the accelerator or detector calibration systems, the \dword{daqdss} must form a \dword{trigdecision},
which in turn produces a \dword{trigcommand}.
This command summarizes the observed activity that led to the decision
and provides addresses (in channel-time space) of the data in the
upstream \dword{daq} buffers that capture information about the activity.
This command is sent to and then consumed and executed by the \dfirst{daqbes} as described in Section~\ref{sec:fd-daq:design-backend}. 
It may also be propagated to the \dword{etl} and from there it may be
distributed to other \dwords{detmodule} or other detector systems
(e.g. calibration) for consideration.

The \dword{daqdss} must select data associated with calibration
signals, as well as beam interactions,
atmospheric neutrinos, rare baryon-number-violating events, and cosmic
ray events that deposit visible energy in excess of 100 MeV with high efficiency ($>$99\%). 
It must also select data associated with potential galactic \dwords{snb}, with galactic coverage
of $>$90\%. Galactic coverage is defined as efficiency-weighted probability of an intra-galactic \dword{snb}. 
To meet the requirement that the \dword{dune} \dword{fd} maintain
$<$\offsitepbpy to permanent storage, the \dword{daqdss}
subsystem must make \dword{daqdsn} decisions in a way that allows the \dword{daq} 
system to reduce its input data by almost four orders of magnitude
without jeopardizing the above efficiencies.

To meet its requirements, the \dword{daqdss} subsystem design follows a hierarchical data selection strategy, where low-level decisions are fed forward into higher-level ones until a module-level trigger is activated. 
The hierarchy is illustrated in Figure~\ref{fig:daq:data-selection-hierarchy} and details may be found in~\citedocdb{11215}.
At the lowest level, trigger primitives are formed on a per-channel basis, and represent, for the baseline design, a ``hit on a wire/channel'' activity summary. 
\Dwords{trigprimitive} are aggregated into \dwords{trigcandidate}, which represent information associated with higher-level constructs derived from trigger primitives, for example ``clusters of hits''. 
\dword{trigcandidate} information is subsequently used to inform a module-level trigger decision, which generates a \dword{trigcommand}; this takes the form of either a localized high energy trigger or an extended SNB trigger, and each prompts readout of an event record. 
Post-event-building, further data selection is carried out in the form of down-selection of event records, through a high level filter. 
The data selection strategy is applicable to both \dword{pds} and \dword{tpc} indistinguishably, up to the module level trigger stage, where \dword{pds} and \dword{tpc} information can be combined to form a module level trigger decision. 
Data selection design efforts have taken the approach of validating and demonstrating a \dword{tpc}-based data selection.
An additional \dword{pds}-based data selection component can be accommodated within the same data selection design and may augment data selection capabilities and efficiencies.

The subsystem structure is illustrated in
Figure~\ref{fig:daq:data-selection}. The structure
reflects the three stages of \dword{daqdsn}: (1) low level trigger, which consists of
\dword{trigprimitive} generation and subsequent
\dword{trigcandidate} generation; (2) module level trigger; and (3)
high level filter. Each stage is described in subsequent
sections. An additional subsystem component is the external trigger interface,
which serves as a common interface for the
module level trigger of each of the FD \dwords{detmodule} and between
the module level trigger and other systems (e.g.,~calibration,
accelerator and timing system) within a single
\dword{detmodule}. After sufficient confirmation of quality the
external trigger interface also sends \dword{snb} triggers
to global coincidence trigger recipients such as  \dword{snews} \cite{snews}.

\begin{dunefigure}[Data selection strategy and hierarchy]{fig:daq:data-selection-hierarchy}{Data selection
    strategy and hierarchy.}
  \includegraphics[width=0.9\textwidth, trim=0cm 5cm 0cm 0cm]{DS_hierarchy.pdf}
\end{dunefigure}

\begin{dunefigure}[DUNE DAQ data selection subsystem]{fig:daq:data-selection}{Block diagram of \dword{dune} \dword{daq}
    \dword{daqdsn} subsystem, illustrating hierarchical structure of
    subsystem design, and subsystem functionality.}
  \includegraphics[width=0.95\textwidth, trim=0 2cm 0 0]{DS_summary_2.pdf}
\end{dunefigure}

To facilitate partitioning, the \dword{daqdsn} subsystem will be
able to be instantiated multiple times, and multiple instances will be
able to operate in parallel. Within any
given partition, the \dword{daqdsn} subsystem will also be
informed and aware of current detector configuration and conditions and
apply certain masks and mapping on subdetectors or their fragments in
its decision making. This information is delivered to the
\dword{daqdss} from the \dword{daqccm} system.

Ultimately, each \dword{trigdecision} culminates in a command sent to \dword{daqbes}. 
This command contains all the logical detector addresses and time ranges
required such that an \dword{eb} may properly query the upstream \dword{daq}
buffers and finally collect and output the corresponding detector data
and the corresponding trigger data. To avoid duplication of data
records associated with \dwords{trigcommand} that overlap in readout
record ``space'', the \dword{daqdsn} system must also time-order and
prioritize trigger decisions. The details for forming this
command are described next and the operation of the \dword{daqbes} is
described in Section~\ref{sec:fd-daq:design-backend}.

The first stage of DUNE FD operations will have two general classes of trigger
decisions that are categorized in terms of the distribution of activity
in time and channel (mapped to physical) space from which they are derived: 
\begin{itemize}
\item Any given far detector module will make a 
  trigger decision with $>$99\% efficiency for any given particle
  type (electron, muon, photon, etc.) 
  that deposits more than 100 MeV of visible energy in a locally
  confined region (e.g.~a single \dword{apa}, or between two neighboring \dword{apa}s). To achieve this
  goal, data selection algorithms are targeting a trigger threshold of
  \SI{10}{\MeV} in visible energy, ensuring $>99$\% efficiency or better
  at \SI{100}{\MeV}. 
  This type of trigger is referred to as localized high
  energy trigger.
\item Localized activity associated with low deposited energy (as low as below 10 MeV). 
  Individually, such activity can be caused by neutrinos from burst or relic supernova as well as from the Sun.
  Collectively, information on these interactions is input to a different type of trigger,
  referred to as extended low energy trigger, which intended for
  capturing \dword{snb}.
  It differs from localized high
  energy triggers in that it considers the coincidence of localized
  activity across the entire module, and over an integration period of up to 10
  seconds.
\end{itemize}
Each trigger type furthermore prompts readout
    of the entire module but over significantly different time
    ranges: localized triggers prompt readout of \spreadout event records; extended
    triggers prompt readout of \SI{100}{\second} event records. 

Viable data selection algorithms for the low level and module level trigger already exist, including
algorithms for a module level \dword{snb} trigger.  It has
been demonstrated offline with Monte Carlo simulations that the resulting efficiencies meet the DUNE
requirements \cite{bib:docdb11215}. On the other hand, the pipelines of processing required
for \dword{daqdsn} can be executed using different firmware and software
implementations. Development is actively ongoing to demonstrate
and compare performance of different implementations. In satisfying
the philosophy and strategies of the \dword{daq} design, there is built-in
flexibility in defining whether each element of a pipeline executes on
\dword{fpga}, CPU, GPU, or, in principle, some other future hardware
architecture. A purely software implementation of data selection is being
implemented for demonstration at \dword{protodune}; it will be then modified to match the baseline design in which trigger primitives are generated in \dword{fpga}.

\subsubsection{Low Level Trigger: Trigger Primitive Generation}
\label{sec:sp-daq:design-trigger-primitives}

A \dword{trigprimitive} is defined nominally on a per-channel basis. In the case of
the \dword{spmod}, it is identified as a collection-channel signal rising above some
noise-driven threshold for some minimum period of time (here called a
``hit'').
A \dword{trigprimitive} takes the form of an information packet that 
summarizes the above-threshold waveform information in terms of its
threshold crossing times and statistical measures of its \dword{adc} samples. 
In addition, these packets carry a flag indicating the occurrence of any
failures or other exceptional behavior during \dword{trigprimitive} processing.

% \Dwords{trigprimitive} derived from \dword{tpc} and \dword{pds} data
% are produced in the upstream \dword{daq},
% nominally in \dword{fpga} firmware (\dword{tpc}) and CPU (\dword{pds}), as described in
% Section~\ref{sec:daq:design-upstream}.

Algorithms for generating \dwords{trigprimitive} are under development
\cite{bib:docdb11275}.  Trigger primitive generation proceeds
by establishing a waveform baseline
for a given channel, subtracting this baseline from each sample, maintaining
a measure of the noise level with respect to the baseline, and searching for the waveform to cross a
threshold defined in terms of the noise level. 
The  trigger primitive  (aka ``hit'') is said to span the time period when the waveform is above the noise threshold.
Such algorithms (see, e.g.~\cite{bib:docdb11236}) have been validated
using both Monte Carlo simulations and 
real data from \dword{protodune}. 
Trigger primitive generation performance is summarized in
Section~\ref{sec:sp-daq:design-validation}.

The format and schema of \dwords{trigprimitive} are subject to further
optimization, as they are further tightly coupled with the generation of
trigger candidates, discussed in the following subsection. Nominally,
each trigger primitive comprises the channel address (32 bit), hit
start time (64 bit), the time over
threshold (16 bit), the integral ADC value (32 bit), an error flag (16
bit), and possibly also
the waveform peak (12 bit) associated with the hit. 
As such, 20-22 bytes provides a generous data
representation of \dword{trigprimitive} information. 
The \dword{trigprimitive} rate will be dominated by the rate of decay of naturally occurring
$^{39}$Ar, which is about \SI{10}{\mega\hertz} per module.
This leads to a detector module aggregate rate of
\SI{200}{\mega\byte/\second}.
The subsequent stage of the \dword{daqdsn} must continuously absorb and process this
rate providing trigger candidates as described next.

\subsubsection{Low Level Trigger: Trigger Candidate Generation}

At the trigger candidate generation stage of the low level trigger,
\dwords{trigprimitive} from individual, contiguous fragments of the
detector 
module are cross-channel and -time correlated, and further selection
criteria are applied. This may result in the
output of trigger candidates. 
More specifically, once activity is localized in time and channel
space, it is
possible to apply a rough energy-based threshold based on the combined
metrics carried by the cross-correlated \dwords{trigprimitive};
satisfying this criteria defines a trigger candidate. 

A trigger candidate packet carries information about all the trigger
primitives that were used in its formation. 
In particular, it provides a measure of the total activity represented
by these primitives, as well as a measure of their collective time and channel
location and extent within the module.
These measures are used downstream by the module level trigger, 
as described more in the next section.

While the selection applied in the previous stage (trigger primitive
generation) is driven by a measure of noise, at trigger candidate
generation stage, prior to applying any thresholds, the rate is driven by
background activity.  
In particular, $^{39}$Ar decays would provide \SI{50}{\kilo\hertz} of
trigger candidates per \dword{apa} face if the threshold was set very low, ie at
\SI{0.1}{\MeV}.
Next, activity from the $^{42}$Ar decay chain would be substantial for a
threshold well below about \SI{3.5}{\MeV}.
Nominally, individual candidates, or groups of candidates nearby in
detector space and time, with measures of energy greater than these two
types of decays, will be passed to the module level trigger. 

This stage of data selection is
implemented in O(100) CPU servers, which receive the trigger primitive
stream from the upstream \dword{daq} and distribute trigger candidates to the
module level trigger stage, described next, via the 10 Gbps \dword{daq} network. Studies are
underway to demonstrate CPU resource utilization and latency, as are
efforts to demonstrate online trigger candidate generation at \dword{protodune}.
Trigger candidate generation performance is summarized in
Section~\ref{sec:sp-daq:design-validation}. 

%Additional studies are expected but nominally individual candidates, or
%groups of candidates nearby in detector space in time, with measures of
%energy greater than these two types of decays will be passed with little
%or no prescaling.


\subsubsection{Module Level Trigger}
\label{sec:daq:mlt}

Data selection is further facilitated as trigger candidates are consumed
by the module level trigger in order to form the ultimate trigger
decision which prompts the readout of data records from buffers kept by the upstream DAQ. 
The physical (channel and time) location, extent as well as the energy measure of the
candidates are used at this stage to categorize the activity in terms
of a localized high energy trigger or an extended low energy trigger. 
Specifically, N isolated, low energy candidates found in coincidence
over the integration period of up to 10 seconds across the full \dword{detmodule}
indicate the latter; individual high energy candidates, found
otherwise, indicate the former.

When a particular condition in a category is satisfied, the trigger
decision is made and a \dword{trigcommand} is formed. 
The \dword{trigcommand} packet includes information of the candidates (and primitives)
that were used to form it. 
The decision also provides direction as to what set of detector subcomponents
are to be read out and over what time period (localized or extended as described above). 
The module level trigger publishes a stream of \dwords{trigcommand} and the primary subscriber is expected to be the \dword{daqdfo} of the \dword{daqbes} as described in Section~\ref{sec:fd-daq:design-backend}.

The module level trigger is implemented in \bigo{1} CPU server (with 100\%
redundancy), which
receives the trigger candidate stream from the low level trigger stage
of the data selection and distributes \dwords{trigcommand} to the
backend \dword{daq} via the 10 Gbps \dword{daq} network. Studies are
underway to demonstrate CPU resource utilization and latency, as are
efforts to demonstrate online \dword{trigcommand} generation at \dword{protodune}.
\Dword{trigcommand} generation performance is summarized in
Section~\ref{sec:sp-daq:design-validation}.

\subsubsection{External Trigger Interface}

The \dfirst{daqeti} provides a loose coupling between the \dwords{mlt}, sources of external information such as beam spill times and information to or from components of the calibration system. 

As an interface between \dwords{mlt}, it is responsible receive and distribute information about module-level \dword{snb} trigger decisions.
This allows any detector module which alone may not have satisfied a \dword{snb} trigger requirement to nonetheless perform a \dword{snb} readout.
The \dword{daqeti} is also responsible for forming a coincidence between module-level \dword{snb} trigger decisions and publishing the results, eg for consumption by the \dfirst{snews}.

Information about beam spill times can be received from the accelerator by the \dword{daqeti}. 
These times can drive a model of the beam timeline to predict when beam spills will occur. 
These predictions can then be distributed to components of the calibration system so that they may avoid producing activity in the detector that may interfere with activity from beam neutrinos.
The beam time information can also be sent to the \dwords{mtl} so that they may either alter trigger decision criteria or merely include the information in contemporaneous trigger decisions.



\subsubsection{High Level Filter}
\label{sec:fd-daq:design-data-reduction}

The last processing stage in the \dword{daqdsn} subsystem is the
high level filter, which resides in the \dword{daqbes}.
The high level filter acts on triggered, read out, and aggregated data. 
It therefore serves primarily to down-select and thus
limit the total triggered data rate to offline storage, thereby keeping 
efficiency high in collecting information on activities of interest
while maintaining low selection and content bias, and reducing the output data
rate. It may do so via 
further filtering, lossy data reduction, and/or further event
classification.
As it benefits from operating on relatively low-rate data it can accommodate a higher level of
sophistication in algorithms for \dword{daqdsn} decisions.

More specifically, the high level filter may further reduce the rate of data output to offline storage by
applying refined selection criteria which may otherwise be prohibitive
to apply to the pre-trigger data stream.  For example, instrumentally-generated signals (e.g.~correlated noise)
may produce trigger candidates that can not be rejected by the module
level trigger and if left unmitigated may lead to an undesirably high
output data rate. 
Post processing the triggered data may allow reducing this unwanted
contamination.
% I (bv) don't agree with this next statement.
Furthermore, it can also reduce the triggered data set by further identifying
and localizing interesting activity. A likely candidate hardware
implementation of this level of \dword{daqdsn} is a GPU-based system
residing on surface at SURF\cite{georgia-add-something}.
\fixme{Give a reference to details on this idea.}

To fully understand how much and what type of data reduction may be
beneficial, simulation studies are ongoing \cite{bib:docdb11311} and will
necessarily have to be
validated with initial data analysis after
first DUNE \dword{fd} operation. Development efforts are also being
carried out to determine the scale of 
processing required by the \dword{fd}.


\subsection{Back-End DAQ}
\label{sec:fd-daq:design-backend}

The functionality of the \dfirst{daqbes} is comprised of the \dfirst{daqdfo}, \dfirst{eb} and storage buffer blocks in Figure~\ref{fig:fd-daq:layout}. 
It accepts \dwords{trigcommand} produced by the \dfirst{daqdss} as described in Section~\ref{sec:sp-daq:design-data-selection}. 
It queries the upstream \dword{daq} buffers and accepts returned data as described in Section~\ref{sec:daq:design-upstream}. 
Finally, it records \dwords{trigcommand} and the corresponding detector data to the output storage buffer. 
After application of the high level filter, the data is finally transferred to \fnal for offline storage.

\subsubsection{Data Flow Orchestration}

\fixme{Kurt, please check, especially against Roland's comments.}

To minimize the latency in the readout of the data from the upstream
\dword{daq} buffers, the \dword{daqbes} will support parallel readout of event
records into a pool of \dword{eb} processes.  This asynchronous, parallel readout will be coordinated by a \dword{daqdfo}.  Its operation is illustrated in Figure~\ref{fig:daq:backend} and is discussed here:

\begin{dunefigure}[DUNE DAQ back-end operation]{fig:daq:backend}{Illustration of \dword{dune} \dword{daqbes} operation.}
  \includegraphics[width=0.8\textwidth]{daq-backend.pdf}
\end{dunefigure}

\begin{itemize}
\item \dword{daqdfo} accepts a time ordered stream of \dwords{trigcommand} and dispatches each for execution possibly by first splitting up each command into one or more contiguous segments.
\item Each segment will then be dispatched to an \dword{eb} process as described in Section~\ref{sec:fd-daq:design-event-builder} for execution.
\item Execution entails interpreting the \dword{trigcommand} segment and querying the appropriate \dword{fe} buffer interfaces to request data from the period of time. 
\item Requests and their replies may be sent synchronously, and
  replies are expected even if data has already been purged from the
  \dword{fe} buffer. (In that case, an error message would be sent out.)
\item The data received may then undergo processing and aggregation
  until finally it is saved to one or more files on the output storage
  system before it is transferred offline.
\end{itemize}
%\fixme{Add time order requirement on DS above}
%\fixme{Add return-empty-response requirement to buffer above}



\subsubsection{Event builder}
\label{sec:fd-daq:design-event-builder}
%\fixme{module-generic}
%\metainfo{Explain art\dword{daq}, handling of trigger commands by asynchronous, parallel queries to front end Data Selector (but take care not to duplicate between here and in the overview).}

\fixme{Kurt, please check.}

The \dword{daqbes} will provide the instances of the \dfirst{eb} most likely as \dword{artdaq}~\cite{artdaq} components of the same name.
As described above, each EB instance will request selected data from
the appropriate \dfirst{daqubi}, as addressed by a received \dword{trigcommand}. 
An \dword{eb} will aggregate the selected data and record it the output storage buffer.
%%The final output files shall use data schema and file formats as described in Section~\ref{sec:fd-daq:design-data-model}.



\subsubsection{Data Quality Monitoring}
\label{sec:fd-daq:design-data-quality}

While the \dword{daqccm} contains an element of monitoring (Section~\ref{sec:daq:design:ccm:monitoring}), here \dfirst{dqm} refers to a subsystem that quickly analyzes the data in order to determine the general quality of the detector and DAQ operation.
This is in order to allow operators to promptly detect and respond to any unexpected changes and assure high exposure times for later physics analyses. 
A \dword{daq} \dfirst{dqm} 
will be developed (including necessary infrastructure, visualization,
and algorithms), which will process a subset of detector data in order
to provide prompt feedback to the detector operators. 
This system will be designed to allow it to evolve as the detector and its data is understood during commissioning and early operation and to cope with any evolution of detector conditions.

\subsubsection{Output Buffer}

The output buffer system is a hardware resource provided by \dword{daq}.
It has two primary purposes. 
First, the buffer decouples the production of data by the DAQ from the transfer of this output data to offline storage.
Second, it provides local storage sufficient for uninterrupted \dword{daq} operation in the unlikely event that the network connection between the \dword{fd} and \fnal is lost. 
A capacity of at least \SI{0.5}{\peta\byte} is envisioned,
sufficient buffer the nominal output of the entire \dword{fd} for about one week.
Based on prior experience
of the consortium with unusual losses of 
connectivity at other far detector experiment sites, this is a
conservative storage capacity value.

\subsubsection{Data Model}
\label{sec:fd-daq:design-data-model}
\fixme{Add reference to live document. Georgia}

\metainfo{Describe the data model. 
  This isn't a strict schema just things like how various parts of the
  detector readout map to files, etc.}



\subsection{Control, Configuration, and Monitoring}
\label{sec:fd-daq:design-run-control}
% \fixme{module-generic}

% \metainfo{Describe run control and \dword{daq} operation monitoring. 
%   How it makes use of the Message Passing System. 
%   What is an ``epoch''.
%   How Epoch Change Requests lead to zero downtime reconfiguration. 
%   Public key based ``iron'' authentication between run control processes and the controlled processes. 
%   Describe how RC will configure partitioning, initiate reconfiguration, handle ``run'' changes, node discovery, configuration, logging, startup, shutdown and failures are handled. Describe how RC will support detector electronics configuration.}

The \dfirst{daqccm} subsystem, illustrated in Figure~\ref{fig:daq-ccm-subsys}, encompasses, as its name suggests, the software needed to control, configure, and monitor the rest of the \dword{daq}, as well as itself. 
It provides a center for the highly distributed \dword{daq} components, allowing them to be treated and managed as a single, coherent system. 
Figure~\ref{fig:daq-conceptual-overview} shows the central role of the \dword{daqccm} within the complete \dword{daq} system.
The following sections describe each of the three \dword{daqccm} subsystems. 
Details on potential designs patterns that may be followed can be found in~\citedocdb{10482} 

\begin{dunefigure}[DAQ CCM subsystem interaction]{fig:daq-ccm-subsys}{Main interaction among the three \dword{daqccm} subsystems.}
  \includegraphics[width=0.8\textwidth]{daq-ccm-subsys.pdf}
\end{dunefigure}

\subsubsection{Control}
\label{sec:daq:design:ccm:control}


The \dword{daq} control subsystem actively manages \dword{daq} software process lifetimes, asserts access control policies, executes commands, initiates configuration changes, detects and handles exceptions, and provides an interface for human operators.

% \fixme{I (bv) redrew this from Giovanna's to get vector PDF and match \dword{dune} color palette.  I made two content changes: 1: move UI out of partition as it can't both initiate a partition and be inside it and likely will allow for creation/viewing of more than just one partition.  2: I had to guess on how garbage collection might go and so addded a line from RM to PM. }

\begin{dunefigure}[DAQ control subsystem roles and services]{fig:daq-ccm-control}{Roles and services that compose the \dword{daq} control subsystem.}
  \includegraphics[width=0.8\textwidth]{daq-ccm-control.pdf}
\end{dunefigure}

The control subsystem comprises a number of functional blocks of either global or partition scope as illustrated in Figure~\ref{fig:daq-ccm-control}. 
In the figure, ``partition'' refers to a logical segmentation of the \dword{daq} where each segment operates to some extent independently from others. 
The segmentation applies to the portion of detector electronics that provides input to the partition's \dword{daqdsn} and readout. 
The largest partition %would be 
is that of one \dword{detmodule}. 
The functional blocks in the figure represent one or more semi-autonomous agents, each with defined roles, capabilities, and access. 
Although drawn as single blocks, they typically are implemented as multiple peer agents to assure redundancy and fail-over. 
The blocks at partition scope are first described.

\begin{itemize}
\item Partition naming service provides \dword{daqdispre} for the components of a \dword{daqpart}. 
  That is, it allows one component to be made aware of the creation and continued operation of other components (discovery) or that others have recently become unresponsive (presence). 

\item Run control provides a central director;  its creation is the first step in initiating a \dword{daqpart}. 
  The \dword{rc} accepts, interprets, and validates input commands that may come either from a human via a user interface, or from other blocks (expert systems). 
  The commands describe a desired state of the \dword{daqpart}. 
  \dword{rc} will query other blocks to validate commands and then execute the commands by allocating processes through process management. 
  Once successfully allocated, their lifetimes are managed by \dword{rc}. 
  Throughout its lifetime, \dword{rc} will reconfigure an existing process, destroy it, or allocate additional processes. 

\item Supervisor provides a locus of expert system automation. 
  This block is instantiated by the \dword{rc} in order to augment or facilitate human commands with automated ones. 
  For example, it is expected that some set of commonly occurring exceptions or errors will be cataloged as the operators gain experience with the \dword{daq}.   Where possible, appropriate responses to these will be codified in the form of \dword{rc} commands.  The supervisor then will be charged with issuing these commands when one of the known exceptions are raised.

\end{itemize}

Global scope controls \dword{daq} components for all \dwords{daqpart} across all \dwords{detmodule}.  It consists of the following blocks:


\begin{itemize}
\item Global naming service aggregates the \dword{daqdispre} information across partitions.  

\item Process management allocates and may reclaim sets of processes on behalf of a requesting component (specifically \dword{rc} and resource management). 
  Process management only allocates processes if their requester has appropriate access privileges as determined by access management and if resource management determines sufficient resources exist. 
  
\item Resource management determines whether any process allocation can proceed.  It also enacts process garbage collection to reap any processes which have outlived a controlled shutdown command.

  
\item Access management is responsible for providing authentication and authorization for all \dword{daq} functions which require some level of access control. 

\end{itemize}

The last block in Figure~\ref{fig:daq-ccm-control} represents a number of different applications that provide a human-machine interface (UI) to the control subsystem.
At least one UI will be developed to allow a trained operator to construct and issue commands required to initiate, configure, potentially reconfigure, and finally terminate \dwords{daqpart}. 
UIs may enact access control in issuing commands but this does not replace access control inside the RC when accepting commands.
Any UI used for nominal operation or monitoring of the DUNE FD \dword{daq} (``shift work'') will be usable remotely from any DUNE institution and from a variety of personal computing platforms.  
Additional UI elements will be developed as described in sections~\ref{sec:daq:design:ccm:configuration} and~\ref{sec:daq:design:ccm:monitoring}.


\subsubsection{Configuration}
\label{sec:daq:design:ccm:configuration}

The \dword{daq} configuration subsystem provides persistent data storage for all historic, current, and future configuration information applicable to the \dword{daq}.
It provides a singular point (via high-availability, redundant services) for the allocation of unique and monotonically increasing \dwords{daqrunnum}.
The configuration data stores operate in an ``insert-only'' mode so that no prior information may be overwritten. 
The following types of information will be managed:

\begin{itemize}

\item Partition structure contains descriptions of the multiplicity and connectivity of \dword{daq} components for any partition.   Eg, this describes the overall connectivity of ``nodes'' in the \dword{daq} ``graph''.

\item Component parameters comprise configuration information associated with any given \dword{daq} component.  Eg, thresholds in a trigger candidate processor or gains and peaking times of a FE amplifier.

\item Run number provides a monotonically increasing sequence of \dwords{daqrunnum} that are allocated upon request to assure that each is unique.

\item Partition instances associate a \dword{daqrunnum} and the set of component parameters that were used to initiate a \dword{daqpart} or which are used to reconfigure an existing \dword{daqpart}.  

\item Constraints define rules that must be held true by resource management servicing requests for process allocations.  This information store also includes which constraints were used by resource management over time.
\end{itemize}


Access to configuration information is via a service that hides the choice of storage technology from any client queries.
This interface will also be used by editors run by human operators or by  generators run as part of an expert system.

\subsubsection{Monitoring}
\label{sec:daq:design:ccm:monitoring}

The \dword{daq} monitoring subsystem will help both humans and expert systems in detecting, diagnosing, and correcting anomalous activity, observing intended operation, and providing a historical record.
This subsystem will accept required information produced by any \dword{daq} component (here called status).

The precise implementation of the production, acceptance, store, post-processing, querying, and visualization of monitored status requires additional work. 
However, a publish-subscribe (PUB/SUB) network communication pattern is expected to be adopted for transport of monitoring messages. 
This will decouple production and consumption and facilitate development of a variety of status viewers, expert systems, debugging tools, etc. 
The types of messages include but are not limited to the following:

\begin{itemize}
\item Common to all will be a ``header'' holding a message type indicator, a sender address and the associated detector data time and the recent host computer time.
 
\item Logging messages add an importance label (e.g., debug, info, warning, error) and a succinct, human-readable information string providing an explanation of what occurred.
  
\item Metrics will provide structured data carrying specific information about predefined aspects of the sender. 
  This is similar to logging, but the messages support automated consumption and reaction by expert systems.  

\item Quality messages summarize information derived from the detector data (e.g., from waveforms) or its metadata (e.g., timestamps, error codes) while that data is ``in flight'' through the \dword{daq}.

\end{itemize}

In general, the \dword{daq} will retain all status records at least long enough to allow for any offline data quality validation procedures to be performed. 
However, some status feeds may be processed prior to storage if their raw form requires prohibitive amount of storage. 
In particular, the quality stream data rate may be too substantial for long term storage. 
Such streams will be summarized into histograms or other statistical representations prior to storing for longer term use.

In addition to this \dword{daq} \dword{daqccm} monitoring subsystem, a separate system must be used to monitor in depth the quality of the detector data content itself. 
See Section~\ref{sec:fd-daq:design-data-quality} for the description of this data quality monitoring system.

\subsubsection{Inter-Process Communication}
\label{sec:daq:design-ipc}

The DUNE FD \dword{daq} is an asynchronous, parallel distributed data processing system. 
It is composed of many independent processes which ingest and produce messages. 
The mechanisms of such message passing are generally called \dword{ipc}. 
Referring to Figure~\ref{fig:fd-daq:layout}, \dword{ipc} is used for both in-band detector data flow between upstream \dword{daq} and back-end \dwords{eb} and for out-of-band messages as part of \dword{daqccm}.  The \dword{ipc} used by the \dword{daqdsn} spans both descriptions as it passes derivations of a subset of detector data (trigger primitives, candidates) and culminates in a source of out-of-band message (\dwords{trigcommand}) to direct the readout by \dword{eb} and other components of detector data that is held in the upstream \dword{daq} buffers.

The ZeroMQ~\cite{zeromq} smart socket library is 
the basis of a system being developed and evaluated for parts of both in-band and out-of-band \dword{ipc}. 
As part of the \dword{daqccm}, this includes the issuing of control and reconfiguration commands to and receiving of monitoring messages from essentially all \dword{daq} components. 
As part of \dword{daqdsn}, this includes the transfer of \dword{trigprimitive}, candidate and command messages. 
In the upstream \dword{daq} this includes the \dfirst{daqubi} that  provides access to the upstream \dword{daq} primary buffers for queries by \dword{eb} and other components. 
\dword{ipc}  must be implemented broadly across many \dword{daq} systems and ZeroMQ allows their problems to be solved in common, shared software.  As \dword{daqccm} has the most complex \dword{ipc}  needs, this work is organizationally considered part of this system.

% One ZeroMQ facility in particular is worth additional description. 
% Zyre\footnote{An open source framework for proximity-based peer-to-peer applications. \url{https://github.com/zeromq/zyre}.} 
% will provide a decentralized implementation \dword{daqdispre} (see Section~\ref{sec:daq:design:ccm:control}). 
% It uses \dword{udp} broadcast beacons and a connected followup to provide peer discovery on the local network. 
% A heartbeat mechanism provides presence so that a component may discover when a peer has become unresponsive.  
% As Zyre uses the network itself, there is no central point of failure. 
% The network provides the naming service described above. 
% Zyre also allows for more centralized, connection oriented, discovery which can be used to allow the process to span networks.

As described in~\ref{sec:fd-daq:design-backend}, \dword{artdaq}~\cite{artdaq} utilizes \dword{ipc}  between its back-end components. 
It has been well tested with \dword{protodune} and other experiments. 
\dword{artdaq} may be used for some portions of the \dword{ipc}  described above. 
For example, if the \dword{daqubi} is implemented as an \dword{artdaq} Board Reader it would necessarily use \dword{artdaq} \dword{ipc} . 
This would limit the types of clients that could query for data in the buffers to be \dword{artdaq} modules. 
Understanding how to optimally select an \dword{ipc}  for such parts of the \dword{daq} connection graph is an area of ongoing R\&D effort.

\subsection{Timing and Synchronization}
\label{sec:sp-daq:design-timing}
%\fixme{single-phase module}
%\fixme{Is it indeed still single-phase specific?}
%\metainfo{Hardware, consumers, links.}

All components of the \dword{fd} use clocks derived from a single
\dfirst{gps} disciplined source, and all module components are
synchronized to a common \SI{62.5}{MHz} clock.
%
This rate is chosen in order for this common clock to satisfy the requirements of the detector electronics of both the single-phase and the dual-phase far detector modules.
%
To make full use of the information from the \dword{pds}, the common clock must be aligned within a single detector %unit 
module with an accuracy of \bigo{\SI{1}{\nano\second}}. 
For a common trigger for a \dword{snb} between modules, the timing must have an accuracy of order \SI{1}{\milli\second}.
However, a tighter constraint is the need to calibrate the common clock to universal time derived from \dword{gps} so the \dword{daqdsn} algorithm can be adjusted inside an accelerator spill, which again requires an absolute accuracy of order \SI{1}{\micro\second}.

The \dword{dune} \dword{fd} uses a version of the \dword{protodune} timing
system, where a design principle is to transmit synchronization messages over
a serial data stream with the clock embedded in the data. The format
is described in \citedocdb{1651}. The timing system design is
described in detail in \citedocdb{11233}.

Central to the timing system are four types of signals:
\begin{itemize}
\item a \SI{10}{\mega\hertz} reference used to discipline a stable master clock,
\item a \dfirst{pps} from the GPS,
\item a \dword{ntp} signal providing an absolute time for each \dword{pps}, and
\item an \dfirst{irig} time code signal
  used to set the timing system 64-bit time stamp.
\end{itemize}
%The timing system associates the \dword{gps} time to its master clock by using the latter to stamp the arrival time of the \dword{pps} of the former. To provide an absolute time to the nearest second the information from \dword{ntp} is used.
%\fixme{This may no longer be correct.  Will NTP be used and the host computer clock consulted for the absolute time?  Will PTP used in its stead?  How does IRIG fit in?  Finally, this sentence, however it needs to be written, is redundant with the itemized list.}

The timing system synchronization codes are distributed to the \dword{daq} readout components in the \dfirst{cuc} and the readout components on the cryostat via single mode fibers and passive splitters/combiners.
All custom electronic components of the timing system are contained in two \dword{utca} shelves; at any time, one is active while the other serves as a hot spare.
The \SI{10}{MHz} reference clock and the \dword{pps} signal are received through a single-width \dword{amc} at the center of the \dword{utca} shelf.
This master timing \dword{amc} is a custom board and produces the timing system signals, encoding them onto a serial data stream.
This serial data stream is distributed over a backplane to a number of fanout \dwords{amc}.
The fanout \dword{amc} is an off-the-self board with two custom \dwords{fmc}.
Each \dword{fmc} has four \dword{sfp} cages where fibers connect the timing system to each detector component (e.g., \dword{apa}) or where direct attach cables connect to other systems in the \dword{cuc}.

To provide redundancy, two independent GPS systems are used,
one with an antenna at the surface of the Ross shaft, and the other
with an antenna at the surface of the Yates shaft. Signals from either
GPS are fed through optical single mode fibers to the \dword{cuc}, where
either GPS signal can act as a hot spare while the other is active. 
Differential delays between these two paths are resolved by a second pair of fibers, one running back from the timing system to each antenna.


\section{Design Validation and Development Plans}
\label{sec:fd-daq:validation}

The following strategy will be followed in order to validate and
develop the \dword{dune} \dword{fd} \dword{daq} design:
\begin{itemize}
\item Use of \dword{protodune} as a design demonstration and
  development platform. 
\item Use of vertical slice teststands for further development and testing of
  individual \dword{daq} subsystems and for key aspects of the
  overall \dword{daq}
\item Use of horizontal slice tests to demonstrate scaling the design
  where the multiplicity of components in subsystem layers is important.
\item Use of \dword{fd} \dword{mc} simulations and emulations in order
  to augment actual hardware demonstrations at \dword{protodune} and teststands.
\item Benefit from developments and measurements from other ongoing
  \dword{lartpc} experiments, including MicroBooNE, SBND, and ICARUS.
\end{itemize}

This strategy reflects the current \dword{daq} project schedule,
provided in Section~\ref{sec:fd-daq:schedule}, which
comprises several phases, including an intense development phase
through 2020 that culminates in an engineering design
review (EDR) in Q1 of 2021. At this milestone, the system design will be
finalized and demonstrated to be capable of meeting the requirements of the
final \dword{daq} system. After the development phase, a
pre-production phase will begin and will end with a production readiness
review (PRR). By then, final designs of all components
will be complete.

The following subsections summarize past, ongoing, and planned
development and validation studies and identify how anticipated outcomes
will be used to finalize the \dword{daq} design.

\subsection{Design Validation and Development at ProtoDUNE and Other
  LArTPCs} % Experiments}

\label{sec:fd-daq:protodune}

The \dword{fd} \dword{daq} consortium constructed and operated the \dword{daq} system~\cite{Sipos:2018auh} for \dword{protodune}, which included two \dword{daq} readout architectures, one based on \dword{felix}, developed by ATLAS \cite{atlas-felix}, and the other on \dword{rce}, developed at SLAC \cite{daq-slac-rce}. \dword{daq} design and construction for \dword{protodune} began in Q3 of 2016, and the system became operational at the start of the beam data run in Q4 of 2018. The detector is continuing to run as of the writing of this document, recording cosmic ray activity, and providing further input for \dword{daq} development toward \dword{dune}. 

Figure~\ref{fig:daq-protodune} depicts the \dword{protodune} \dword{daq} system. 
The \dword{daq} is split between the \dword{felix} and \dword{rce} implementations. The two architectures share the same back-end and timing and trigger systems. 
Neither of these tested architectures exclusively represents the baseline design for the DUNE \dword{fd}. Instead, each qualitatively maps into one of two data processing approaches: one in which the data is processed exclusively in custom-designed \dword{fpga} firmware and carrier board, and the other in which the data is processed primarily with custom software run in commodity servers.
The baseline system for a \dword{detmodule} instead merges elements of the two approaches.
Specifically, it uses \dword{felix} as the hardware platform for data receiving and handling, and an co-processor FELIX daughter card (analogous to the \dword{rce} platform used at \dword{protodune}) that to provide additional, dedicated data processing resources.


\begin{dunefigure}[ProtoDUNE DAQ system]{fig:daq-protodune}{The \dword{protodune} \dword{daq} system.}
  \includegraphics[width=0.8\textwidth]{daq-protodune-overview.png}
\end{dunefigure}

Besides overall readout architecture, the \dword{protodune} and \dword{dune} \dwords{daq} exhibit two key differences. 
First, the \dword{protodune} \dword{daq} is externally triggered (and at a trigger rate over an order of magnitude higher than that anticipated for \dword{dune}). Because of this, the \dword{protodune} \dword{daq} does not facilitate online data processing from the \dword{tpc} or \dword{pd} systems for self-triggering. 
Second, the \dword{protodune} system sits at the surface with a much higher data occupancy due to cosmic ray activity.
Overcoming the first key difference to demonstrate \dword{daqdsn} capability for the \dword{fd} \dword{daq} design is a main component of future \dword{daq} development plans, described in Section~\ref{sec:sp-daq:design-plans}.

Continuous self-tirggering of the detector is also new with respect to
other ongoing or planned near-term \dword{lartpc} experiments, including
MicroBooNE, SBND, and ICARUS. Both MicroBooNE and ICARUS have demonstrated
self-triggering in coincidence with external gates, which effectively
limits both data and trigger rates, and is not a viable solution for
DUNE's off-beam physics program, as it would effectively limit
exposure and therefore physics sensitivity. %On the other hand, ICARUS
%has ...
MicroBooNE has demonstrated successful continuous readout of a \dword{lartpc},
via use of dynamic and fixed-baseline zero-supression implemented in
firmware for both \dword{tpc} and \dword{pds} readout. SBND, which utilizes the same readout
system as MicroBooNE, will investigate trigger primitive generation
and self-triggering based on \dword{tpc} information in the timescale of 2021-2023.

\fixme{Add references.}

\subsection{ProtoDUNE Outcomes}

The ProtoDUNE DAQ supported a test-beam experiment. The requirements of the DUNE DAQ are substantially different.
Yet the successful operation of the \dword{protodune} \dword{daq} has provided several key demonstrations for \dword{dune} \dword{daq}, in particular data flow architecture, run configuration and control, and back-end functionality.

More specifically, \dword{protodune} has demonstrated 
\begin{itemize}
\item Upstream \dword{daq}: front-end readout hardware and data flow functionality servicing two out of the six \dwords{apa} was successfully employed in \dword{protodune}.
  The data from each APA was continuously streamed to a single FELIX board hosted in a dedicated computer which then transferred it to two other computers for buffering and readout.
  The baseline upstream DAQ for \dword{dune} FD will retain one APA per FELIX board but place two FELIX boards and the buffering and readout functionality all together in a single \dword{fec}. 
  In addition to data flow functionality, \dword{protodune} \dword{fe} readout also demonstrates interface to front-end electronics, and scalability to DUNE. It also supports host server requirements and specifications. Finally, it serves as platform for further development involving co-processor implementation and data selection aspects.
\item \dword{daqbes}, \dword{daqccm} and software infrastructure:
 successful \dword{daqbes} implementation, including event builder
  farm and disk buffering, as well as \dword{daqccm}. This has allowed the
  development and exercising of system partitioning, and provides a
  basis for scalability to DUNE. \dword{protodune} also serves as
  a platform for further system development, in particular in the
  areas of \dword{daqccm} and for the data flow orchestrator part of the
  \dword{daqbes}.
\item Data selection and timing: \fixme{\dword{daqdsn} and \dword{daqtss}?} successful operation of the timing
  distribution system, and external trigger distribution to the
  front-end readout. Although \dword{protodune} was externally triggered, the
  system serves as a development platform for data-driven data selection.
\end{itemize}

Besides demonstrating end-to-end data flow, an important outcome of
\dword{protodune} \dword{daq} has been the delineation of
interfaces, i.e.~understanding the exact \dword{daq} scope and the interfaces to \dword{tpc}, \dword{pds}, and offline. The use of commodity solutions
where possible, and leverage of professional support from CERN IT 
substantially expedited the development and success of the project, as
did the strong on-site presence of experts from within the consortium during early installation and
commissioning. 
Outcomes specific to \dword{protodune} subsystems are discussed in
greater detail in~\cite{Hennessy:CDRReview}. 

\subsection{Ongoing Development}
\label{sec:sp-daq:design-validation}

%\fixme{this section and onward still needs work}

Subsystem development is ongoing at \dword{protodune} at the time of the
writing of this document. A detailed schedule for 2019 is available
in \cite{bib:docdb14095}. Major development plan milestones are:
\begin{itemize}
\item optimization and tuning of the \dword{fe} readout;
\item optimization and tuning of the \dword{artdaq} based dataflow software;
\item enhancement of monitoring and troubleshooting capabilities;
\item introduction of CPU-based hit finding (necessary for \dword{pds} readout);
\item introduction of \dword{fpga}-based hit finding (for \dword{tpc} readout);
\item implementation of online software data selection beyond trigger
primitive stage (introduction of trigger candidate generation, and
\dword{trigcommand} generation), and tests on well identified interaction
topologies (e.g. long horizontal tracks, or Michel electrons from muon decay);
\item integration of online triger command and modified data flow to event
builder to facilitate self-triggering of detector;
\item implementation of extended \dword{fpga}-based front-end functionality
(e.g., compression);
\item prototyping of fake \dword{snb} data flow in \dword{fe} and back-end.
\end{itemize}

Below, we focus on ongoing developments related to upstream \dword{daq},
data selection, which is a key challenge for DUNE and new with respect
to \dword{protodune} (and other existing or planned \dword{lartpc} detectors),
and \dword{daqccm} prototyping, which is common across multiple \dword{daq} subsystems.


\subsubsection{Upstream DAQ Development}

%The detector readout for DUNE will be based Front-End Link EXchange (FELIX)[?] system, originally developed for the ATLAS experiment DAQ upgrades.
The use of \dword{felix} as the front-end readout technology for DUNE was
successfully prototyped at \dword{protodune}, initially for the readout of one
\dword{apa}. In \dword{protodune}, \dword{felix} allows streaming of data coming over multiple 10
Gbps optical point-to-point links into commercial host memory and,
from there, storing, dispatching or processing of the data via
software. 

In \dword{protodune}, a single \dword{apa} \dword{felix}-based readout consists of two servers with a point-to-point 100 Gbps network connection to the \dword{felix} host computer.
The \dword{felix} I/O card interfaces with its host PC through 16-lane PCIe 3.0 (theoretical bandwidth of \SI{16}{GB/s}).
It transfers the incoming WIB data directly into the host PC memory using continuous DMA transfer. The \dword{felix} host PC runs a software process that publishes the data to any client subscribing to it.
Subscriptions may identify from which input optical links the received data stream will be sourced.
The clients consuming these streams are ``BoardReader'' processes implemented in the \dword{artdaq} data acquisition toolkit.
In order to sustain the data rate, modest modifications of the firmware and software were carried out specifically for \dword{protodune}: each \dword{felix} host receives and publishes data at $\sim$75 Gbps. The BoardReader hosts are equipped with embedded Intel QuickAssist (QAT)~\footnote{Intel\textregistered QuickAssist Technology, \url{https://www.intel.com/content/www/us/en/architecture-and-technology/intel-quick-assist-technology-overview.html}.} technology for hardware accelerated data compression. The \dword{protodune} application of the \dword{felix} \dword{fe} readout is shown in Figure~\ref{fig:sp-daq:felix-pd-impl}.


In DUNE only a very small fraction of the data received via the \dword{felix}
system will ever need to leave the host: thus it is not required to
implement very high speed networked data dispatching. On the other
hand it may be interesting to carry out data processing and buffering
on the host. While this is not the baseline design for DUNE, R\&D is
ongoing at \dword{pdsp} to evaluate the feasibility of
implementing hit finding, data buffering, and possibly even local
\dwords{trigcandidate} generation on the \dword{felix} host. 


\begin{dunefigure}[Topology of the FELIX-based
    upstream DAQ of ProtoDUNE]{fig:sp-daq:felix-pd-impl}{The topology of the \dword{felix}-based
    upstream \dword{daq} of \dword{protodune} (from~\cite{pdsp-felix}). The \dword{felix} host servers are publishing the data from the \dwords{wib} over \SI{100}{Gb} network interfaces. The BoardReader hosts are carrying out trigger matching, data compression and forwarding of fragments to the event builder.}
  \includegraphics[width=0.9\textwidth]{daq-protodune-server-topology-2x2.pdf}
\end{dunefigure}

The \dword{daq} team is investing substantial effort into the introduction of
a triggering chain based on the \dword{tpc} data into \dword{protodune}, which will
allow to carry out pre-design prototyping studies of the complete flow
of data of the DUNE \dword{daq}.  
The \dword{felix}-based readout system will be adapted to support the
different studies, from co-processor based data handling in firmware
(including trigger primitive generation, compression, and buffering) to software-based processing, on a single
server. Benchmarking and optimization of the \dword{felix} firmware and
software will also continue, with the aim of further compacting the
readout by supporting two \dwords{apa} on a single server. 

\subsubsection{Co-processor Development}

\fixme{Cussans, please improve this text and provide a diagram showing how connections between FELIX and co-processor.}

Upstream \dword{daq} development efforts at \dword{protodune} include a parallel test 
platform for trigger
primitive generation, compression, and buffering firmware validation for
the co-processor board.  The platform for these tests will initially
use a Xilinx ZCU102 development board, which will later be replaced
with a ``\dword{felix} demonstrator'' which demonstrates the use
of Xilinx Virtex-7 Ultrascale+ \dword{fpga} and \hlfix{four lane PCIe 4.0}{confirm gen and lanes} and which has a FMC+
connector which will be connected to a \hlfix{PBM}{introduce what a PBM is}.
Passive optical splitters
will be inserted into the fiber path downstream of the WIBs, providing
duplicate data inputs for the test hardware, without disrupting the
main readout path of \dword{protodune}. Tests using the development board will
first focus on generation of trigger primitives, which will be read
out over the network via IPBus\cite{Larrea:2015wra}. The ZCU102 board
includes 512MBytes DDR4 RAM connected to the \dword{fpga} programmable logic,
as well as a \hlfix{four lane PCIe 4.0}{confirm gen and lanes} socket which will host an \dword{nvme} SSD on an adapter,
which will allow tests of buffering \hlfix{and compression}{how does NVMe SSD test compression?} of readout
data. 

Tests using the development board will focus on functionality
rather than data throughput. However, the tests will provide estimates
of firmware resources that can be scaled up to the full system. Tests
using the \dword{felix} Demonstrator and PBM at \dword{protodune} will focus on
scaling the functional tests performed using the ZCU102, to a full
demonstration of trigger and readout functionality for a full \dword{apa}. In
addition, this platform will facilitate integration with the prototype
\dword{daqdss} and \dword{daqbes} subsystems
at \dword{protodune}. 


\subsubsection{Data Selection Development}

\begin{dunefigure}[CPU core-time for primitives and CPU utilization in live data]{fig:daq-cpu-hf-speed}{CPU core-time (left) required to find primitives in simulated signal and noise data across 960 collection channels.
    In this test, the data has been pre-formatted to facilitate the use of SIMD hardware accelerated functions (AVX2). 
    Per thread CPU utilization (right) to process live data from a \dword{protodune} \dword{apa}.
    Processing includes data reformatting, trigger primitive generation, and output message formation.
    Each thread, one per FELIX link, requires about 75\% CPU core utilization to keep up.
    Variation in utilization reflects the variations in levels of ionization and noise activity in the input data and is due to output message formation.
    The peak usage at the start arises from input buffering while the process is initializing. 
    Once operational state is achieved, this brief backlog is processed.}
  \includegraphics[height=6cm,clip,trim=4cm 16.5cm 4cm 2cm]{daq-primitive-cpu-speed.pdf}%
  \includegraphics[height=6cm,clip,trim=0cm 0cm 0cm 5mm]{apa5-processing-ptmp-pinning-2019-06-10-run8219-for-tdr.pdf}
\end{dunefigure}

\begin{dunefigure}[Trigger primitives in ProtoDUNE-SP data]{fig:daq-hitfinder}{Example result of the trigger-primitive software (``hit finder'') applied to ADC waveform data spanning 96 collection channels that are sensitive to activity in the drift volume of \dword{pdsp}. 
    At left, the figure shows the ADC sample values relative to pedestal which are input to the algorithm. 
    At right, these same data are shown with a green $\times$ marking each trigger primitive found. 
    The inset is a zoomed region showing in detail the alignment of the input waveforms and the derived trigger primitives.
    In this test, the algorithm runs on the continuous stream of ADC waveforms prior to readout while actual readout was prompted by an external trigger.}
  % \includegraphics[width=0.9\textwidth]{daq-hitfinder-montage.pdf}
  \begin{tikzpicture}
    \node[inner sep=0] (raw) at (0,0) {
      \includegraphics[height=7cm]{raw-event-96chans-run8583-evt1.pdf}
    };
    \node[inner sep=0, right = 0cm of raw] (hit) at (raw.east) {
      \includegraphics[height=7cm]{raw-event-and-hits-96chans-with-zoom-rect-run8583-evt1.pdf}
    };
    \node[inner sep=0] (zoom) at (4,4) {
      \includegraphics[height=5cm]{raw-event-and-hits-zoom-run8583-evt1.pdf}
    };
    \draw[->, line width=0.3mm, red] (6.25, -.32) -- (1.5, 2.05);
    \draw[->, line width=0.3mm, red] (7.01,  0.8) -- (5.65, 5.9);    
    %\draw[step=1,black,thin,xshift=1,yshift=1] (0,0) grid (10,10);
  \end{tikzpicture}
\end{dunefigure}

\begin{dunefigure}[Trigger primitive rates in ProtoDUNE-SP, four categories]{fig:daq-tp-rates}{Trigger primitive rates in \dword{pdsp} as a function of threshold in four categories: all data (top left),  after removal of particularly noisy channels (top right), with HV off so no contribution to signal (bottom left) and HV off and noisy channels excluded (bottom right).}
  \begin{minipage}[b]{0.5\linewidth}
    \begin{center}
      \includegraphics[page=1,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}

      \includegraphics[page=2,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}
    \end{center}
  \end{minipage}%
  \begin{minipage}[b]{0.5\linewidth}
    \begin{center}
      \includegraphics[page=3,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}

      \includegraphics[page=4,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}
    \end{center}
  \end{minipage}

\end{dunefigure}


During early stages of design, significant effort has been dedicated on
trigger primitive generation through simulations.
Specifically, charge collection efficiency and fake rates due to noise
and radiologicals have been studied as a function of hit threshold,
demonstrating that requirements can be met, given sufficiently low
electronics noise levels and radiological rates~\cite{bib:docdb11236}. 
Ongoing efforts within DUNE's radiologicals task force aim to validate
or provide more accurate background predictions, upon which this
performance will be validated.
In addition, offline studies demonstrate the performance of trigger
primitive generation algorithms as a function of number of CPU cores
used.  
The results are summarized in Figure~\ref{fig:daq-cpu-hf-speed} and show
that four cores are sufficient to keep up with 960 channels.
The test does not include reformatting of the data required to put it in
a form that allows AVX2 hardware SIMD acceleration.
In tests with live \dword{protodune} data, it is found that ten cores at
and average 65\% usage were enough to handle both reformatting and
trigger primitive selection. 
Effort on understanding and removing contribution
from cosmics/cosmogenics and (known) noisy channels is ongoing.
These results are summarized in Figure~\ref{fig:daq-tp-rates}
Additional details may be found in~\citedocdb{14062}.

\begin{dunefigure}[Efficiency for forming trigger candidates as input trigger primitives]{fig:daq-tc-eff-vis}{Efficiency for forming trigger candidates as input trigger primitives from two algorithms, online (blue) and offline (red).}
  \includegraphics[width=0.7\textwidth]{Electron_Efficiency_Comparison.pdf}
\end{dunefigure}

\begin{dunefigure}[Efficiency for forming trigger candidates from ionization activity]{fig:daq-tc-eff-allinc}{All-inclusive, efficiency for
    forming trigger candidates from ionization activity from beam $\nu_e$
    (left) and beam $\nu_\mu$ (right) interactions at or above a given
    visible energy. 
    At smaller energies, somewhat more events with their visible energy
    dispersed in space and time fail the trigger candidate selection
    criteria. 
    High efficiency is obtained at \SI{100}{\MeV} visible. 
    The trigger candidate algorithm used is the offline version, see
    Figure~\ref{fig:daq-tc-eff-vis} for comparison with online version.}
  \includegraphics[width=0.45\textwidth]{Integrated_Nu_e_Efficiency_MCC10.pdf}%
  \includegraphics[width=0.45\textwidth]{Integrated_Nu_mu_Efficiency_MCC10.pdf}
\end{dunefigure}


\dword{trigcandidate} generation, building on trigger primitives information
and considering integral \dword{adc} and trigger primitive proximity by channel
and time (in 50 microseconds) space, has also been studied with Monte
Carlo simulations~\cite{bib:docdb11215}.
Trigger candidates with sufficient total integral \dword{adc}  can be accepted to
generate corresponding \dwords{trigcommand} for localized high energy
activity, such as for beam, atmospheric neutrinos, baryon number
violating signatures, and cosmics.
Simulation studies demonstrate that this scheme meets efficiency
requirements for localized high energy triggers.
Specifically, simulations demonstrate that $>99$\% efficiency is
achievable for $>100$ MeV visible energy deposited by single particles
(shown in Figure~\ref{fig:daq-tc-eff-vis} for $e^-$), and that the
corresponding effective threshold for localized triggers for the system
is at $\sim$10 MeV. This translates to all-inclusive efficiencies for
beam $\nu_e$ and $\nu_\mu$ events in excess of 99\% for visible energies
above 100 MeV, as shown in Figure~\ref{fig:daq-tc-eff-allinc}.

Low-energy \dwords{trigcandidate} furthermore can serve as input to the
SNB trigger. Simulation demonstrates that the trigger candidate
efficiency for any individual \dword{snb} neutrino interaction is on the order
of 20-30\% \cite{pierre}. Simulations have further demonstrated that a
multiplicity-based \dword{snb} trigger decision that integrates low-energy
trigger candidates over an up to 10 seconds
integration window yields high ($>90$\%) galactic coverage while
keeping fake \dword{snb} trigger rates to one per month, per system
requirements \cite{pierre}. An energy-weighted multiplicity count scheme could be
applied to further increase efficiency and minimize background.
The dominant contributor to fake \dword{snb} triggers is
radiological backgrounds from neutrons, followed by radon \cite{rad}. It is
crucial to continue working closely with the radiological task force
to validate radiological simulation assumptions.

In the case of the high level filter, the consortium is also exploring the
use of machine learning techniques, specifically image classification
with the use of \dwords{cnn} on GPUs, as a way to
classify and down-select individual sections of \dword{tpc} channel vs. time
(``frames''), with extent of one \dword{apa}'s worth of collection plane
channels by one drift length (2.25 ms, or, 4500 samples). \dwords{cnn} have
been trained on \dword{mc} simulations of frames with each of
the following off-beam event topologies: 
atmospheric neutrino interactions, baryon number violating
interactions (proton decay or neutron-antineutron oscillation), cosmic
ray interactions, supernova neutrino interactions, or no interactions at all -- all
with radiological and noise background included in the
simulations. Preliminary studies show that a \dword{cnn} can be sucessfully trained classify any
given input frame as one of three categories: empty, containing a \dword{snb} neutrino
interaction, or containing a high-energy (atmospheric neutrino, baryon
number violating, or cosmic ray) interaction. Specifically, empty frames can be
rejected with an efficiency of $>$99\%, while frames containing or
partially containing 
a supernova neutrino, an atmospheric/cosmic interaction, or a baryon
number-violating interaction can be preferentially selected with efficiency
$>$88\%, $>$92\%, or $>$99\%, respectively  \cite{bib:docdb11311}. Such a 
filter could potentially be applied to reduce the event record size by
more than two orders of magnitude. Details may be found in~\citedocdb{11311}.
The speed at which machine learned inference may be applied is under study.


\subsubsection{Prototype Trigger Message Passing}

A prototype trigger message passing (PTMP) system using elements of the \dword{ipc} mechanism described in Section~\ref{sec:daq:design-ipc} is currently under development and testing at \dword{protodune}.
The primary goals of this prototype is to add a self-triggering mechanism to the \dword{protodune} detector that includes many of the features needed for the far detector DAQ.
Throughput and latency of the mechanism is being evaluated and optimized.
Message schema and application level protocols have been designed and are being improved.
Future work will include prototyping \dword{daqccm} functionality including \dword{daqdispre}.

PTMP has been successfully exercised to transfer trigger primitives from the software based hit finder.
These trigger primitives have also successfully been aggregated across an APA and the result fed to a trigger candidate finder which identifies horizontal muons.
\hlfix{From this output a trigger decision can be made.}{This is ongoing and hopefully can a full-chain will be tested and described before final draft.}

\subsection{Additional Teststands}
\label{sec:sp-daq:validation-demonstrators}
%\fixme{single-phase module}
%\metainfo{Describe VST demonstrators and why we must build them.}

Concurrently with \dword{pdsp} operation and development, a number of
``vertical slice'' teststands will be built to allow 
development and testing of individual parts of the \dword{daq} system
as well as testing of key aspects of the design and overall
scalability. A data selection subsystem vertical slice teststand will be
constructed and operated on fake-generated data, to assist in the
development of data selection, exercise the system for a variety of
configurations, perform small-scale tests that stress the critical
parts of the corresponding infrastructure, 
and identify likely failure points and/or bottlenecks. The subsystem
will also be deployed and exercised on existing HPC clusters of
comparable resources and specifications as planned for the final
production system for ``horizontal slice'' tests of similar nature. The
\dword{daqbes} will be developed and tested in a similar way.

In addition to dedicated vertical and horizontal slice teststands, a number of
\dword{daq} development kits will be available for the consortium for
specific component testing, as well as to other detector and
calibration consortia to support their own development, production, and quality assurance programs. The \dword{daq} 
kit will also form the basis for testing at \dword{apa} Construction sites beginning in 2020. 

% \subsection{Plan for Future Development}
% \label{sec:sp-daq:design-plans}
% \fixme{Giovanna and Allesandro}

\section{Production, Assembly, Installation and Integration}
\label{sec:sp-daq:production}

Development of these activities is still underway.
\fixme{This will be completed at a later time.}

\subsection{Production and Assembly}
\metainfo{Describe how hardware, firmware and software will produced. }

\subsubsection{Computing Hardware}

\subsubsection{Custom Hardware Fabrication}

\subsubsection{Software and Firmware Development}
\metainfo{Processes and practices.}

\subsection{Installation and Integration}

%\metainfo{Describe how we get stuff in place underground, how we will put it all together and make sure it works. 
%  What can we do to minimize the effort needed underground both in
 % terms of physical work but also in working out the bugs both in
 % individual processes and in emergent behavior of the system as a
 % whole?}

The DAQ will be installed in an enclosure in the west end of the Central
Utilities Cavern (\dword{cuc}) (Figure~\ref{fig:cavern-layout}).  Roughly
half of this space will be office space (including experimental control
workstations) and the other will be a computer room to hold the DAQ
front-end computing (Figure~\ref{fig:install-cuc}).  The Facilities
interface document is \citedocdb{6988} and the Installation interface
document is \citedocdb{7015}.

% TDR refs DocDB, not EDMS.  EDMS is only a "working area".
% \fixme{Docdb 7105 is obsolete, refers to https://edms.cern.ch/document/2145183}

\fixme{The installation interface document is also missing from Table 7.4}

\fixme{We could put a figure in here from Tim, which is far nicer than
  the CUC one referenced in the install chapter.}

Infrastructure in the \dword{cuc} will be installed starting in Q4~2022
(Figure~\ref{fig:high-level-schedule}).  At that point, CF will have
handed the DAQ group an empty room with cooling water and power
connections.  Over the next nine months, racks for the DAQ computing
will be installed, plumbed into cooling water, and connected to power
and networking.  The network connection from this data room to the fiber
trunks going up the shaft will also be made, as will preparations to
receive the multi-mode fiber connections from the \dword{wib} to the
\dword{felix} cards housed in servers in these racks.  There is space
for sixty racks, with four set aside for other consortia, twelve per
module for upstream DAQ electronics, and the remaining space for
networking and other DAQ computing needs.

Starting in Q3~2023, the data room will be ready for the installation of
the DAQ servers servicing the first module described in
Section~\ref{sec:fd-daq:considerations}.  This will proceed over the
next year, with servers being installed, cabled up, and tested.
As much configuring and commissioning work as possible will be done over
the network from the surface (or home institutions), to limit the number
of people underground.  Note that this data room is sized for all four
modules of DAQ computing, so one quarter will be installed at this
point.  If more computing power is needed for the commissioning of the
first module (for example, to deal with unexpected noise levels), space
and power will be borrowed from future modules until the problems are
alleviated. 
Additional space for eight racks will be on the surface in the Ross Dry
Room.  This will house the downstream \dword{daq} computers.

Starting in Q3~2024, the DAQ will thus be ready to connect fibers to the
\dwords{wib} on the detector top as planes are installed, to aid in
their commissioning.

The installation phase of the \dword{daq} system has the largest safety
concerns, which are discussed in Section~\ref{sec:fd-daq:safety}.

\section{Organization and Project Management}
\label{sec:sp-daq:organization}

This section begins with an overview of the organization of the DAQ consortium, that currently consists of 34 international institutions. Table~\ref{tab:daq-ib} provides the list of the participating institutions. Subsequently we present the current estimates of the resources required for the completion of the first SP module, followed by the schedule for design and R\&D, production, integration and installation of the DAQ within the detector. Finally, we discuss the risks associated to the project.

\subsection{Consortium Organization}

The \dword{daq} consortium was formed in 2017 as a joint single and
dual phase consortium, with a consortium leader and a technical
leader. The organization of the consortium is shown in
Figure~\ref{fig:daq-org}. The \dword{daq} institution board currently comprises
representatives from 34 institutions as shown in Table~\ref{tab:daq-ib}. The consortium leader is the spokesperson for the consortium and responsible for the overall scientific program and management of the group. The technical leader of the consortium is responsible for
managing the project for the group. The leadership is assisted in its dutied by the Project Office, composed by the Resource Manager, the Software and Firmware coordinator and the Integration and Support Coordinator, providing support in the corresponding area. 
The consortium is organized in working groups addressing the design, R\&D integration, and in the future, commissioning and installation of the key DAQ systems. The Physics Performance and Facility working groups are not associated to a specific system but provide oversight on the general DAQ performance in the physics context and the on the interface with the facility infrastructure. The DAQ working groups mandates are detailed in~\citedocdb{14938}.

\begin{dunefigure}[DAQ consortium org chart]{fig:daq-org}{Organizational chart for the \dword{daq} Consortium
}
  \includegraphics[width=0.9\textwidth]{daq-organogram.pdf}
\end{dunefigure}

\begin{dunetable}
[DAQ Consortium Board institutional members and countries]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-ib}
{DAQ Consortium Board institutional members and countries.}   
Member Institute & Country  \\ \toprowrule
CERN & CERN     \\ \colhline
Universidad Sergio Arboleda (USA) & Colombia     \\ \colhline
Czech Technical University & Czech Republic \\ \colhline
Lyon & France \\ \colhline
INFN Bologna & Italy \\ \colhline
Iwate & Japan     \\ \colhline
KEK & Japan     \\ \colhline
NIT Kure & Japan     \\ \colhline
NIKHEF & Netherlands    \\ \colhline
University of Birmingham & UK     \\ \colhline
Bristol University & UK     \\ \colhline
University of Edinburgh & UK     \\ \colhline
Imperial College London & UK     \\ \colhline
University College London (UCL) & UK     \\ \colhline
University of Liverpool & UK     \\ \colhline
Oxford University & UK     \\ \colhline
Rutherford Appleton Lab (RAL) & UK     \\ \colhline
University of Sussex & UK     \\ \colhline
University of Warwick & UK     \\ \colhline
Brookhaven National Lab (BNL) & USA     \\ \colhline
Colorado State University (CSU) & USA     \\ \colhline
Columbia University  & USA     \\ \colhline
University of California, Davis (UCD) & USA     \\ \colhline
Duke University & USA     \\ \colhline
University of California, Irvine (UCI) & USA     \\ \colhline
Fermi National Accelerator Laboratory (Fermilab) & USA     \\ \colhline
Iowa State University & USA     \\ \colhline
University of Minnesota, Duluth (UMD) & USA     \\ \colhline
University of Notre Dame & USA     \\ \colhline
University of Pennsylvania (Penn) & USA     \\ \colhline
South Dakota School of Mines and Technology (SDSMT) & USA     \\ \colhline
Stanford Linear Accelerator Lab (SLAC) & USA     \\ \colhline
\end{dunetable}

% \subsection{Cost and Labor}
% \label{sec:sp-daq:cost}

% Table~\ref{tab:daq-cost} shows the current cost estimates for the \dword{daq}
% subsystems major components necessary to serve the first DUNE FD
% module. Costs are expected to be reduced for subsequent modules, since
% a number of components are common across modules. %When appropriate, the quantities of
% %components are shown, along with the total cost and a brief description of
% %what is included in the cost estimate. 
% The cost estimates include
% materials and supplies (M\&S) for production, and packing and
% shipping to SURF, but not 
% spares. 

% Table~\ref{tab:daq-cost} also provides estimates of
% labor hours for construction. Labor depends on personnel category (e.g., faculty, student,
% technician, post-doc, engineer), and vary by region and
% institution. As such, costs are quantified using labor hours needed to
% fulfil a given task. Signficant physics and simulation
% effort is needed in particular for data selection related studies; those
% labor resources are included as part of Design and R\&D.

% \begin{dunetable}
% [DAQ System Cost Summary]
% {p{0.5\textwidth}p{0.2\textwidth}p{0.2\textwidth}}
% {tab:daq-cost}
% {DAQ System Cost Summary}
% Cost Item & M\&S (k\$ US) & Labor Hours \\ \toprowrule
% \rowcolor{dunepeach} Design, Engineering and R\&D & & \\ \colhline
% Facility & & \\ \colhline
% Timing & & \\ \colhline
% Detector Readout & & \\ \colhline
% Trigger & & \\ \colhline
% Data Flow & & \\ \colhline
% Event Filter & & \\ \colhline
% DAQ Control & & \\ \colhline
% \rowcolor{dunepeach} Production Setup & & \\ \colhline
% \rowcolor{dunepeach} Production & & \\ \colhline
% Facility & & \\ \colhline
% Timing & & \\ \colhline
% Detector Readout & & \\ \colhline
% Trigger & & \\ \colhline
% Data Flow & & \\ \colhline
% Event Filter & & \\ \colhline
% DAQ Control & & \\ \colhline
% \rowcolor{dunepeach} DUNE FD Integration \& Installation & & \\ \colhline
% Facility & & \\ \colhline
% TIming & & \\ \colhline
% Detector Readout & & \\ \colhline
% Trigger & & \\ \colhline
% Data Flow & & \\ \colhline
% Event Filter & & \\ \colhline
% DAQ Control & & \\ 
% \end{dunetable}
% \begin{dunetable}
% [DAQ System Cost Summary]
% {p{0.15\textwidth}p{0.1\textwidth}p{0.15\textwidth}p{0.4\textwidth}}
% {tab:daq-cost}
% {Cost estimates for different DAQ subsystems. All cost estimates
%   include M\&S for construction only. Packing and shipping costs are
%   included; spares are not included. }   
% System & Quantity & Cost (under development) (k\$ US) & Description \\ \toprowrule
% TPC Front-end Readout & 150 & - & Detector links  \\ \colhline
% TPC Front-end Readout & 75 & - & Felix, host server (CPU), and networking  \\ \colhline
% TPC Front-end Readout & 150 & - & Co-processor  \\ \colhline
% PDS Front-end Readout & 6-8 & - & Felix, host server (CPU), and networking  \\ \colhline
% Low level TPC Data Selection & 75 & - & Server (CPU), and networking \\ \colhline
% Low level PDS Data Selection & 6-8 & - & Server (CPU), and networking \\ \colhline
% Trigger Primitive local storage & 1 & - & 1PB hard drive storage \\ \colhline
% Module Level Trigger & 1+1 & - & Server (CPU), networking \\ \colhline
% External Trigger Interface & 1+1 & - & Server (CPU), networking, and
% interface boards \\ \colhline
% High-Level Filter & 5 & - &  Server (GPU), and networking \\ \colhline
% Back-end DAQ & 30 & - & Event Builder servers, and networking \\
% \colhline 
% Back-end DAQ Buffer & 1 & - & 1PB hard drive storage \\
% \colhline 
% CCM configuration, run control, databases & 4 & - & Server (CPU), and networking \\
% \colhline 
% CCM data management and transfer & 4 & - & Server (CPU), and networking \\
% \colhline 
% Timing \& Synchronization & 1 & - & Timing system, GPS \\ \colhline
% Facilities & 16 & - & Racks, professional installation \\ \colhline
% Infrastructure & - & - & IT \\ \colhline
% \end{dunetable} 


% \begin{dunetable}
% [DAQ System Labor]
% {p{0.25\textwidth}p{0.15\textwidth}p{0.1\textwidth}p{0.08\textwidth}p{0.08\textwidth}p{0.1\textwidth}p{0.08\textwidth}}
% {tab:daq-labor}
% {Estimate of labor hours for each category of personnel for different DAQ subsystems.}
% System  & Faculty/Scientist & Post-doc & Student & Engineer & Technician  &  \textbf{Total}\\ \toprowrule
% & (hours) & (hours)& (hours)& (hours)& (hours)& (hours)\\ \toprowrule
% Upstream DAQ & - & -& -& -& - & - \\ \colhline
% Data Selection & - & -& -& -& - & - \\ \colhline
% Back-end DAQ & - & -& -& -& - & - \\ \colhline
% Software Infrastructure & -& -& -& -& - & - \\ \colhline
% CCM & -& -& -& -& - & - \\ 
% Physics \& Simulation & -& -& -& -& - & - \\ \colhline
% \end{dunetable}


% Following the funding model envisioned for the consortium, various responsibilities are being distributed across institutions within the consortium.
% At this stage of the project firm founding decisions are still being made, but the aspirational institutional responsibilities identified so far indicate that sufficient core funding and resources are available for the implementation of the first SP DUNE detector module.

% Following the funding model envisioned for the consortium, various
% responsibilities have been distributed across institutions within the
% consortium. At this stage of the project, these should be \hlfix{considered
% as ``aspirational'' responsibilities}{???} until firm funding decisions are
% made. Table~\ref{tab:daq-inst-resp} shows the current institutional
% responsibilities for primary \dword{daq} subsystems. Only lead institutes are
% listed in the table for a given effort. For physics and simulation
% studies, and validation efforts at \dword{pdsp}, wider institutional effort is
% involved. A detailed list of tasks and institutional responsibilities
% is provided in the project WBS \cite{WBS}.

% \begin{dunetable}
% [DAQ System Institutional Responsibilities]
% {p{0.65\textwidth}p{0.25\textwidth}}
% {tab:daq-inst-resp}
% {Institutional responsibilities in the DAQ Consortium}
% DAQ Subsystem  & Institutional Responsibility\\ \toprowrule
% Upstream DAQ &  \\ \colhline
% Data Selection &  \\ \colhline
% Back-end DAQ &   \\ \colhline
% Software Infrastructure &  \\ \colhline
% CCM &   \\ \colhline
% Infrastructure and Facilities &   \\ \colhline
% Physics \& Simulation &   \\ \colhline
% \end{dunetable}

\subsection{Schedule and Milestones}
\label{sec:fd-daq:schedule}

The high-level DAQ milestones are listed in Tab.~\ref{tab:daq-sched}, interleaved with the top-level DUNE project milestones, and illustrated in in Fig.~\ref{fig:fd-daq:schedule}. Since the DAQ project is largely based on commercial off-the-shelf components, it can be seen in the overall timeline that many of the components are procured relatively in the project schedule.

\begin{dunetable}
[DAQ Consortium Schedule]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-sched}
{\dword{daq} Consortium Schedule}   
Milestone & Date (Month YYYY)   \\ \toprowrule

Upstream DAQ Architecture Technology Decision & June 2020 \\ \colhline
Engineering Design Review for Timing System &  June 2020   \\ \colhline
\rowcolor{dunepeach} Start of \dword{pdsp}-II installation& \startpduneiispinstall      \\ \colhline
Production Readiness Review for Timing System & June 2021 \\ \colhline
Preliminary Software Design Review & January 2022 \\ \colhline
Engineering Design Review for Hardware/Firmware & March 2022 \\  \colhline
\rowcolor{dunepeach} Start of \dword{pddp}-II installation& \startpduneiidpinstall      \\ \colhline
\rowcolor{dunepeach}South Dakota Logistics Warehouse available& \sdlwavailable      \\ \colhline
%\dword{prr} dates & December 2021 \\ \colhline
%Production Readiness Review Dates &  January 2022   \\ \colhline
Start of Racks Procurement & July 2022  \\ \colhline
Start of DAQ Server Procurement (I) & September 2022  \\ \colhline
\rowcolor{dunepeach}Beneficial occupancy of cavern 1 and \dword{cuc}& \cucbenocc      \\ \colhline 
Production Readiness Review for Readout Hardware/Firmware & December 2022  \\ \colhline
End of Racks Procurement & March 2023  \\ \colhline
Start of  DAQ Custom Hardware Production &  March 2023    \\ \colhline
\rowcolor{dunepeach} \dword{cuc} counting room accessible& \accesscuccountrm      \\ \colhline
End of DAQ Server Procurement (I) & May 2023  \\ \colhline
Start of DAQ Installation & May 2023 \\ \colhline
DAQ Software Final Design Review & June 2023  \\ \colhline

End of  DAQ Custom Hardware Production &  December 2023    \\ \colhline
\rowcolor{dunepeach}Top of \dword{detmodule} \#1 cryostat accessible& \accesstopfirstcryo      \\ \colhline
\rowcolor{dunepeach}Start of \dword{detmodule} \#1 TPC installation& \startfirsttpcinstall      \\ \colhline
Start of DAQ Server Procurement  (II) & September 2024  \\ \colhline
\rowcolor{dunepeach}Top of \dword{detmodule} \#2 accessible& \accesstopsecondcryo      \\ \colhline

End of DAQ Server Procurement  (II) & May 2025  \\ \colhline
End of DAQ installation & May 2025 \\ \colhline
\rowcolor{dunepeach}End of \dword{detmodule} \#1 TPC installation& \firsttpcinstallend      \\ \colhline
 \rowcolor{dunepeach}Start of \dword{detmodule} \#2 TPC installation& \startsecondtpcinstall      \\ \colhline
End of DAQ Standalone Commissioning & December 2025 \\ \colhline
\rowcolor{dunepeach}End of \dword{detmodule} \#2 TPC installation& \secondtpcinstallend      \\ \colhline
DAQ Server Procurement  (III) & July 2026  \\ \colhline
End of DAQ Commissioning & December 2026  \\ 
\end{dunetable}

\begin{dunefigure}[DAQ schedule for first \SI{10}{\kilo\tonne} module]{fig:daq-schedule}{\dword{daq} schedule for first \SI{10}{\kilo\tonne} module. \label{fig:fd-daq:schedule}}
  \includegraphics[width=0.95\textwidth,clip,trim=1cm 2cm 1cm 2cm]{daq-schedule.pdf}
\end{dunefigure}


\subsection{Safety and Risks}
\label{sec:fd-daq:safety}

Personnel safety during design, construction, testing, installation, and
commissioning of the system is crucial for the success of the
project. Detector safety during installation,
commissioning and operations is also key to project success.
The consortium will strictly follow ES\&H guidelines for the
project as well as follow the safety rules of the institutions where
the work is performed, including national laboratories, SURF, and
participating universities.  

Two overall safety plans will be followed by the FD DAQ. General work underground will comply
with all safety procedures in place for working in the detector caverns and CUC underground at
SURF. DAQ-specific procedures for working with racks full of
electronics or computers, as defined 
at Fermilab, will be followed, especially with respect to electrical safety and the fire suppression
system chosen for the counting room. For example, a glass wall between the server room space and
the other areas in the CUC will be necessary to prevent workers in the server room from being
unseen if they are in distress, and an adequate hearing protection
regime must be put in place.

There are no other special safety items for the DAQ system not already covered by the more
\hlfix{general safety plans referenced above}{not actually referenced}. The long-term emphasis is on remote operations capability
from around the world, limiting the need for physical presence at
SURF, and with underground access required only for urgent
interventions or hardware replacement. 

A set of risks to the successful operation of the DAQ system has been
identified by the consortium, and is provided,
together with mitigation strategies and pre-mitigation risk level, 
in Table~\ref{tab:risks:SP-FD-DAQ}. Post-mitigation risk levels are
currently being re-evaluated. Risk is quantified with respect to
probability, cost impact, and schedule impact. High (H), medium (M), and low (L)
probability is identified as
$>25$\%, $10-25$\%, and $<10$\%, respectively; high (H), medium (M), and low (L)
cost impact is identified as
$>20$\%, $5-20$\%, and $<5$\% cost increase, respectively; and high
(H), medium (M), and low (L) schedule impact is identified as 
$>6$ months, $2-6$ months, and $<2$ months delay, respectively.

\fixme{Post-mitigation risks are still H? Need some discussion of
  risks. These are pre-mitigation, actually. Will update these later.}

\input{generated/risks-longtable-SP-FD-DAQ}

The following risks and mitigation strategies have been identified:

\begin{description}
\item[Detector noise specs not met] Excessive noise will make it
  impossible for the data selection system to meet physics goals while
  generating reasonable data volumes. Prior (to construction) mitigation includes
  studying noise conditions at ProtoDUNE, and leaving provisions in the
  system for additional front-end filtering (in the form of the
  upstream DAQ upgradable processing resources) and/or post-event builder processing (in
  the form of the high level filter). Mitigation (post-construction) includes augmenting
  filtering resources using a larger computing system for the high
  level filter.

\item[Externally-driven schedule change] The DAQ has schedule links during
  testing, construction, and installation phases with most other
  subsystems. Schedule slip eslsewhere will potentially cause a schedule
  delay to the DAQ. Prior mitigation includes making provisions for
  stand-alone testing and commissioning of DAQ components, in the form
  of vertical and horizontal slice tests, at ProtoDUNE or
  elsewhere. Mitigation includes adjusting schedule for stand-alone
  testing and commissioning phases. 

\item[Lack of expert personnel] A significant number of experts in
  hardware, software, firmware are needed, and must be sustained
  throughout the project. Lack of personnel will increase technical
  risks and cause delay. Prior mitigation includes developing a full
  resource-loaded plan for DAQ, backed by national and institutional
  commitments, and avoiding single points of failure. Mitigation
  includes adaptation of the DAQ schedule, using schedule float.

\item[Power/space requirements exceed CUC capacity] The CUC has fixed
  space and power allocation for DAQ 
which cannot be exceeded.  Prior mitigation includes allowing
sufficient bandwidth up the shafts to move the upstream DAQ components for
subsequent DUNE far detector modules (modules 3 and 4) to the
surface, or moving some of the DAQ components to the detector caverns,
and carrying out a feasibility study for doing so. 
Mitigation includes expending additional resources on an
expanded surface facility, up front.

\item[Excess fake trigger rate from instrumental effects] Instrumental
  effects (beyond excessive noise) can cause fake triggers. Prior
  mitigation includes studying ProtoDUNE performance in detail, and monitoring detector
  performance during installation. Mitigation includes substantially increasing
  data volume, and increasing processing resources in the high level
  filter.

\item[Calibration requirements exceed acceptable data rate]
  Calibration schemes may require substantial data 
volumes, far in excess of triggered data volume, beyond currently
envisioned; e.g., due to offline analysis inefficiencies. Prior
mitigation includes allowing for back-end (event builder) system expansion to cope
with the increased data rate, and allowing for a high-level filter data
selections stage to carry out online analysis and data
reduction. Mitigation includes increasing the back-end DAQ and high
level filter system capacity. 

\item[Cost/performance of hardware/computing excessive] Costs of
  system-as-designed may exceed available budget, due to IT technology (FPGA, servers, storage) and market evolution developing in an unfavorable way. Prior mitigation
  includes the planning of prototyping and pre-construction phases to allow
  realistic appraisal of system costs, and applying sufficient margin
  in performance estimates.  Mitigation includes reducing performance
  or identifying additional funds. 

%\item[Optical components obsolete before production] Chosen data
%  transmission technology may be 
%obsolete (e.g. parallel 1Gb/s receivers). Prior mitigation includes
%conducting a thorough market survey before the final design choices
%for front-end electronics and DAQ interface technology, and sticking
%to commercial standards. Mitigation 
%includes change of design for both DAQ and front-end electronics.

%\item[Insufficient FPGA processing resources] FPGA processing
 % resources may be found to be
%insufficient after construction. Prior mitigation includes a careful
%assessment of resource needs in situ at ProtoDUNE and through
%simulation, as well as allowing for an upgrade of the system via FPGA
%replacement. Mitigation includes replace FPGAs or sustaining higher
%data rates through the system.

%\item[Insufficient throughput in computing system] The computing system
 % as-constructed may not scale to meet throughput requirements of
 % DAQ. Prior mitigation includes designing the system so as to allow
%   for expansion of resources as required (i.e.~by avoiding
%   single-point bottlenecks). Mitigation includes expanding the system capacity before commissioning. 

% \item[Event builder throughput insufficient] The event builder system
%   as-constructed may not scale to 
% meet throughput requirements of DAQ.  Prior mitigation includes
% designing the system so as to allow for expansion of resources
% (i.e.~by avoiding single-point bottlenecks). Mitigation includes
% expanding the system capacity before commissioning.

% \item[Additional data reduction steps required after event buiding] An
%   additional step of data reduction or online filtering
%  or analysis may be required, after the event building step, beyond
%  what can be accommodated in high level filter. Prior mitigation
%  includes the provision for a scalable high level filter on
%  surface. Mitigation includes expanding the high level filter
%  resources.

\item[PDTS fails to scale for DUNE requirements] The ProtoDUNE timing system concept may
  not scale to DUNE in scale or performance.  Prior mitigation
  includes test the PDTS at realistic scale before the final
  design. Mitigation includes replacing the PDTS with upgraded
  hardware.

% \item[Full remote operation of DAQ proves non-viable] The DAQ consoritum
%   concept is for remote operations 
%  with on-site maintenance activities as needed;
%  the system must be reliable enough to support this. Prior mitigation
%  includes carrying out remote operation tests during prototyping and
%  pre-production test phases, and designing human interface tools for
%  remote operations. Mitigation includes allocating surface or
%  underground control room space and facilities for DAQ operations at
%  SURF. 

\item[WAN network] The network connectivity to the experiment from
  remote locations may be proven unstable, making remote control and monitoring
  inefficient. Prior mitigation includes ensuring that minimal human intervention
  is needed on the system for steady data taking and that automted
  error recovery is well developed. Mitigation includes effort to further
  improve automated data taking, and increased cost for improving network
  connectivity. In the worst case, one would foresee presence of personnel at SURF.

\item[Infrastructure] The power/cooling systems on which the DAQ
  relies on cause more frequent than expected downtime. Prior
  mitigation includes designing, wherever possible, independent and redundant
  systems. Mitigation includes adding more uninterruptible power
  supplies, and improving the water cooling system to overcome otherwise
  degraded experiment uptime. 

\item[Custom electronics manifacturing issues] Large-scale production of high-speed custom
  electroncis proves challenging, resulting in DAQ installation delays. Prior mitigation includes diversifying manifacturers used for prototype production; assess manifacturer capability to meet specifications.  Mitigation includes running early pre-production with selected manifacurers, applying stringent QA criteria to ensure compliance with specifications.

\end{description}
