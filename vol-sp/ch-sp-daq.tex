\chapter{Data Acquisition}
\label{ch:sp-daq}

\fixme{Fix SI units throughout, and use consistent dword
  references. Brett}
\fixme{Add missing references. Georgia}
\fixme{Add a couple of figures on co-processor. Georgia}

\section{Introduction}
\label{sec:fd-daq:introduction}

The \dword{fd} \dword{daq} system is responsible for receiving,
processing, and recording data from the \dword{dune} \dword{fd}. In doing so, it provides
timing and synchronization for all \dwords{detmodule} and
subdetectors; receives, synchronizes, compresses, and buffers streaming
data from the subdetectors; extracts information from the data at a
local level to subsequently make local, module, and cross-module data
selection decisions; builds event records %``events''
 from selected space-time volumes 
and relays them to permanent storage; and carries out local data
reduction and filtering of the data as needed.

This chapter provides a description for the design of the \dword{dune}
\dword{fd} \dword{daq} system developed by the \dword{dune} \dword{fd}
\dword{daq} consortium. 
This consortium brings together resources and expertise from CERN,
Colombia, Czech Republic, France, Italy, Japan, the Netherlands, the UK, and the USA. 
Its members bring considerable experience from ICARUS, MicroBooNE,
SBND, and
DUNE prototype LArTPCs, as well as from ATLAS at the LHC and other major
HEP experiments across the world.

The system is designed %so that it 
to service all \dword{fd}
\dword{detmodule} designs interchangeably. % different \dwords{detmodule}. 
%The descriptions for those parts of the system are reproduced verbatim in some sections of this chapter as found in the TDR volume for each \dword{detmodule}.
%A small part 
However, some aspects of the \dword{daq} design are tailored to meet
module-specific requirements, and those are documented in sections of this chapter which are 
unique to the \dword{detmodule} covered in this TDR volume;  
these sections are %identified 
identifiable by their use of module-specific terms. In general, the
DAQ services each \dword{fd}
\dword{detmodule} independently, but cross-module communication is
implemented at the trigger level.

The chapter begins with an overview of the \dword{daq} design (Section~\ref{sec:fd-daq:overview}),
including requirements that the design must meet, and specification of
interfaces between the \dword{daq}  and other \dword{dune} \dword{fd} systems. 
Subsequently, Section~\ref{sec:fd-daq:design}, which comprises the
bulk of this chapter, describes the design of the \dword{fd}
\dword{daq} in greater detail.
Section~\ref{sec:fd-daq:validation} describes design validation efforts
to date, and future design development and validation plans. At the
center of these efforts is the 
\dword{protodune} \dword{daq} system (described in Section~\ref{sec:fd-daq:protodune}), which has served as a demonstrator of several
key aspects of the  \dword{dune}  \dword{daq}  design, and continues to serve as a
platform for further design development and validation toward the
DUNE DAQ design. 
Finally, the chapter finishes with two sections
(Sections~\ref{sec:sp-daq:production} and \ref{sec:sp-daq:organization}) providing details on
the management of the
\dword{daq} project, including schedule to completion of the design, 
production, and installation of the system, as well as cost, resources, and
safety considerations.

\section{Design Overview}
\label{sec:fd-daq:overview}

An overview of the \dword{dune} \dword{fd} \dword{daq} system 
servicing a single \dword{fd}
\dword{detmodule} is
provided in Fig.~\ref{fig:fd-daq:layout}. The system is
physically located at the \dword{fd} site, and it is split between the
4850 ft level and the surface level at SURF. Specifically, the DAQ occupies space and
power both in the central utility cavern (CUC) and the on-surface DAQ
room.  The upstream part of the system, which is responsible for
raw detector data reception, buffering, and pre-processing, lives
underground in the CUC, while the back-end part of the system, which is responsible for
event-building, as well as run control and monitoring, live on the
surface. Data flows through the DAQ from 
upstream to the back-end and then to offline. The majority
of raw data processing and buffering is performed underground, in the
upstream DAQ, thus minimizing data bandwidth to the surface. A
hierarchical data selection subsystem consumes minimally-processed
information from the upstream DAQ, and through further data processing
constructs module-level trigger decisions. Upon such a decision, a data
flow orchestrator process is activated as part of the back-end DAQ
to retrieve data to be built as part of an event record. At event
building stage, optional down-selection of the data is possible via
high level filtering, prior to shipping the data to offline.  All
detector subcomponents are synchronized and timed against a global,
common clock, provided by the timing and synchronization
subsystem. Cross-module communication as well as communication
to the outside world for data selection purposes is facilitated
through the external trigger interface. The
specifics of design implementation and data flow are described in Section~\ref{sec:fd-daq:design}.

\begin{dunefigure}{fig:fd-daq:layout}{\dword{daq} design physical
    layout focusing on a single \SI{10}{\kilo\tonne}
    module. Highlighted in orange, blue, yellow, gray, and brown are
    the Upstream DAQ, Data Selection, Back-end DAQ (Data Flow
    Orchestrator, Event Builder and Buffer), Timing and
    Synchronization, and Control,
    Configuration and Management subsystems, respectively. Not depicted in this figure
    is the Infrastructure Software subsystem, which includes Inter-process
    Communication, Databases, etc.
  }
  \includegraphics[width=0.8\textwidth,trim=4cm 3cm 4cm 3cm]{daq-layout-2.pdf}
\end{dunefigure}


\subsection{Requirements and Specifications}
\label{sec:fd-daq:requirements}

The \dword{dune} \dword{fd} \dword{daq} system is designed to meet the
\dword{dune} top-level as well as \dword{daq}-level requirements
summarized in Table~\ref{tab:specs:SP-DAQ}. The \dword{daq}-level requirements are
imposed to ensure that the 
system %is capable of 
can record all necessary information for offline 
analysis of data that is associated with on- and off-beam physics events, as directed
by the \dword{dune} physics mission, and with minimal compromise to
\dword{dune}'s physics sensitivity. The requirements must be met by following the 
specifications provided in the same table. Those specifications are
associated with trigger functionality, readout considerations,
and operations considerations, and are motivated further in the following subsections.

\subsubsection{How DUNE's Physics Mission Drives the DAQ Design}

The DUNE Far Detector has three main physics drivers: neutrino \dword{cpv} and related
long baseline oscillation studies using the high intensity beam provided
by \fnal, off-beam measurements of atmospheric neutrinos and searches
for rare processes such baryon-number-violating decays,
and detection of neutrinos from a \dfirst{snb} occurring within our galaxy. The
\dword{dune} \dword{fd} \dword{daq} system must facilitate data
readout for delivering on these main physics drivers, while keeping
within physical (space, power) and resource constraints for
the system. In particular, the off-beam measurements require the
continuous readout of the detector, and the lack of external triggers for such
events requires real-time or online data processing, and
self-triggering capabilities. Since the
continuous raw data rate of the far detector module, as received by
the DAQ system, reaches multiple
terabits per second, significant data buffering and processing
resources are needed as part of the design.

The \dword{dune} \dword{fd} modules employ two
active detector components from which the DAQ system must acquire
data: the \dfirst{tpc} and the \dfirst{pds}. The two components access the physics %each 
by sensing and collecting signals associated with very different 
sensing time scales.
Ionization charge measurement by the \dword{tpc} for any given
activity in the detector requires a
nominal recording of data over a time window of order \SIrange{1}{10}{\milli\second}. 
This time scale is determined by the ionization electron drift speed in
\lar and the detector dimension along the drift direction, and is
nominally set to 5.4~ms, corresponding to 2.4$\times$\SI{2.25}{\milli\second}. The
latter (\SI{2.25}{\milli\second}) assummes DUNE's nominal drift electric field of 500~V/cm.
On the other hand, the \dword{pds} measures argon scintillation light emission, which
occurs and is detected over a timescale of multiple nanoseconds to
microseconds for
any given event and/or subsequent subevent process. Unlike the TPC,
the PDS data is zero-suppressed in
the PDS electronics (see Chapter~\ref{}); therefore the total raw data volume received by
the DAQ system is expected to be dominated by
the TPC data, which is sent out from the front-end electronics as a continuous stream.
 
Figure \ref{fig:daq-rates} provides the expected activity rates in a
single far detector module as a function of true energy associated
with given types of signal.
At low energy ($<$10 MeV), activity is dominated by radiological backgrounds
intrinsic to the detector, and
low-energy solar neutrino interactions. Supernova burst neutrinos,
expected to arrive at a galactic \dword{snb} rate of once per century, 
would span the 10-30 MeV range. At higher energies (generally
above 100 MeV), rates are dominated by cosmic rays, beam neutrino interactions,
and atmospheric neutrino interactions. With the exception of supernova
burst neutrinos, the activity associated with any of these physics
signals is localized in space and particularly in time. Supernova burst
neutrinos on the other hand are characteristically different, as they arrive as multiple
signals of localized activity that extends over the entirety of the
detector and over multiple seconds. %The arrival time and energy
%distribution of
%neutrinos from a \dword{snb} is model-dependent; as such, the DAQ must
%utilize model-independent approach in selecting \dword{snb} activity.

The nature and rates of these signatures necessitates a data selection strategy which handles two
distinct cases: a localized high-energy activity trigger, prompting an event record readout for
activity associated with a minimum of 100 MeV of deposited energy; and an extended
low-energy activity trigger, prompting an event record readout when
multiple localized low-energy activity candidates with low deposited energy each are found over a short (less than 10
seconds) time period and over the entirety of a 10 kton 
module. Because of the high granularity of the detector readout elements, a
hierarchical data selection subsystem is employed to provide data processing and
triggering, and facilitate optional data reduction and filtering. The
DAQ  
system is required to yield $>$99\% efficiency for particles
depositing $>$100 MeV of energy in the detector, for localized
high-energy triggers; it is also required to yield sufficient efficiency for low-energy
deposition in the detector as needed to yield $>$90\% galactic
supernova burst trigger coverage, for extended low-energy triggers. Galactic coverage is defined as
the supernova burst trigger efficiency, weighted by the probability of
a galactic supernova burst (which is galactic distance-dependent).

By offline considerations, the DAQ is limited to sending no more than
30 PB per year to offline. As such, the steady state rate of localized triggers
from the entire far detector (including all four modules) is
effectively limited to less than 0.1 Hz, otherwise
more than 30 PB of data (uncompressed) would be generated per
year. This assumes (conservatively) that each localized high-energy
trigger prompts \SI{5.4}{\milli\second} of losslessly compressed TPC
data plus \SI{5.4}{\milli\second} of lossy compressed PDS data from
the entire module to be read out as part of the event record. The average rate
of supernova burst triggers is limited to 1 per month, per similar
considerations; this assumes that an extended low-energy trigger prompts 100 s of losslessly
compressed data from the entire module to be read out as part of the
event record. The one (1) extended trigger per month would amount to a
7\% addition to the total data volume generated by a 0.1 Hz localized
trigger rate.

The capability of recording data losslessly is built
into the design as a conservative measure; a particular concern is
charge collection efficiency in the case of zero
suppression. MicroBooNE is currently investigating the impact of zero
suppression on reconstruction efficiency and energy resolution for
low-energy events. Expected
data rates from physics signals of interest, which fit the 30 PB
yearly generated volume and
trigger rate requirements, are summarized in Table~\ref{tab:sp-daq:rates}.

\begin{dunefigure}{fig:daq-rates}{Expected physics-related activity
    rates in a single \SI{10}{\kilo\tonne} module. \label{sec:fd-daq:rates}
}
  \includegraphics[width=0.7\textwidth,clip,trim=6cm 6cm 10cm 2cm]{daq-event-type-rates-vs-energy.pdf}
\end{dunefigure}

\begin{dunetable}
[Expected DAQ Yearly Data Rates]
{p{0.25\textwidth}p{0.1\textwidth}p{0.5\textwidth}}
{tab:sp-daq:rates}{Summary
    of expected data rates. The rates assume no compression, and are
    given for a single \SI{10}{\kilo\tonne} module. Trigger primitives
    (see Section~\ref{sec:sp-daq:design-data-selection})
  are not kept permanently; they are temporarily stored for 1-2 months
  at a time.}
Source  & Annual Data Volume & Assumptions \\\toprowrule
Beam interactions & 27 TB & 10 MeV threshold in coincidence with beam
time, including cosmic coincidence; \SI{5.4}{\milli\second} readout \\\colhline
Cosmics and atmospheric neutrinos & 10 PB & \SI{5.4}{\milli\second} readout \\\colhline
Radiological backgrounds & $<1$ PB & $<1$ per month fake rate for SNB
trigger\\\colhline
Cold Electronics calibration & 200 TB & \\\colhline
Radioactive source calibration & 100 TB & $<10$ Hz source rate; single
APA readout; \SI{5.4}{\milli\second} readout \\\colhline
Laser calibration & 200 TB & 10$^6$ total laser pulses; half the
TPC channels illuminated per pulse; lossy
compression (zero-suppression) on all channels\\\colhline
Random triggers & 60 TB & 45 per day\\\colhline
Trigger primitives & $13$ PB & Dominated by $^{39}$Ar (50~kHz per APA face); collection
channels only; 20 bytes per trigger primitive \\\colhline
\end{dunetable}

Self-triggering on \dword{snb} activity is a unique challenge for the
DUNE FD, and an aspect of the design which has never been demonstrated
in a LArTPC. The challenge with \dword{snb} triggering is two-fold. 
First, the activity of the individual \dword{snb} neutrino interactions
is expected to be of relatively low energy (\SIrange{10}{30}{\mega\electronvolt}),
sometimes indistinguishable from pile-up of radiological background activity in the
detector.  Triggering on an ensemble of \bigo{100} events expected on
average in the case of a galactic supernova burst is therefore
advantageous; however, since this ensemble of events is expected to occur sparsely over the
entire detector and over an extended period of \bigo{10}\si{s},
sufficient buffering capability must be designed into the system to
capture the corresponding signals. 
Furthermore, to ensure high efficiency in collecting \dword{snb} interactions
that, individually, are below low-energy activity threshold, data from
all channels in the detector will be recorded over an extended and contiguous period of
time \bigo{100}\si{s} around every \dword{snb} trigger.

\input{generated/req-longtable-SP-DAQ} 

\subsubsection{Practical Considerations for Design}

The DAQ system is designed as a single, scalable system which can service
all FD modules. It is also designed on the principle that the system should be
able to record and store full detector data with zero dead time; and
that it should be evolutionary, taking advantage of the staged
construction for the DUNE FD, and thus beginning very conservatively
for the first DUNE FD module, and agressively reducing the design
conservatism as further experience is gained with detector
operations. At the same time, it is designed to preserve the possibility of additional capacity
as required. The bulk of processing and buffering of raw detector data is
done underground, in the upstream DAQ part of the system (see Figure~\ref{fig:fd-daq:layout}), in order to
minimize data traffic to surface. Power, cooling, and space
constraints in the CUC are limited to 600 kW total and 52 racks for all four FD
modules.

There are three key challenges for the DUNE FD DAQ system: 
\begin{itemize}
\item First, the system must accommodate
a long (``permanent'') commissioning state for the far detector, and
must therefore be a fully ``partitionable'' system. 

Given operational considerations, and in particular based
on the need to minimize \dword{snb} dead time, partitioning the \dword{daq} system allows 
a significant portion of the detector to remain physics-operational
even if a fault interrupts data collection in
some part of the detector. 
This partitionable operation mode also
permits detector development and specialized runs (e.g.,~calibrations)
to run in parallel with normal physics data taking, for small subsets
of the detector.

\item Secondly, the \dword{snb}  physics
requirements necessitate large buffering in the upstream DAQ and low fake
supernova burst trigger rates. 

The implementation of a continuous storage element in the data flow
architecture allows for the formation and capture of delayed, data-driven
trigger decisions with minimal loss of physics information. The
specification for this look-back buffer is set in consultation with
physics groups. The buffer size is driven primarily by the need to record up to ten
seconds of unbiased data preceding a \dword{snb} (with the
neutronization time taken as the time of the burst), and it is
specified to be greater than four seconds. This four-second
buffering provision works in tandem with a trigger latency
specification of less than one second. This aspect remains to be validated
with simulation, to ensure that high coverage (greater than 90\%) for galactic \dwords{snb} is achieved by the \dword{snb} trigger.

The \dword{daq} system is also designed so as to be able to %capable of
apply lossless compression to these records, as well as
%additionally 
filter them %in order 
to remove unnecessary data regions
in an intelligent way, i.e.~without compromising physics performance.

A programmable trigger priority scheme ensures %be provided, and
%implemented in a way 
that the readout for the main physics triggers
is never or rarely inhibited so as %, in order 
to enable easy determination of the live-time of
these triggers. % to be easily determined. 
At the same time, generation
of overlapping triggers will be possible, %but 
and ordering and prioritization will %be used to 
prevent data readout duplication. 

\item Finally, the difficult-to-access
location requires that the DAQ operates with high reliability and fully remote operation.

Furthermore, to ensure minimal impact to overall detector live-time, the \dword{daq} system is fully configurable,
controllable, and operable from remote locations, with
authentication implemented to allow exclusive control. The DAQ  
furthermore facilitates online monitoring of the detector and of
itself. %the \dword{daq} system in itself.

\end{itemize}

\subsection{Summary of Key Parameters}
\label{sec:sp-daq:parameters}

Table~\ref{tab:sp-daq:parameters} summarizes %all 
the important parameters
driving the \dword{daq} design. These parameters set the scale of
data buffering, processing, and transferring resources which must be
built into the design of each \dword{fd} module. 

\begin{dunetable}
[Key \dword{daq} Parameters]
{ll}
{tab:sp-daq:parameters}
{Summary of important parameters driving the \dword{daq} design.}
Parameter & Value \\ \toprowrule
TPC Channel Count per Module & 384,000\\ \colhline
TPC Collection Channel Count per Subdetector (APA) & 960\\ \colhline
TPC Induction Channel Count per Subdetector (APA) & 1,600\\ \colhline
PDS Channel Count per Module & TBD\\ \colhline
PDS Channel Count per Subdetector & TBD\\ \colhline
TPC \dword{adc} Sampling Rate & 2 MHz\\ \colhline
TPC \dword{adc} Dynamic Range& 12 bits\\ \colhline
PDS \dword{adc} Sampling Rate & TBD \\ \colhline
PDS \dword{adc} Dynamic Range & TBD \\ \colhline
PDS \dword{adc} Readout Length & TBD \\ \colhline
Localized Event Record Window & \SI{5.4}{\milli\second}\\  \colhline
Extended Event Record Window &  100 s\\  \colhline
Full size of TPC Localized Event Record per Module & 6.22 GB \\  \colhline
Full size of TPC Extended Event Record per Module & 115 TB\\  \colhline
Full size of PDS Localized Event Record per Module & TBD \\  \colhline
Full size of PDS Extended Event Record per Module & TBD \\  \colhline
\end{dunetable}


\subsection{Interfaces}
\label{sec:sp-daq:interfaces}

The \dword{daq} system scope begins at the optical fibers streaming raw digital data from the detector active components
(TPC and PDS), and ends at a wide area network (WAN) interface that
distributes the data from on site at \surf to offline centers off
site. The \dword{daq} also provides common computing and network services for
other \dword{dune} systems, although slow control and safety functions
fall outside \dword{daq} scope.

Consequently, the \dword{dune} \dword{fd} \dword{daq} system interfaces with the TPC \dword{ce}, \dword{pds}
readout, computing, \dword{cisc}, and calibration systems of the %\dword{dune}
\dword{fd}, as well as with facilities and underground installation. The
 interface agreements with the \dword{fd} systems 
are listed in Table~\ref{tab:sp-daq:interfaces}, and described
briefly in the following subsections. Interface agreements with
facilities and underground installation are described in Section~\ref{sec:sp-daq:production}.

\fixme{Add summaries in table. Georgia}
\begin{dunetable}
[\dword{daq} System Interface Links]
{p{0.3\textwidth}p{0.3\textwidth}p{0.2\textwidth}}
{tab:sp-daq:interfaces}
{Data Acquisition System Interface Links }
Interfacing System & Description & Reference \\ \toprowrule
TPC CE & & \citedocdb{6742}{v6}\\ \colhline
PDS Readout & &  \citedocdb{6727}{v2} \\ \colhline
Computing &&  \citedocdb{7123} \\ \colhline
CISC & & \citedocdb{6790}{v1} \\ \colhline
Calibration & Constraint on total volume of the calibration data;
trigger and timing distribution from the DAQ & \citedocdb{7069} \\ \colhline
Timing and Synchronization &&  \citedocdb{11224} \\ \colhline
Integration Facility & & \citedocdb{7042}{v0} \\
Facilities &&  \citedocdb{6988}{v1} \\ \colhline
\end{dunetable}

\begin{description}
\item[TPC Cold Electronics] The \dword{daq} and TPC \dword{ce} interface is described in
\citedocdb{6742}. The physical interface is at the \dword{cuc}, where optical links from the \dwords{wib} transfer
the raw TPC data to the \dword{daq} \dword{fe} readout (\dword{felix}; see
Section~\ref{sec:daq:design-upstream}). This ensures the \dword{daq} is electically decoupled from the detector
cryostat. Ten \SI{10}{Gbps} links are expected per \dword{apa}, and have
been specified as 300m OM4 multi-mode fibers from \dword{sfp}+ at the \dword{wib} to
\dword{minipod} on \dword{felix}. The data format has been specified
to use a custom communication protocol and no
compression.

\item[PDS Readout] The \dword{daq} and \dword{pds} readout interface is described in
\citedocdb{6727}. It is anticipated to
be of the form of 150  \SI{10}{Gbps} OM4 fibers from one FD module. 
This
is similar to the interface to the TPC \dword{ce}, except the overall
data volume is lower by an order of magnitude. The data format has been specified to use
compression (zero supression) and a custom communication protocol.
%\metainfo{Data reception physical and logical, configuration information delivery.}

\item[Computing] The \dword{daq} and computing interface is described in \citedocdb{7123}.
\metainfo{Buffer disk.  Agreement on system administration support and computer
  procurement, ssh gateways, non data networks.  Address reference how
  the data model described above is acceptable.}
 The computing consortium %online computing coordination 
 is responsible for the online areas of WAN connection between \surf and
\fnal, while the \dword{daq} consortium is responsible for disk buffering
to handle any temporary WAN disconnects and the infrastructure needed
for real-time data quality monitoring.  The computing consortium 
is also
responsible for the offline development and operation of the tools for data
transfers to \fnal. The primary
constraint in defining the \dword{daq} and offline computing interface is the
requirement to produce less than \SI{30}{PB/year} %to be transferred 
for transfer to
\fnal. \dword{daq} and %offline 
computing consortia are jointly responsible for data
format definition and data access libraries, as well as real-time data
quality monitoring software. The former is specified in the form of a 
data model documented in \citedocdb{??}.

\item[CISC] The \dword{daq} and \dword{cisc} interface is described in
\citedocdb{6790}. The \dword{daq} provides a network in the \dword{cuc} for \dword{cisc},  operation information and hardware
monitoring information to \dword{cisc}, and power distribution and
rack status units in \dword{daq} racks. The information from \dword{cisc}
feeds back into the \dword{daq} for run control operations.

\item[Calibration] The \dword{daq} and calibration interface is described in
\citedocdb{7069}. Two calibration systems are envisioned for the
%\dword{dune} 
\dword{fd}: a laser calibration system and a neutron
generator. Calibration pulses will be issued by the calibration
systems themselves, upon receipt of time stamps from the DAQ system;  %, and those 
the latter will be issued by the data selection system and distributed through the
\dword{daq} timing system.


\item[Timing and Synchronizationm] The timing system of the
  \dword{dune} \dword{fd} connects with 
almost all detector systems and with the calibration system and has a uniform interface to each of
them. 
A single interface document,
\citedocdb{11224}, describes all these timing interfaces. 

Accuracy of timestamps delivered to  detector endpoints will be $\pm$\SI{500}{\nano\second} with respect to UTC.  Synchronization between any two endpoints in the detector will be better than \SI{10}{\nano\second} on average.   Between detector modules, synchronization will be better than \SI{25}{\nano\second} on average.  The timing system will also provide a synchronized clock source by which DAQ computer system clocks may be synchronized using standard network time protocols.  System clocks are expected to be synchronized to within a ms using NTP and \si{\micro\second} using PTP standards. 
\end{description}

\section{Data Acquisition System Design}
\label{sec:fd-daq:design}

This section begins with an overview of the \dword{daq}
design followed by %sections giving the design of the \dword{daq}  
descriptions of the subsystem designs and implementation specifics. The
implementation details are evolving rapidly; as such, more information
is provided in technical notes as listed in Table~\ref{tab:sp-daq:tech-notes}. 

\begin{dunetable}{p{0.7\textwidth}p{0.2\textwidth}}{tab:sp-daq:tech-notes}{Summary of %relevant and 
detailed \dword{daq} technical notes on DUNE DAQ subsystem design.}
  Title & Reference \\ \toprowrule
  \dword{dune} \dword{fd} Data Volumes & \citedocdb{9240}\\ \colhline
  The \dword{daq} for the \dword{dune} prototype at CERN &
  \citedocdb{8708}\\ \colhline
 A System for Communication Between \dword{daq} Elements & \citedocdb{10482}\\\colhline
  Data Selection for \dword{dune} Beam and Atmospheric Events & \citedocdb{11215}\\\colhline
  Data orchestrator and event building for \dword{dune} \dword{fd}
  \dword{daq} & t.b.d. \\\colhline
ML-based Data Selecion for DUNE Far Detector & \citedocdb{11311} \\\colhline
 High Level ProtoDUNE tests & \citedocdb{14062} \\ \colhline
  \dword{dune} Run Control, Configuration \& Monitoring (CCM) & t.b.d. \\\colhline
  \dword{dune} \dword{daq} Readout & t.b.d. \\\colhline
  DUNE FD Timing and Synchronization System & \citedocdb{11233} \\\colhline
  What are the DUNE FD \dword{daq} Bottlenecks? & \citedocdb{11461}
  \\\colhline
\end{dunetable}


\subsection{Overview}
\label{sec:fd-daq:design-overview}

The  DAQ system is composed of six distinct subsystems: (1) Upstream
DAQ (Section~\ref{sec:daq:design-upstream}), (2) Data Selection
(Section~\ref{sec:sp-daq:design-data-selection}), (3) Back-end DAQ (Section~\ref{sec:fd-daq:design-backend}), (4) Control,
Configration, and Monitoring (CCM) (Section~\ref{sec:fd-daq:design-run-control}), (5) Timing and
Synchronization (Section~\ref{sec:sp-daq:design-timing}), and (6) Infrastructure Software. The physical extent
the first five of the DAQ subsystems
can be specified in reference to Figure~\ref{fig:fd-daq:layout}: the
Upstream DAQ and Timing and Synchronization live underground in the CUC; Data Selection occupies
both underground and above-ground spaces; Back-end DAQ is above-ground
and includes data flow orchestration, event building and buffering, before distribution of data
to offline; and CCM extends throughout the entire physical layout of the
system, supported on a private network throughout the DAQ system. Each of these subsystems is described in further
detail in the following subsections. To first order, the DAQ system
is implemented as an online data processing and data flow system. As
such, Inter-process Communication (IPC) is an
additional, central component to the FD DAQ, which spans across multiple
subsystems, as do other Infrastructure Software components, e.g.~databases. Due
to its central role, IPC is described below in a separate subsection (Section~\ref{sec:daq:design-ipc}).

Front-end readout is carried out by the Upstream DAQ, using custom data receiver and
co-processing FPGA/CPU hardware, all of which is hosted in O(100) servers in the CUC. A
similar number of additional servers is responsible for the execution
of subsequent software-based low-level processing of ``trigger
primitives''
generated in the upstream DAQ for the purposes of data selection; the collective
low-level information (trigger primitives and ``trigger candidates''
constructed from trigger primitives) is propagated to a central server responsible
for further processing and module-level triggering; the module level
trigger also
interfaces to a second server which is responsible for receiving and
propagating cross-module and external trigger and timing
information. The module level trigger considers trigger candidates and
external trigger inputs in issuing a ``trigger command'' to the back-end DAQ
subsystem. The Back-end DAQ subsystem 
facilitates event building in O(10) servers and buffering for built
events on non-volatile storage; upon receiving a trigger command, the back-end DAQ queries
data from the upstream DAQ buffers and builds that into an ``event
record'', which is temporarily stored as (a number of) files. Event records can be optionally processed in a high-level
filter/data reduction stage, which is part of overall data selection,
 prior to event records being shipped to DUNE offline. Pervasively,
 the  \dfirst{daqccm} subsystem provides the central orchestration
 (Section~\ref{sec:fd-daq:design-run-control}), \dfirst{ipc} provides
 overall communication (Section~\ref{sec:daq:design-ipc}), and the
 \dfirst{daqtss} provides synchronization
 (Section~\ref{sec:sp-daq:design-timing}). Figure~\ref{fig:daq-conceptual-overview}
 provides a conceptual illustration of the overal DAQ system
 functionality, while Figure~\ref{fig:daq-design}
specifies the design implementation. 

\begin{dunefigure}{fig:daq-conceptual-overview}{\dword{daq} Conceptual
   Overview of DAQ System Functionality for a single 10 kton module}
  \includegraphics[width=0.9\textwidth]{daq-toplevel-conceptual-nebot.pdf}
\end{dunefigure}

\begin{dunefigure}{fig:daq-design}{\dword{daq} DAQ Design
    Implementation for a single 10 kton module}
  \includegraphics[width=0.8\textwidth]{daq-overview.pdf}
\end{dunefigure}

Key to the implementation of the DAQ design is the requirement that
the system is partitionable. Specifically, the system can operate in
the form of multiple independent DAQ instances, each 
executed across all DAQ subsystems and uniquely mapped among subsystem components. 
More specifically, a given partition may span the entire %up to one
\dword{detmodule} or some subset of it; its extent is configurable at
run start. This ensures continual readout of the
majority of the detector in normal physics data-taking run mode, while
enabling simultaneous calibration or test runs of small portion(s) of the
detector without interruption of normal data-taking. 

\subsection{Upstream DAQ}
\label{sec:daq:design-upstream}
%\fixme{module-generic}

The Upstream DAQ provides the first link in the data flow chain of
the \dword{daq} system and is where raw data from detector electronics
is received by the DAQ.
It implements a receiver, buffer, and a portion of low-level data
selection (trigger primitive generation; see Section~\ref{sec:sp-daq:design-data-selection}) as detailed in Figure~\ref{fig:daq:readout}.
It is physically connected to the detector electronics via optical
fiber(s) and buffers and serves data to other \dword{daq} subsystems,
namely the \dword{daqdsn} and the back-end DAQ.

\begin{dunefigure}{fig:daq:readout}{\dword{dune} upstream \dword{daq} subsystem and its connections.}
  \includegraphics[width=0.8\textwidth]{daq-readout.png}
\end{dunefigure}

\begin{dunefigure}{fig:daq:readout-blocks}{\dword{dune} upstream
    \dword{daq} subsystem functional blocks.}
  \includegraphics[angle=-90,width=0.8\textwidth]{daq_functional_blocks_upstream.pdf}
\end{dunefigure}

The upstream DAQ system comprises many similar \dword{daqrou}, each
connected to a subset of electronics from a detector module and
interfacing with the \dword{daq} switched network. In the case of the
TPC, 75 functionally identical RU are each responsible for the readout of raw data from two
APAs. In the case of the PDS, 6-8 RU are each responsible for the
readout of raw data from a collection of PDS subdetectors, where each
collection corresponds to an optically isolated region in the
detector. 
Each RU encompasses a commercial off-the-shelf server that hosts a
collection of custom hardware, 
firmware, and software that collectively form four functional blocks,
which work together 
as illustrated in Figure~\ref{fig:daq:readout-blocks}, and are
itemized below:

\begin{enumerate}
\item Data reception (Section~\ref{sec:fd-daq:upstream-receiver}), facilitated by a FELIX card in the host server
\item Network based I/O (Section~\ref{sec:fd-daq:upstream-io}), facilitated by a commercial off-the-shelf network card
\item Data processing (Section~\ref{sec:fd-daq:upstream-proc}), facilitated by FPGA resources on the FELIX
  card and/or on-host CPU resources, or, in the case of the TPC RU only,
  additional FPGA resources in the form of two dedicated co-processing
  boards (interfacing directly with the FELIX card)
\item Buffering (Section~\ref{sec:fd-daq:upstream-buf}), facilitated by host RAM and SSD, or, in
  the case of the TPC RU only, RAM
  and SSD available on the co-processing boards
\end{enumerate}

Each of these blocks is described below.  In addition, and like all
other \dword{daq} subsystems, the upstream DAQ makes use of the common software framework for control, configuration, and monitoring as described in Section~\ref{sec:fd-daq:design-run-control}.

\subsubsection{Data reception}
\label{sec:fd-daq:upstream-receiver}
The physical interface between the detector electronics and the \dword{daq} to transmit data consists of 10 Gbps point-to-point serial optical links, running a simple (e.g., 8/10 bit encoded) protocol. 
The number of links per \dword{dune} module varies from approximately 1000 to 2000, depending on the detector technology adopted.

To minimize the space and power consumption footprint of the \dword{daq}, 10-20 links are aggregated into \dword{felix} boards hosted in commercial, off-the-shelf computers.
\dword{felix} is a \dword{fpga}-based PCIe board developed initially for ATLAS and now proposed or already used in several experiments, including \dword{protodune}. 
Existing firmware has been adopted and is being adapted to ensure
decoding and format checking of incoming data and then to marshal the
data to other blocks of the upstream DAQ subsystem.

\subsubsection{Network based I/O}
\label{sec:fd-daq:upstream-io}

The upstream DAQ subsystem provides access to the \dword{daqdsn} and \dword{daqbes} through a commercial, off-the-shelf switched network as illustrated in Figure~\ref{fig:daq:readout}).
The network communication protocol is as described in Section~\ref{sec:daq:design-ipc}.
The network I/O is handled by the \dwords{daqrou} via software; dedicated hardware or firmware development is not required.

\subsubsection{Data processing}
\label{sec:fd-daq:upstream-proc}

The data processing functional block resides either on the FELIX board FPGA, or on
an additional, dedicated co-processor (see Section~\ref{sec:sp-daq:design-validation}) providing additional FPGA
processing resources, or both. Data processing can also be carried out in the host
server processor. This functional block is ultimately
responsible for identifying regions of interest in the
detector (in the TPC or PDS) as a function of time.

As a preliminary step, data is pre-processed, i.e., organized in a way
that better suits subsequent data analysis. This implies, e.g.~reorganizing
data into different streams (collection plane vs. induction plane(s),
or re-arranging time and channel order and aggregating samples into frames), applying noise filtering algorithms, and compressing or zero-suppressing data.

The upstream DAQ system also facilitates the identification and
summary of regions of interest within the data, 
on a per-channel basis, into information packets
called \dwords{trigprimitive}.  These are forwarded to subsequent
stages of the
\dword{daqdsn} system which makes correlations and ultimately decides whether and which data is to be saved.

This functional block in principle may be implemented via FPGA, GPU, CPU, or a
combination of these elements. Deciding on the exact implementation is
premature at this stage; this is central to ongoing development efforts 
within upstream DAQ and Data Selection at protoDUNE. The DAQ design includes both FPGA and
CPU elements in the upstream DAQ, which provides the needed
flexibility and 
adaptability to meet the ultimate processing needs of the detector;
those will depend, for example, on noise levels in the detector.

\subsubsection{Buffering}
\label{sec:fd-daq:upstream-buf}

In \dword{dune}, the upstream DAQ system is in charge of buffering all
detector data until the \dword{daqdss} has issued a trigger command
(see Section~\ref{sec:sp-daq:design-selection-algs}) and until the
\dword{daqbes} (Section~\ref{sec:fd-daq:design-backend}) has requested
and received the corresponding selected data. 
In addition, in the case of a \dword{snb} trigger, data received after
the issuing of the trigger must be buffered for a longer period to
avoid loss of data due to any possible downstream
bottlenecks. Localized and extended trigger activity are associated
with two rather different time scales and data throughput metrics, and
those collectively dictate the temporary storage technology and scale. 

The buffering time required to select data associated with a localized
trigger is dominated by processing speed, pipeline depths and network latency. Some studies must still be performed but initial estimates indicate that the time buffering time required should not exceed approximately one second. 
A \SI{5.4}{\milli\second} block of data must be copied from the buffer
in order to fully capture interesting localized activity. As the full stream of data must constantly be buffered, a RAM
technology is selected based on providing sufficient throughput,
endurance, and capacity. 

Extended triggers present a far more challenging set of buffering requirements.  
Low-energy activity that is associated with a \dword{snb} trigger
decision may exist for as much as \snbpretime prior to the issuing of \dword{snb}
trigger (trigger time).
%This below threshold activity may be extracted using more sophisticated and processing intensive algorithms if the data can be recorded. 
A second challenge in recording data containing extended activity is
that all channels must be recorded for 100 seconds around the trigger
time, and requires extracting as much as 115 TB from the TPC upstream DAQ.
It is not cost effective to design the \dword{daq} to extract such
extended data record (100-second-long waveform from each TPC channel
and corresponding-in-time PDS information) in an online fashion. Thus, additional buffering is provided to catch
the temporary backlog of data that an extended trigger will produce.

The technology and scale of this additional buffering must satisfy several requirements. 
It must accept the full data rate of the detector module (as much as \SI{2}{\tera\byte/\second}). 
The data must then reside on nonvolatile media. 
The media must have sufficient capacity and allow for sufficient
extraction throughput so that it is unlikely 
to become too occupied to accept another extended data record. In
terms of storage, it should be capable of containing 
several successive extended data records.
Furthermore, assuming that, on average, a \dword{snb} trigger condition will be
satisfied once per month, the most optimal technology is %the use of 
solid-state devices, which at the scale required to provide suitable input bandwidth, can provide a capacity to write the data from several extended activity triggers.
Providing only a modest overhead to normal operations, this data can
be extracted from such storage from the \dword{daq} in well under a day.

For both types of activity, the buffering requirements may be reduced by employing lossless compression to the data prior to it entering the buffer.
A factor of at least two reduction in buffer input rate is
expected, based on MicroBooNE and protoDUNE experience. 
If expected noise levels are achieved, this compression would provide a factor of four to ten, depending on the detector module technology.
Effort is currently underway to understand the costs and technology involved in exploiting this benefit.

\subsection{Data Selection}
\label{sec:sp-daq:design-data-selection}


%\fixme{There are many studies which could go into this section. Some of this may very likely be moved into one or more tech notes and referenced.}
%\fixme{We
%have a viable algorithm for generating TP and high-energy TCs, and
%demonstrations of efficiency for SN bursts that meet the requirements. Be as explicit as possible on what the existing plans are, and discuss
%alternatives either as risk mitigation or opportunities.}

%The \dword{daq} must record all non-\dword{snb} unbiased\footnote{In the case of PDS readout, the PDS system only reads out biased
%  information; here it is implied that no additional bias is introduced by the DAQ.} data that span the maximum drift time window for any non-\dword{snb} trigger. 
%This time span is configurable at run start within a range between 20 microseconds and 30
%milliseconds, and it is nominally set to 5.4 ms (driven by the drift
%time). For \dword{snb} triggers, the \dword{daq} must record unbiased
%data spanning up to 100 seconds.
%%is to be recorded. 

%The data-driven trigger decision will be facilitated within the data selection subsystem, which will be an online, primarily software-based system, implemented accross multiple levels of data selection for both TPC and PDS readout. 

%The \dword{daqdsn} subsystem, an online, primarily software-based system implemented accross multiple levels of data selection for both TPC and PDS readout, will facilitate the data-driven trigger decision. 
%Information from the
%lower \dword{daqdsn} levels is aggregated and correlated at the
%\dword{detmodule} level to form a module-wide trigger command, intended to
%prompt module-wide readout.
%Some trigger information will be indirectly propagated between \dwords{detmodule} in order to facilitate a higher-efficiency \dword{snb} triggering than would be possible at the module level alone.


The \dfirst{daqdsn} subsystem is a hierarchical, online, primarily
software-based system. It is responsible for immediate and continuous processing of a substantial fraction of the entire input data stream. 
This includes data from TPC and PDS subdetectors.
From that input, as well as external inputs provided, for example, by
the accelerator or detector calibration systems, the \dword{daqdss} must form a \dword{trigdecision},
which in turn produces a \dword{trigcommand}.
This command summarizes the observed activity that led to the decision
and provides addresses (in channel-time space) of the data in the
upstream DAQ buffers that capture information about the activity.
This command is sent to and then consumed and executed by the back-end
DAQ as described in Section~\ref{sec:fd-daq:design-backend}. 
It is also propagated to the External Trigger Interface and from there it may be
distributed to other \dwords{detmodule} or other detector systems
(e.g. calibration) for consideration.

The \dword{daqdss} must select data associated with calibration
signals, as well as beam interactions,
atmospheric neutrinos, rare baryon-number-violating events, and cosmic
ray events that deposit visible energy in excess of 100 MeV with high efficiency ($>$99\%). 
It must also select data associated with potential galactic \dwords{snb}, with galactic coverage\footnote{Galactic coverage is defined as efficiency-weighted probability of galactic \dword{snb}.} of $>$90\%.
To meet the requirement that the \dword{dune} \dword{fd} maintain
$<$\SI{30}{\peta\byte/\year} to permanent storage, the \dword{daqdss}
subsystem must make \dword{daqdsn} decisions in a way that allows the \dword{daq} 
system to reduce its input data by almost four orders of magnitude
without jeopardizing the above efficiencies.

To meet its requirements, the \dword{daqdss} subsystem design follows
a hierarchical data selection strategy, where low-level
decisions are fed forward into higher-level ones until a  module-level
trigger is activated. 
 The hierarchy
is illustrated in Figure~\ref{fig:daq:data-selection-hierarchy}. 
At the lowest level, trigger
primitives are formed on a per-channel basis, and represent, for the
baseline design, a ``hit on
a wire/channel'' activity summary. Trigger primitives are aggregated
into Trigger Candidates, which represent information associated with
higher-level constructs derived from trigger primitives,
for example
``clusters of hits''. Trigger
Candidate information is subsequently used to inform a module-level trigger decision, which
generates a Trigger Command; this takes the form of either a localized high energy
trigger or an extended SNB trigger, and each prompts readout of an
event record. Post-event-building, further data selection is carried
out in the form of down-selection of event records, through a high
level filter. The data selection strategy is applicable to both PDS
and TPC indistinguishably, up to the module level trigger stage, where
PDS and TPC information can be combined to form a module level trigger
decision. Data selection design efforts
have taken the approach of validating and demonstrating a solely
TPC-based data selection, with the expectation that PDS-based data
selection (which can be accommodated within the same data selection design)
can only augment data selection capabilities and
efficiencies.

The subsystem structure is illustrated in
Figure~\ref{fig:daq:data-selection}. The structure
reflects the three stages of \dword{daqdsn}: (1) low level trigger, which consists of
\dword{trigprimitive} generation and subsequent
\dword{trigcandidate} generation; (2) module level trigger; and (3)
high level filter. Each stage is described in subsequent
sections. An additional subsystem component is the external trigger interface,
which serves as a common interface for the
module level trigger of each of the FD \dwords{detmodule} and between
the module level trigger and other systems (e.g.,~calibration,
accelerator and timing system) within a single
\dword{detmodule}. After sufficient confirmation of quality the
external trigger interface also sends \dword{snb} triggers
to global coincidence trigger recipients such as  \dword{snews} \cite{xx}.

\begin{dunefigure}{fig:daq:data-selection-hierarchy}{Data Selection
    strategy and hierarchy.}
  \includegraphics[width=0.9\textwidth, trim=0cm 5cm 0cm 0cm]{DS_hierarchy.pdf}
\end{dunefigure}

\begin{dunefigure}{fig:daq:data-selection}{Block diagram of \dword{dune} \dword{daq}
    \dword{daqdsn} subsystem, illustrating hierarchical structure of
    subsystem design, and subsystem functionality.}
  \includegraphics[width=0.95\textwidth, trim=0 2cm 0 0]{DS_summary_2.pdf}
\end{dunefigure}

To facilitate partitioning, the \dword{daqdsn} subsystem will be
able to be instantiated multiple times, and multiple instances will be
able to operate in parallel. Within any
given partition, the \dword{daqdsn} subsystem will also be
informed and aware of current detector configuration and conditions and
apply certain masks and mapping on subdetectors or their fragments in
its decision making. This information is delivered to the
\dword{daqdss} from the \dword{daqccm} system.

Ultimately, each \dword{trigdecision} culminates in a command sent to \dword{daqbes}. 
This command contains all the logical detector addresses and time ranges
required such that an \dword{eb} may properly query the upstream DAQ
buffers and finally collect and output the corresponding detector data
and the corresponding trigger data. To avoid duplication of data
records associated with trigger commands that overlap in readout
record ``space'', the \dword{daqdsn} system must also time-order and
prioritize trigger decisions. The details for forming this
command are described next and the operation of the \dword{daqbes} is
described in Section~\ref{sec:fd-daq:design-backend}.

The first stage of DUNE FD operations will have two general classes of trigger
decisions that are categorized in terms of the distribution of activity
in time and channel (mapped to physical) space from which they are derived: 
\begin{itemize}
\item Any given far detector module will make a 
  trigger decision with $>$99\% efficiency for any given particle
  type (electron, muon, photon, etc.) 
  which deposits more than 100 MeV of visible energy in a locally
  confined region (e.g.~a single APA, or between two neighboring APAs). To achieve this
  goal, data selection algorithms are targeting a trigger threshold of
  10 MeV in visible energy, ensuring $>99$\% efficiency or better
  at 100 MeV. 
  This type of trigger is referred to as localized high
  energy trigger.
\item    Localized activity associated with low deposited energy (as
  low as below 10 MeV) serves as input to a different type of trigger,
  referred to as extended low energy trigger, which intended for
  capturing supernova neutrino bursts. It differs from localized high
  energy triggers in that it considers the coincidence of localized
  activity across the entire module, and over an integration period of up to 10
  seconds.
\end{itemize}
Each trigger type furthermore prompts readout
    of the entire module but over significantly different time
    ranges: localized triggers prompt readout of \SI{5.4}{\milli\second} event records; extended
    triggers prompt readout of \SI{100}{\second} event records. 

Viable data selection algorithms for the low level and module level trigger already exist, including
algorithms for a module level supernova burst trigger, and it has
been demonstrated offline with Monte Carlo simulations that the resulting efficiencies meet the DUNE
requirements \cite{xx}. On the other hand, the pipelines of processing required
for \dword{daqdsn} can be executed using different firmware and software
implementations. Development is actively ongoing to demonstrate
and compare performance of different implementations. In satisfying
the philosophy and strategies of the \dword{daq} design, there is built-in
flexibility in defining whether each element of a pipeline executes on
\dword{fpga}, CPU, GPU, or, in principle, some other future hardware
architecture. A purely CPU implementation of data selection has been
the subject of successful, ongoing demonstration 
tests at ProtoDUNE. An FPGA implementation of the lowest level of data selection
(specifically, trigger primitive generation), 
however, is possible within the upstream DAQ design, and offers an
opportunity to reduce CPU costs as well as costs associated with power
consumption during detector operations. It is therefore the subject of
ongoing development at ProtoDUNE.

\subsubsection{Low Level Trigger: Trigger Primitive Generation}
\label{sec:sp-daq:design-trigger-primitives}

A \dword{trigprimitive} is defined nominally on a per-channel basis. In the case of
the \single TPC, it is identified as a collection-channel signal rising above some
noise-driven threshold for some minimum period of time (here called a
``hit'').
A \dword{trigprimitive} takes the form of an information packet that 
summarizes the above-threshold waveform information in terms of its
threshold crossing times and statistical measures of its \dword{adc} samples. 
In addition, these packets carry a flag indicating the occurrence of any
failures or other exceptional behavior during \dword{trigprimitive} processing.

\Dwords{trigprimitive} derived from TPC and PDS data
are produced in the upstream DAQ,
nominally in FPGA firmware (TPC) and CPU (PDS), as described in
Section~\ref{sec:daq:design-upstream}.

Algorithms for generating \dwords{trigprimitive} are under continual development
\cite{docid-11275}. Nominally, trigger primitive generation proceeds
by establishing a waveform baseline
for a given channel, subtracting this baseline from each sample, maintaining
a measure of the noise, and searching for the waveform to cross a
threshold defined in terms of the noise level. This thredhold crossing
represents a ``hit''. 
Such algorithms (see, e.g.~\cite{docid-11236}) have been validated
using both Monte Carlo simulations and 
real data from \dword{protodune}. 
Trigger primitive generation performance is summarized in
Section~\ref{sec:sp-daq:design-validation}.

The format and schema of \dwords{trigprimitive} are subject to further
optimization, as they are further tightly coupled with the generation of
trigger candidates, discussed in the following subsection. Nominally,
each trigger primitive comprises the channel address (32 bit), hit
start time (64 bit), the time over
threshold (16 bit), the integral ADC value (32 bit), an error flag (16
bit), and possibly also
the waveform peak (12 bit) associated with the hit. 
As such, 20-22 bytes provides a generous data
representation of \dword{trigprimitive} information. 
The \dword{trigprimitive} rate will be dominated by the rate of decay of naturally occurring
$^{39}$Ar, which is about \SI{10}{\mega\hertz} per module.
This leads to a detector module aggregate rate of
\SI{200}{\mega\byte/\second}.
The subsequent stage of the \dword{daqdsn} must continuously absorb and process this
rate providing trigger candidates as described next.

%\metainfo{Include plot and discussion of \dword{dune} trigger primitive rate in \dword{protodune}. 
%  The fact that this includes many more cosmics will not matter much as the rate is expected to be dominated by Ar39. 
%  Phil has this already but the LAr purity is not yet high enough to see Ar39 across the whole drift distance.}

\subsubsection{Low Level Trigger: Trigger Candidate Generation}

At the trigger candidate generation stage of the low level trigger,
\dwords{trigprimitive} from individual, contiguous fragments of the
detector 
module are cross-channel and -time correlated, and further selection
criteria are applied. This may result in the
output of trigger candidates. 
More specifically, once activity is localized in time and channel (``space'') it is
possible to apply a rough energy-based threshold based on the combined
metrics carried by the cross-correlated \dwords{trigprimitive};
satisfying this criteria defines a trigger candidate. 

A trigger candidate packet carries information about all the trigger
primitives that were used in its formation. 
In particular, it provides a measure of the total activity represented
by these primitives, as well as a measure of their collective time and channel
location and extent within the module.
These measures are used downstream by the module level trigger, 
as described more in the next section.

While the selection applied in the previous stage (trigger primitive
generation) is driven by a
measure of noise, at trigger candidate generation stage it is driven by background activity.  
In particular, the very high rate, low energy
$^{39}$Ar decays are expected to dominate 
trigger candidate rates, followed by activity from the $^{42}$Ar decay
chain. Nominally, individual candidates, or
groups of candidates nearby in detector space in time, with measures of
energy greater than these two types of decays, will be passed to the
module level trigger. 

This stage of data selection is
implemented in O(100) CPU servers, which receive the trigger primitive
stream from the upstream DAQ and distribute trigger candidates to the
module level trigger stage, described next, via the 10 Gbps DAQ network. Studies are
underway to demonstrate CPU resource utilization and latency, as are
efforts to demonstrate online trigger candidate generation at ProtoDUNE.
Trigger candidate generation performance is summarized in
Section~\ref{sec:sp-daq:design-validation}. 

%Additional studies are expected but nominally individual candidates, or
%groups of candidates nearby in detector space in time, with measures of
%energy greater than these two types of decays will be passed with little
%or no prescaling.


\subsubsection{Module Level Trigger}

Data selection is further facilitated as trigger candidates are consumed
by the module level trigger in order to form the ultimate trigger
decision which prompts the readout of event records. 
The physical (channel and time) location, extent as well as the energy measure of the
candidates are used at this stage to categorize the activity in terms
of a localized high energy trigger or an extended low energy trigger. 
Specifically, N isolated, low energy candidates found in coincidence
over the integration period of up to 10 seconds across the full \dword{detmodule}
indicate the latter; individual high energy candidates, found
otherwise, indicate the former.

When a particular condition in a category is satisfied, the trigger
decision is made and a trigger command is formed. 
The trigger command packet includes information of the candidates (and primitives)
that were used to form it. 
The decision also provides direction as to what set of detector subcomponents
are to be read out and over what time period. 
As described at the start of this section, localized triggers will instruct
the readout of the entire detector module for a period equal to \SI{5.4}{\milli\second}.  Extended triggers (\dword{snb}) will
instruct the readout of much longer period of \SI{100}{s}.

The module level trigger sends its produced trigger commands to the
\dfirst{daqbes} for the detector module, specifically the data flow
orchestrator. This in turn dispatches the command to an \dword{eb} for
execution as described in Section~\ref{sec:fd-daq:design-backend}. The
entire data selection chain as well as resulting query of data from
the upstream DAQ buffers must be 
accomplished promtly (within less than a second, not accounting for
integration time in the case of a supernova burst trigger). This
latency is governed by the upstream DAQ 
buffer size limitation.

The module level trigger is implemented in O(1) CPU server (with 100\%
redundancy), which
receives the trigger candidate stream from the low level trigger stage
of the data selection and distributes trigger commands to the
backend DAQ via the 10 Gbps DAQ network. Studies are
underway to demonstrate CPU resource utilization and latency, as are
efforts to demonstrate online trigger command generation at ProtoDUNE.
Trigger command generation performance is summarized in
Section~\ref{sec:sp-daq:design-validation}.

\subsubsection{External Trigger Interface}

The external trigger interface provides an interface between the
module level trigger and the timing and synchronization subsystem, calibration system,
other far detector modules, and the
outside world (external to the far detector). It is also responsible for correlating system timing with trigger information. 

Trigger commands generated by the module level trigger are also sent,
via the external trigger interface, to other detector modules, and
vice versa. This is needed in order to allow for cross-module
coincidences to be formed and thus produce an overall lower threshold for
capturing potential \dword{snb} occurrences. 
The external trigger interface also forwards \dword{snb} trigger
commands, after suitable quality confirmation, to external recipients
such as \dword{snews}.

In addition to accepting cross-module triggers via the external trigger
logic unit, the module level trigger must take inputs from out-of-band sources such as
needed for beam, calibration, or random triggering. These inputs are all
received via the external trigger interface, where they are correlated against the
global clock (derived from the timing system); the global timestamp of
each input is forwarded to the module level trigger. The module level trigger may
also issue timestamps (in anticipation of beam triggers,
for example) for calibration signals. Those are forwarded to the
timing subsystem via the external trigger interface, and get further
distributed to calibration systems by the timing subsystem (see
Section~\ref{sec:sp-daq:design-timing}). 

The external trigger interface is implemented in O(1) CPU server (with 100\%
redundancy), which hosts a number of White Rabbit node cards for
receiving accelerator and other timing inputs.

\subsubsection{High Level Filter}
\label{sec:fd-daq:design-data-reduction}

The last processing stage in the \dword{daqdsn} subsystem is the
high level filter, which resides in the back-end part of the \dword{daq}.
The high level filter acts on triggered, read out, and aggregated data,
produced by an \dword{eb}. 
It therefore serves primarily to down-select and thus
limit the total triggered data rate to offline, thereby keeping %. This allows to keep
efficiency high in collecting information on activities of interest
while minimizing selection and content bias, and reducing the output data
rate. It may do so via 
further filtering, lossy data reduction, and/or further event
classification. As it benefits from a longer latency (time between
$\sim$Hz-level built event records), it can accommodate a higher level of
sophistication in algorithms for \dword{daqdsn} decisions.

More specifically, the high level filter may further reduce the rate of data saved to final output storage by
applying refined selection criteria which may otherwise be prohibitive
to apply to the pre-trigger data stream.  For example, instrumentally-generated signals (e.g.~correlated noise)
may produce trigger candidates that can not be rejected by the module
level trigger and if left unmitigated may lead to an undesirably high
output data rate. 
Post processing the triggered data may allow reducing this unwanted
contamination.
% I (bv) don't agree with this next statement.
Furthermore, it can also reduce the triggered data set by further identifying
and localizing interesting activity. A likely candidate hardware
implementation of this level of \dword{daqdsn} is a GPU-based system
residing on surface at SURF.

To fully understand how much and what type of data reduction may be
beneficial, simulation studies are ongoing \citedocdb{xx} and will
necessarily have to be
validated with initial data analysis after
first DUNE \dword{fd} operation. Development efforts are also being
carried out to determine the scale of 
processing required by the \dword{fd}.


\subsection{Back-end DAQ}
\label{sec:fd-daq:design-backend}

The functionality of the \dfirst{daqbes} is represented by the ``Event Builder'' and ``Data Flow Orchestrator'' in Figure~\ref{fig:fd-daq:layout} or the ``Output'' circle in Figure~\ref{fig:daq-conceptual-overview}. 
It accepts trigger commands produced by the \dfirst{daqdss} as described in Section~\ref{sec:sp-daq:design-selection-algs}. 
It queries the upstream DAQ buffers and accepts returned data as described in Section~\ref{sec:daq:design-upstream}. 
Finally, it records trigger commands and the corresponding data in the
form of ``event records'' to the output storage buffer, from which the data is transferred to offline.

\subsubsection{Dataflow Orchestration}

To minimize the latency in the readout of the data from the Upstream
DAQ buffers, the back-end DAQ will support parallel readout of event
records into a pool of Event Builder processes.  This asynchronous, parallel readout will be coordinated by a dataflow orchestrator (DFO).  Its operation is illustrated in Figure~\ref{fig:daq:backend} and is discussed here:

\begin{dunefigure}{fig:daq:backend}{Illustration of \dword{dune} \dword{daq} back-end operation.}
  \includegraphics[width=0.8\textwidth]{daq-backend.pdf}
\end{dunefigure}

\fixme{Remove egress reference from figure. Replace with Back-end
  DAQ. Do we need to show event builder idle pool? Brett}

\begin{itemize}
\item \dword{daqdfo} accepts a time ordered stream of \dwords{trigcommand} and dispatches each for execution possibly by first splitting up each command into one or more contiguous segments.
\item Each segment will then be dispatched to an \dword{eb} process as described in Section~\ref{sec:fd-daq:design-event-builder} for execution.
\item Execution entails interpreting the trigger command segment and querying the appropriate \dword{fe} buffer interfaces to request data from the period of time. 
\item Requests and their replies may be sent synchronously, and
  replies are expected even if data has already been purged from the
  \dword{fe} buffer. (In that case, an error message would be sent out.)
\item The data received may then undergo processing and aggregation
  until finally it is saved to one or more files on the output storage
  system before it is transferred offline.
\end{itemize}
%\fixme{Add time order requirement on DS above}
%\fixme{Add return-empty-response requirement to buffer above}



\subsubsection{Event builder}
\label{sec:fd-daq:design-event-builder}
%\fixme{module-generic}
%\metainfo{Explain art\dword{daq}, handling of trigger commands by asynchronous, parallel queries to front end Data Selector (but take care not to duplicate between here and in the overview).}

The \dword{daq} back-end subsystem will provide the instances of the \dfirst{eb} most likely as \textit{artDAQ}~\cite{artdaq} components of the same name.
As described above, each EB instance will request selected data from
the appropriate \dfirst{daqfbi}, as addressed by a received trigger command. 
An \dword{eb} will aggregate the selected data and potentially apply processing and reduction (Section~\ref{sec:fd-daq:design-data-reduction}) as well as monitor its quality while in flight.
Finally it will record the resulting data to the output storage system.
The final output files shall use data schema and file formats as described in Section~\ref{sec:fd-daq:design-data-model}.



\subsubsection{Data Quality Monitoring}
\label{sec:fd-daq:design-data-quality}

Section~\ref{sec:daq:design:ccm:monitoring} describes a monitoring system for the \dword{daqccm} subsystems. 
Monitoring the quality of the information held in the detector data itself is critical to promptly responding to unexpected conditions and maximizing the quality of acquired data. 
A \dword{daq} \dfirst{dqm} 
will be developed (including necessary infrastructure, visualization,
and algorithms), which will process a subset of detector data in order
to provide prompt feedback to the detector operators. 
This system will be designed to allow it to evolve as the detector and its data is understood during commissioning and early operation and to cope with any evolution of detector conditions.
It is likely that many of the software modules developed by the
offline effort will be reused in the DQM.

\subsubsection{Output Buffer}

\metainfo{Describe the output buffer system, how it's shared with offline, data hand-off prototocols.  Responsibility scope (eg, who handles transfer to FNAL).}

The output buffer system is a hardware resource provided by \dword{daq} and used offline by \dword{dune}. 
It has two primary purposes. 
First, the buffer decouples the production of event data from the transfer of that data from the far site to archival storage.
Second, it provides local storage sufficient for uninterrupted \dword{daq} operation in the unlikely event that the connection between the \dword{fd} and the Internet is lost. 
A capacity of approximately \SI{0.5}{\peta\byte} is envisioned,
sufficient to service the entire \dword{fd}. This corresponds to one
week's worth of data at nominal data rates. Based on prior experience
of the consortium with unusual losses of 
connectivity at other far detector experiment sites, this is a
conservative storage capacity value.

\subsubsection{Data Model}
\label{sec:fd-daq:design-data-model}
\fixme{Add reference to live document. Georgia}

\metainfo{Describe the data model. 
  This isn't a strict schema just things like how various parts of the
  detector readout map to files, etc.}



\subsection{Control, Configuration, and Monitoring}
\label{sec:fd-daq:design-run-control}
% \fixme{module-generic}

% \metainfo{Describe run control and \dword{daq} operation monitoring. 
%   How it makes use of the Message Passing System. 
%   What is an ``epoch''.
%   How Epoch Change Requests lead to zero downtime reconfiguration. 
%   Public key based ``iron'' authentication between run control processes and the controlled processes. 
%   Describe how RC will configure partitioning, initiate reconfiguration, handle ``run'' changes, node discovery, configuration, logging, startup, shutdown and failures are handled. Describe how RC will support detector electronics configuration.}

The \dfirst{daqccm} subsystem, illustrated in Figure~\ref{fig:daq-ccm-subsys}, encompasses, as its name suggests, the software needed to control, configure, and monitor the rest of the \dword{daq}, as well as itself. 
It provides a center for the highly distributed \dword{daq} components, allowing them to be treated and managed as a single, coherent system. 
Figure~\ref{fig:daq-conceptual-overview} shows the central role of the \dword{daqccm} within the complete \dword{daq} system.
The following sections describe each of the three \dword{daqccm} subsystems. 

\begin{dunefigure}{fig:daq-ccm-subsys}{Main interaction among the three \dword{daqccm} subsystems.}
  \includegraphics[width=0.8\textwidth]{daq-ccm-subsys.pdf}
\end{dunefigure}

\subsubsection{Control}
\label{sec:daq:design:ccm:control}


The \dword{daq} control subsystem actively manages \dword{daq} software process lifetimes, asserts access control policies, executes commands, initiates configuration changes, detects and handles exceptions, and provides an interface for human operators.

% \fixme{I (bv) redrew this from Giovanna's to get vector PDF and match \dword{dune} color palette.  I made two content changes: 1: move UI out of partition as it can't both initiate a partition and be inside it and likely will allow for creation/viewing of more than just one partition.  2: I had to guess on how garbage collection might go and so addded a line from RM to PM. }

\begin{dunefigure}{fig:daq-ccm-control}{Roles and services that compose the \dword{daq} control subsystem.}
  \includegraphics[width=0.8\textwidth]{daq-ccm-control.pdf}
\end{dunefigure}

The control subsystem comprises a number of functional blocks of either global or partition scope as illustrated in Figure~\ref{fig:daq-ccm-control}. 
In the figure, ``partition'' refers to a logical segmentation of the \dword{daq} where each segment operates to some extent independently from others. 
The segmentation applies to the portion of detector electronics that provides input to the partition's \dword{daqdsn} and readout. 
The largest partition %would be 
is that of one \dword{detmodule}. 
The functional blocks in the figure represent one or more semi-autonomous agents, each with defined roles, capabilities, and access. 
Although drawn as single blocks, they typically are implemented as multiple peer agents to assure redundancy and fail-over. 
The blocks at partition scope are first described.

\begin{itemize}
\item Partition naming service provides \dword{daqdispre} for the components of a \dword{daqpart}. 
  That is, it allows one component to be made aware of the creation and continued operation of other components (discovery) or that others have recently become unresponsive (presence). 

\item Run control provides a central director;  its creation is the first step in initiating a \dword{daqpart}. 
  The \dword{rc} accepts, interprets, and validates input commands that may come either from a human via a user interface, or from other blocks (expert systems). 
  The commands describe a desired state of the \dword{daqpart}. 
  \dword{rc} will query other blocks to validate commands and then execute the commands by allocating processes through process management. 
  Once successfully allocated, their lifetimes are managed by \dword{rc}. 
  Throughout its lifetime, \dword{rc} will reconfigure an existing process, destroy it, or allocate additional processes. 

\item Supervisor provides a locus of expert system automation. 
  This block is instantiated by the \dword{rc} in order to augment or facilitate human commands with automated ones. 
  For example, it is expected that some set of commonly occurring exceptions or errors will be cataloged as the operators gain experience with the DAQ.   Where possible, appropriate responses to these will be codified in the form of \dword{rc} commands.  The supervisor then will be charged with issuing these commands when one of the known exceptions are raised.

\end{itemize}

Global scope controls \dword{daq} components for all \dwords{daqpart} across all \dwords{detmodule}.  It consists of the following blocks:


\begin{itemize}
\item Global naming service aggregates the \dword{daqdispre} information across partitions.  

\item Process management allocates and may reclaim sets of processes on behalf of a requesting component (specifically \dword{rc} and resource management). 
  Process management only allocates processes if their requester has appropriate access privileges as determined by access management and if resource management determines sufficient resources exist. 
  
\item Resource management determines whether any process allocation can proceed.  It also enacts process garbage collection to reap any processes which have outlived a controlled shutdown command.

  
\item Access management is responsible for providing authentication and authorization for all \dword{daq} functions which require some level of access control. 

\end{itemize}

The last block in Figure~\ref{fig:daq-ccm-control} represents a number of different applications which provide a Human-Machine Interface (UI) to the control subsystem.
At least one UI will be developed to allow a trained operator to construct and issue commands required to initiate, configure, potentially reconfigure, and finally terminate \dwords{daqpart}. 
UIs may enact access control in issuing commands but this does not replace access control inside the RC when accepting commands.
Any UI used for nominal operation or monitoring of the DUNE FD DAQ (``shift work'') will be usable remotely from any DUNE institution and from a variety of personal computing platforms.  
Additional UI elements will be developed as described in sections~\ref{sec:daq:design:ccm:configuration} and~\ref{sec:daq:design:ccm:monitoring}.


\subsubsection{Configuration}
\label{sec:daq:design:ccm:configuration}

The \dword{daq} configuration subsystem provides persistent data storage for all historic, current, and future configuration information applicable to the \dword{daq}.
It provides a singular point (via high-availability, redundant services) for the allocation of unique and monotonically increasing \dwords{daqrunnum}.
The configuration data stores operate in an ``insert-only'' mode so that no prior information may be overwritten. 
The following types of information will be managed:

\begin{itemize}

\item Partition structure contains descriptions of the multiplicity and connectivity of \dword{daq} components for any partition.   Eg, this describes the overall connectivity of ``nodes'' in the DAQ ``graph''.

\item Component parameters comprise configuration information associated with any given \dword{daq} component.  Eg, thresholds in a trigger candidate processor or gains and peaking times of a FE amplifier.

\item Run number provides a monotonically increasing sequence of \dwords{daqrunnum} that are allocated upon request to assure that each is unique.

\item Partition instances associate a \dword{daqrunnum} and the set of component parameters that were used to initiate a \dword{daqpart} or which are used to reconfigure an existing \dword{daqpart}.  

\item Constraints define rules that must be held true by resource management servicing requests for process allocations.  This information store also includes which constraints were used by resource management over time.
\end{itemize}


Access to configuration information is via a service that hides the choice of storage technology from any client queries.
This interface will also be used by editors run by human operators or by  generators run as part of an expert system.

\subsubsection{Monitoring}
\label{sec:daq:design:ccm:monitoring}

The \dword{daq} monitoring subsystem will help both humans and expert systems in detecting, diagnosing, and correcting anomalous activity, observing intended operation, and providing a historical record.
This subsystem will accept required information produced by any \dword{daq} component (here called status).

The precise implementation of the production, acceptance, store, post-processing, querying, and visualization of monitored status requires additional work. 
However, a publish-subscribe (PUB/SUB) network communication pattern is expected to be adopted for transport of monitoring messages. 
This will decouple production and consumption and facilitate development of a variety of status viewers, expert systems, debugging tools, etc. 
The types of messages include but are not limited to the following:

\begin{itemize}
\item Common to all will be a ``header'' holding a message type indicator, a sender address and the associated detector data time and the recent host computer time.
 
\item Logging messages add an importance label (e.g., debug, info, warning, error) and a succinct, human-readable information string providing an explanation of what occurred.
  
\item Metrics will provide structured data carrying specific information about predefined aspects of the sender. 
  This is similar to logging, but the messages support automated consumption and reaction by expert systems.  

\item Quality messages summarize information derived from the detector data (e.g., from waveforms) or its metadata (e.g., timestamps, error codes) while that data is ``in flight'' through the \dword{daq}.

\end{itemize}

In general, the DAQ will retain all status records at least long enough to allow for any offline data quality validation procedures to be performed. 
However, some status feeds may be processed prior to storage if their raw form requires prohibitive amount of storage. 
In particular, the quality stream data rate may be too substantial for long term storage. 
Such streams will be summarized into histograms or other statistical representations prior to storing for longer term use.

In addition to this \dword{daq} \dword{daqccm} monitoring subsystem, a separate system must be used to monitor in depth the quality of the detector data content itself. 
See Section~\ref{sec:fd-daq:design-data-quality} for the description of this data quality monitoring system.

\subsection{Inter-Process Communication}
\label{sec:daq:design-ipc}

The DUNE FD DAQ is an asynchronous, parallel distributed data processing system. 
It is composed of many independent processes which ingest and produce messages. 
The mechanisms of such message passing are generally called \dword{ipc}. 
Referring to Figure~\ref{fig:fd-daq:layout}, IPC is used for both in-band detector data flow between Upstream DAQ and back-end Event Builders and for out-of-band messages as part of Control, Configuration and Monitoring.  The IPC used by the Data Selection spans both descriptions as it passes derivations of a subset of detector data (trigger primitives, candidates) and culminates in a source of out-of-band message (trigger commands) to direct the readout by Event Builder and other components of detector data that is held in the Upstream DAQ buffers.

The ZeroMQ~\cite{zeromq} smart socket library is the basis of a system being developed and evaluated for parts of both in-band and out-of-band IPC. 
As part of the CCM, this include the issuing of control and reconfiguration commands to and receiving of monitoring messages from essentially all DAQ components. 
As part of Data Selection, this includes the transfer of trigger primitive, candidate and command messages. 
In the Upstream DAQ this includes the Buffer Interface Service which provides access to the Upstream DAQ primary buffers for queries by Event Builder and other components. 
IPC must be implemented broadly across many DAQ systems and ZeroMQ allows their problems to be solved in common, shared software.  As CCM has the most complex IPC needs, this work is organizationally considered part of this system.

One ZeroMQ facility in particular is worth additional description. 
Zyre will provide a decentralized implementation \dword{daqdispre} (see Section~\ref{sec:daq:design:ccm:control}). 
It uses UDP broadcast beacons and a connected followup to provide peer discovery on the local network. 
A heartbeat mechanism provides presence so that a compent may discover when a peer has become unresponsive.  
As Zyre uses the network itself, there is no central point of failure. 
The network provides the naming service described above. 
Zyre also allows for more centralized, connection oriented, discovery which can be used to allow the process to span networks.

As described in~\ref{sec:fd-daq:design-backend}, \textit{artDAQ}~\cite{artdaq} utilizes IPC between its back-end components. 
It has been well tested with ProtoDUNE and other experiments. 
ArtDAQ may be used for some portions of the IPC described above. 
For example, if the Buffer Interface Service is implemented as an artDAQ Board Reader it would necessarily use artDAQ IPC. 
This would limit the types of clients that could query for data in the buffers to be artDAQ modules. 
Understanding how to optimally select an IPC for such parts of the DAQ connection graph is an area of ongoing R\&D effort.

\subsection{Timing and Synchronization}
\label{sec:sp-daq:design-timing}
%\fixme{single-phase module}
%\fixme{Is it indeed still single-phase specific?}
%\metainfo{Hardware, consumers, links.}

All components of the \dword{fd} use clocks derived from a single
\dfirst{gps} disciplined source, and all module components are
synchronized to a common \SI{62.5}{MHz} clock. 
To make full use of the information from the \dword{pds}, the common clock must be aligned within a single detector %unit 
module with an accuracy of \bigo{\SI{1}{\nano\second}}. 
For a common trigger for a \dword{snb} between modules, the timing must have an accuracy of \bigo{\SI{1}{\milli\second}}.
However, a tighter constraint is the need to calibrate the common clock to universal time derived from \dword{gps} so the \dword{daqdsn} algorithm can be adjusted inside an accelerator spill, which again requires an absolute accuracy of \bigo{\SI{1}{\micro\second}}.

The \dword{dune} \dword{fd} uses a version of the \dword{protodune} timing
system, where a design principle is to transmit synchronization messages over
a serial data stream with the clock embedded in the data. The format
is described in \citedocdb{1651}. The timing system design is
described in detail in \citedocdb{11233}.

Central to the timing system are four types of signals:
\begin{itemize}
\item a \SI{10}{\mega\hertz} reference used to discipline a stable master clock,
\item a \dfirst{pps} from the GPS,
\item a \dword{ntp} signal providing an absolute time for each \dword{pps}, and
\item an \dfirst{irig} time code signal
  used to set the timing system 64-bit time stamp.
\end{itemize}
%The timing system associates the \dword{gps} time to its master clock by using the latter to stamp the arrival time of the \dword{pps} of the former. To provide an absolute time to the nearest second the information from \dword{ntp} is used.
%\fixme{This may no longer be correct.  Will NTP be used and the host computer clock consulted for the absolute time?  Will PTP used in its stead?  How does IRIG fit in?  Finally, this sentence, however it needs to be written, is redundant with the itemized list.}

The timing system synchronization codes are distributed to the \dword{daq} readout components in the \dfirst{cuc} and the readout components on the cryostat via single mode fibers and passive splitters/combiners.
All custom electronic components of the timing system are contained in two \dword{utca} shelves; at any time, one is active while the other serves as a hot spare.
The \SI{10}{MHz} reference clock and the \dword{pps} signal are received through a single-width \dword{amc} at the center of the \dword{utca} shelf.
This master timing \dword{amc} is a custom board and produces the timing system signals, encoding them onto a serial data stream.
This serial data stream is distributed over a backplane to a number of fanout \dwords{amc}.
The fanout \dword{amc} is an off-the-self board with two custom \dwords{fmc}.
Each \dword{fmc} has four \dword{sfp} cages where fibers connect the timing system to each detector component (e.g., \dword{apa}) or where direct attach cables connect to other systems in the \dword{cuc}.

To provide redundancy, two independent GPS systems are used,
one with an antenna at the surface of the Ross shaft, and the other
with an antenna at the surface of the Yates shaft. Signals from either
GPS are fed through optical single mode fibers to the \dword{cuc}, where
either GPS signal can act as a hot spare while the other is active. 
Differential delays between these two paths are resolved by a second pair of fibers, one running back from the timing system to each antenna.


\section{Design Validation and Development Plans}
\label{sec:fd-daq:validation}

The following strategy will be followed in order to validate and
develop the \dword{dune} \dword{fd} \dword{daq} design:
\begin{itemize}
\item Use of \dword{protodune} as a design demonstration and
  development platform. 
\item Use of vertical slice teststands for further development and testing of
  individual \dword{daq} subsystems and for key aspects of the
  overall \dword{daq}
\item Use of horizontal slice tests to demonstrate scaling the design
  where the multiplicity of components in subsystem layers is important.
\item Use of \dword{fd} MonteCarlo simulations and emulations in order
  to augment actual hardware demonstrations at \dword{protodune} and teststands.
\item Benefit from developments and measurements from other ongoing
  LArTPC experiments, including MicroBooNE, SBND, and ICARUS.
\end{itemize}

This strategy reflects the current \dword{daq} project schedule,
provided in Section~\ref{sec:fd-daq:schedule}, which
comprises several phases, including an intense development phase
through 2020 that culminates in an engineering design
review (EDR) in Q1 of 2021. At this milestone, the system design will be
finalized and demonstrated to be capable of meeting the requirements of the
final \dword{daq} system. After the development phase, a
pre-production phase will begin and will end with a production readiness
review (PRR). By then, final designs of all components
will be complete.

The following subsections summarize past, ongoing, and planned
development and validation studies and identify how anticipated outcomes
will be used to finalize the \dword{daq} design.

\subsection{Design Validation and Development at ProtoDUNE and Other
  LArTPC Experiments}

\label{sec:fd-daq:protodune}
\metainfo{Here we describe protodune, write what similarities and
  differences there are between \dword{protodune} and \dword{dune}
  \dword{daq} designs. Mention other related efforts at other experiments.}

The \dword{fd} \dword{daq} consortium constructed and operated the \dword{daq} system for
\dword{protodune}, which included %facilitated two incarnations of 
two \dword{daq} readout 
architectures, one based on \dword{felix}, develped by ATLAS \cite{xx}, and the other on \dword{rce}, developed at
SLAC \cite{xx}. \dword{daq} design and construction for
\dword{protodune} began in Q3 of 2016, and the system became operational at the start of the beam data run %when \dword{protodune} beam running began 
in Q4 of 2018. The detector is
continuing to run as of the writing of this document, recording cosmic
ray activity, and %facilitating 
providing further input for \dword{daq} development toward
\dword{dune}. 

Figure~\ref{fig:daq-protodune} depicts the \dword{protodune} \dword{daq} system.  The \dword{daq} is split %among %two readout architecture implementations, one involving 
between the  \dword{felix}  and \dword{rce} implementations. The two architectures share the same back-end and
timing and trigger systems. 
Neither of these tested % readout 
architectures %( \dword{felix}  or RCE) 
exclusively represents the
baseline design for the DUNE \dword{fd}. Instead, each %readout architecture
qualitatively maps into one of
two data processing approaches: one in which the data is processed
exclusively in
custom-designed \dword{fpga}, and the other  in which the data is processed primarily in commodity
CPUs. The baseline system for a %DUNE FD 
\dword{detmodule} instead
merges elements of the two approaches. Specifically, it uses  \dword{felix}  as
the hardware platform for data receiving and handling, and an
\dword{fpga}-based co-processor (analogous to the \dword{rce} platform) that
interfaces with  \dword{felix}  to provide additional, dedicated data processing
resources. In that sense,\dword{protodune} has provided demonstration of the
 \dword{felix}  platform as \dword{fe} readout data receiver, and
demonstration of \dword{fpga}-based data reduction for TPC.

\fixme{Placeholder. Should show felix and rce architectures}
\begin{dunefigure}{fig:daq-protodune}{The \dword{protodune} \dword{daq} system.}
%  \includegraphics[width=0.8\textwidth]{daq-protodune.pdf}
\end{dunefigure}

Besides overall readout architecture,
the \dword{protodune} and
\dword{dune} \dwords{daq} exhibit two key differences. 
First, the \dword{protodune} \dword{daq} is externally
triggered (and at a trigger rate 
over an order of
magnitude higher than that anticipated for \dword{dune}). Because of
this, the
\dword{protodune} \dword{daq} 
does not facilitate online data
processing from the TPC or \dword{pd} systems for self-triggering. 
Second, the \dword{protodune} 
system sits at the surface with a much higher
data occupancy 
due to cosmic ray activity.
Overcoming the first key difference to demonstrate \dword{daqdsn} capability for the \dword{fd} \dword{daq} design is a main component of future \dword{daq} development plans, described in Section~\ref{xx}.

Continuous self-tirggering of the detector is also new with respect to
other ongoing or planned near-term LArTPC experiments, including
MicroBooNE, SBND, and ICARUS. Both MicroBooNE and ICARUS have demonstrated
self-triggering in coincidence with external gates, which effectively
limits both data and trigger rates, and is not a viable solution for
DUNE's off-beam physics program, as it would effectively limit
exposure and therefore physics sensitivity. %On the other hand, ICARUS
%has ...
MicroBooNE has demonstrated successful continuous readout of a LArTPC,
via use of dynamic and fixed-baseline zero-supression implemented in
firmware for both TPC and PDS readout. SBND, which utilizes the same readout
system as MicroBooNE, will investigate trigger primitive generation
and self-triggering based on TPC information in the timescale of 2021-2023.

\fixme{Add references.}

\subsection{ProtoDUNE Outcomes}

Despite being designed for a much more limited scope than DUNE, the %already
successful operation of the \dword{protodune} \dword{daq} has provided several key
demonstrations for \dword{dune} \dword{daq}, in particular data flow
architecture, run configuration and control, and back-end
functionality. % In the following subsection we discuss important lessons learned.

More specifically, \dword{protodune} has demonstrated 
\begin{itemize}
\item Upstream DAQ: successful
front-end readout hardware and data flow functionality for the readout
of two out of the six APAs employed in protoDUNE. This was achieved
with two TPC RU's, without co-processor boards, and only one APA read
out per FELIX board. The \dword{dune} DAQ design will ultimately
accommodate readout of two APAs per FELIX board. In addition to data
flow functionality, ProtoDUNE Front-end readout also demonstrates interface to
front-end electronics, and scalability to DUNE. It also supports
host server requirements and specifications. Finally, it serves as
platform for further development involving co-processor implementation
and data selection aspects.
\item Back-end DAQ, CCM and Software Infrastructure:
 successful back-end DAQ implementation, including event builder
  farm and disk buffering, as well as CCM. This has allowed the
  development and exercising of system partitioning, and provides a
  basis for scalability to DUNE. ProtoDUNE also serves as
  a platform for further system development, in particular in the
  areas of CCM and IPC and for the data flow orchestrator part of the
  back-end DAQ.
\item Data Selection and Timing: successful operation of the timing
  distribution system, and external trigger distribution to the
  front-end readout. Although protoDUNE was externally triggered, the
  system serves as a development platform for data-driven data selection.
\end{itemize}

Besides demonstrating end-to-end data flow, an important outcome of
protodune daq has been the delineation of
interfaces, i.e.~understanding the exact \dword{daq} scope and the interfaces to TPC, \dword{pds}, and offline. The use of commercial off-the-shelf solutions
where possible, and leverage of professional support from CERN IT 
substantially expedited the development and success of the project, as
did the strong on-site presence of experts from within the consortium during early installation and
commissioning. 

Outcomes specific to \dword{protodune} subsystems are discussed in
greater detail in \cite{talkfromCDR}. Figure~\ref{fig:sp-daq:protodunedaqpic} shows the 
ProtoDUNE DAQ hardware as used during beam data taking.

\begin{dunefigure}{fig:daq-rates}{ProtoDUNE DAQ hardware as used during beam data taking. \label{fig:sp-daq:protodunedaqpic}}
  \includegraphics[width=0.6\textwidth]{DAQ_hardware_NP04.pdf}
\end{dunefigure}


\subsection{Ongoing Development}
\label{sec:sp-daq:design-validation}

%\fixme{this section and onward still needs work}

Subsystem development is ongoing at ProtoDUNE at the time of the
writing of this document. A detailed schedule for 2019 is available
in \cite{referencedocdbfromgiovanna}. Major development plan milestones are:
\begin{itemize}
\item optimization and tuning of the Front-end readout
\item optimization and tuning of the artdaq based dataflow software
\item enhancement of monitoring and troubleshooting capabilities
\item introduction of CPU-based hit finding (necessary for PDS readout)
\item introduction of FPGA-based hit finding (for TPC readout)
\item implementation of online software data selection beyond trigger
primitive stage (introduction of trigger candidate generation, and
trigger command generation), and tests on well identified interaction
topologies (e.g. long horizontal tracks, or Michel electrons from muon decay)
\item integration of online triger command and modified data flow to event
builder to facilitate self-triggering of detector
\item implementation of extended fpga based front-end functionality
(e.g. compression)
\item prototyping of fake SNB data flow in front-end and back-end
\end{itemize}

Below, we focus on ongoing developments related to upstream DAQ,
data selection, which is a key challenge for DUNE and new with respect
to ProtoDUNE (and other existing or planned LArTPC detectors),
and as IPC, which is common across multiple DAQ subsystems.


\subsubsection{Upstream DAQ Development}

%The detector readout for DUNE will be based Front-End Link EXchange (FELIX)[?] system, originally developed for the ATLAS experiment DAQ upgrades.
The use of FELIX as the front-end readout technology for DUNE was
successfully prototyped at ProtoDUNE, initially for the readout of one
APA. In ProtoDUNE, FELIX allows streaming of data coming over multiple 10
Gbps optical point-to-point links into commercial host memory and,
from there, storing, dispatching or processing of the data via
software. 

In ProtoDUNE, a single APA,  FELIX-based readout consists of two servers,
with a point-to-point 100 Gbps network connection.
The FELIX I/O card
interfaces with its host PC through 16-lane PCIe Gen3 (theoretical bandwidth of \SI{16}{GB/s}).
It transfers
the incoming WIB data directly into the host PC memory using
continuous DMA transfer. The FELIX host PC runs a software process
which publishes the data to any client subscribing to it, based on
link identifiers. The clients of this application are the FELIX
BoardReader processes, which are part of the artdaq framework. In
order to sustain the data rate, modest modifications of the firmware
(increased block size and chunk merging) and software (scatter-gather
publisher) were carried out specifically for ProtoDUNE: each FELIX
host receives and publishes data at ~75 Gbps. The BoardReader hosts
are equipped with embedded Intel QuickAssist (QAT) \cite{qat} technology for
hardware accelerated data compression. The ProtoDUNE implementation of
the FELIX front-end readout is
shown in Figure~\ref{fig:sp-daq:felix-pd-impl}.

In DUNE only a very small fraction of the data received via the FELIX
system will ever need to leave the host: thus it is not required to
implement very high speed networked data dispatching. On the other
hand it may be interesting to carry out data processing and buffering
on the host: while this is not the baseline design for DUNE, R\&D is
ongoing at hand of ProtoDUNE-SP to evaluate the feasibility of
implementing hit finding, data buffering and possibly even local
trigger candidates generation on the FELIX host. 


\begin{dunefigure}{fig:sp-daq:felix-pd-impl}{The topology of the FELIX based
    upstream DAQ of ProtoDUNE (from~\cite{pdsp-felix}). The FELIX host servers are publishing the data from the WIBs over 100Gb network interfaces. The BoardReader hosts are carrying out trigger matching, data compression and forwarding of fragments to the event builder.}
  \includegraphics[width=0.9\textwidth]{daq-protodune-server-topology-2x2.pdf}
\end{dunefigure}

The DAQ team is investing substantial effort into the introduction of
a triggering chain based on the TPC data into ProtoDUNE, which will
allow to carry out pre-design prototyping studies of the complete flow
of data of the DUNE DAQ.  
The FELIX based readout system will be adapted to support the
different studies, from co-processor based data handling in firmware
(including trigger primitive generation, compression, and buffering) to software-based processing, on a single
server. Benchmarking and optimization of the FELIX firmware and
software will also continue, with the aim of further compacting the
readout by supporting two APAs on a single server. 

\subsubsection{Co-processor Development}

Upstream DAQ development efforts at protoDUNE include a parallel test 
platform for trigger
primitive generation, compression, and buffering firmware validation for
the co-processor board.  The platform for these tests will initially
use a Xilinx ZCU102 development board, which will later be replaced
with a ``FELIX Demonstrator'' (so called because it demonstrates use
of Xilinx Virtex-7 Ultrascale+ FPGA and Gen-4 PCIe) which has a FMC+
connector which will be connected to a PBM. Passive optical splitters
will be inserted into the fiber path downstream of the WIBs, providing
duplicate data inputs for the test hardware, without disrupting the
main readout path of ProtoDUNE. Tests using the development board will
first focus on generation of trigger primitives, which will be read
out over the network via IPBus\cite{ref:ipbus}. The ZCU102 board
includes 512MBytes DDR4 RAM connected to the FPGA programmable logic,
as well as a PCIe x4 socket which will host an NVME SSD on an adapter,
which will allow tests of buffering and compression of readout
data. 

Tests using the development board will focus on functionality
rather than data throughput. However, the tests will provide estimates
of firmware resources that can be scaled up to the full system. Tests
using the FELIX Demonstrator and PBM at ProtoDUNE will focus on
scaling the functional tests performed using the ZCU102, to a full
demonstration of trigger and readout functionality for a full APA. In
addition, this platform will facilitate integration with the prototype
data selection and back-end DAQ subsystems
at ProtoDUNE. 


\subsubsection{Data Selection Development}

\begin{dunefigure}{fig:daq-cpu-hf-speed}{CPU core-time required to find primitives in simulated signal and noise data.  10 CPU cores are required to keep up with live ProtoDUNE data while performing both data reformatting and trigger primitive selection.}
  \includegraphics[width=0.7\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-cpu-speed.pdf}
\end{dunefigure}

\begin{dunefigure}{fig:daq-hitfinder}{Illustration of the trigger primitives found by the CPU algorithm in one nominal readout of one APA in ProtoDUNE-SP data.  Although the data was ultimately recorded due to trigger, the algorithm ran in full-stream mode.}
    \includegraphics[width=0.45\textwidth]{daq-hitfinder-waveforms.png}%
    \includegraphics[width=0.45\textwidth]{daq-hitfinder-primitives.png}
\end{dunefigure}

\begin{dunefigure}{fig:daq-tp-rates}{Trigger primitive rates in ProtoDUNE-SP as a function of threshold in four categories: all data (top left),  after removal of particularly noisy channels (top right), with HV off so no contribution to signal (bottom left) and HV off and noisy channels excluded (bottom right).}
  \begin{minipage}[b]{0.5\linewidth}
    \begin{center}
      \includegraphics[page=1,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}

      \includegraphics[page=2,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}
    \end{center}
  \end{minipage}%
  \begin{minipage}[b]{0.5\linewidth}
    \begin{center}
      \includegraphics[page=3,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}

      \includegraphics[page=4,width=0.8\textwidth,clip,trim=4cm 16cm 4cm 2cm]{daq-primitive-rates-thresholds.pdf}
    \end{center}
  \end{minipage}

\end{dunefigure}


During early stages of design, significant effort has been dedicated on trigger primitive generation
through MonteCarlo simulations. Specifically, charge collection efficiency and fake rates
due to noise and radiologicals have been studied as a function of
hitthreshold with MonteCarlo, demonstrating that requirements can be
met, given sufficiently low electronics noise levels and radiological
rates \cite{ref}. Ongoing efforts within DUNE's Radiologicals Task force
aim to validate or provide more accurate background predictions, upon
which this performance will be validated. In addition, offline emulations
of CPU trigger primitive generation on CPU (4 cores) have been carried
out, demonstrating the ability of software algorithms in CPU to keep
up with expected raw data rates, as shown in Figure~\ref{fig:daq-cpu-hf-speed}. 
Following the commissioning of ProtoDUNE, full-stream, single-APA,
online data reformatting and trigger primitive generation on CPU (10 cores) was successfully
demonstrated at ProtoDUNE. Trigger primitive rates were measured at
ProtoDUNE in situ. Effort on understanding and removing contribution
from cosmics/cosmogenics and (known) noisy channels is ongoing.
These results are summarized in Figure~\ref{fig:daq-tp-rates}

\begin{dunefigure}{fig:daq-tc-eff-vis}{Efficiency for forming trigger candidates as input trigger primitives from two algorithms, online (blue) and offline (red).}
  \includegraphics[width=0.7\textwidth]{Electron_Efficiency_Comparison.pdf}
\end{dunefigure}

\begin{dunefigure}{fig:daq-tc-eff-true}{All-inclusive, integrated efficiency for forming trigger candidates from ionization activity from beam $\nu_e$ (left) and beam $\nu_\mu$ (right) interactions at or above a given true neutrino energy.  The trigger candidate algorithm used is the offline version, see Figure~\ref{fig:daq-tc-eff-vis} for comparison with online version.}
  \includegraphics[width=0.45\textwidth]{Integrated_Nu_e_Efficiency_MCC10.pdf}%
  \includegraphics[width=0.45\textwidth]{Integrated_Nu_mu_Efficiency_MCC10.pdf}
\end{dunefigure}


Trigger candidate generation, building on trigger primitives
information and considering integral ADC and trigger primitive
proximity by channel and time (in 50 microseconds) space,
has also been studied with Monte Carlo
simulations \cite{riveralast}. Trigger candidates with sufficient
total integral ADC can be accepted
to generate corresponding Trigger Commands for localized high energy
activity, such as for beam, atmospheric neutrinos, baryon number
violating signatures, and cosmics. Simulation studies demonstrate that
this scheme meets efficiency requirements for localized high energy
triggers, as shown in Figure~\ref{fig:daq-tc-eff}. Specifically,
simulations demonstrate that $>99$\% efficiency is achievable for
$>100$ MeV visible energy, and that the effective threshold for
localized triggers for the system is at $\sim$10 MeV. 

Low-energy trigger candidates furthermore can serve as input to the
SNB trigger. Simulation demonstrates that the trigger candidate
efficiency for any individual SN neutrino interaction is on the order
of 20-30\%. Simulations have further demonstrated that a
multiplicity-based SNB trigger decision which integrates low-energy
trigger candidates over an up to 10 seconds
integration window yields high ($>90$\%) galactic coverage while
keeping fake SNB trigger rates to one per month, per system
requirements. An energy-weighted multiplicity count scheme could be
applied to further increase efficiency and minimize background.
The dominant contributor to fake SNB triggers is
radiological backgrounds from neutrons, followed by Radon. It is
crucial to continue working closely with the Radiological Task force
to validate radiological simulation assumptions.

Given that simulation studies support requirements and rate
assumptions, the protoDUNE demonstration of ability to keep up with
rates from 1/25$^{th}$ the size of a single DUNE FD SP module, for trigger
rates up to 40 Hz and 3 ms readout window 
allows confident scaling of the protoDUNE back-end DAQ subsystem to
that of DUNE.

In the case of the high level filter, the DAQ consortium is exploring the
use of machine learning techniques, specifically image classification
with the use of convolutional neural networks (CNNs) on GPUs, as a way to
classify and down-select individual sections of TPC channel vs. time
(``frames''), with extent of one APA's worth of collection plane
channels by one drift length (2.25 ms, or, 4500 samples). CNNs have
been trained on Monte Carlo simulations of frames with each of
the following off-beam event topologies: 
atmospheric neutrino interactions, baryon number violating
interactions (proton decay or neutron-antineutron oscillation), cosmic
ray interactions, supernova neutrino interactions, or no interactions at all; all
with radiological and noise background included in the
simulations. Preliminary studies show that a CNN can be sucessfully trained classify any
given input frame as one of three categories: empty, containing a supernova neutrino
interaction, or containing a high-energy (atmospheric neutrino, baryon
number violating, or cosmic ray) interaction. Specifically, empty frames can be
rejected with an efficiency of $>$99\%, while frames containing or
partially containing 
a supernova neutrino, an atmospheric/cosmic interaction, or a baryon
number-violating interaction can be preferentially selected with efficiency
$>$88\%, $>$92\%, or $>$99\%, respectively  \cite{docdb11311}. Such
filter could potentially be applied to reduce the event record size by
more than two orders of magnitude. Inference latency is under study to validate limiting event
record throughput (from the Event Builder) through such filtering
stage, for a reasonably sized GPU cluster. 

% \subsubsubsection{Ongoing  \dword{felix}  Throughput Demonstration at \dword{protodune}}
% \label{sec:sp-daq:validation-pdune-felix}
% %\fixme{single-phase module}

% %\metainfo{Describe how the  \dword{felix}  \dword{daq} at \dword{protodune} demonstrates a
%  %  \dword{felix} +CPU approach. 
%  % Describe the elements that are same or similar (full-rate to host
%  % RAM buffer) and different (higher-rate but external trigger).}

% The  \dword{felix}  \dword{daq} at \dword{protodune} partly demonstrates the front-end readout scheme for
% the PDS system, as well as the  \dword{felix} +CPU readout approach for the
% TPC. The elements of the \dword{protodune}  \dword{felix}  \dword{daq}, which are the
% same in \dword{dune}, have already demonstrated the
% reception of raw data at full rate from a single \dword{apa} to a 
%  \dword{felix}  card and  \dword{felix}  host RAM buffer; upon receiving an external trigger, the
% data is propagated to the back-end system. The back-end system
% operates similarly to \dword{dune} itself. What differs in the final \dword{dune}
% implementation is that neither in the host CPU or
% GPU, nor in the added \dword{fpga} functionality does data processing  trigger primitive  generation and subsequent processing
% of \dwords{trigprimitive} through the \dword{daqdsn} system. Another
% significant difference is the much higher rate of data propagation from the
% host RAM to the back-end system in \dword{protodune} than anticipated for the \dword{fd}. Future development
% will concentrate on data processing and \dword{daqdsn}.  This is described in more detail in Section~\ref{sec:sp-daq:validation-pd-demonstrator}.

% \subsubsubsection{Ongoing RCE Throughput Demonstration at \dword{protodune}}
% \label{sec:sp-daq:validation-pdune-rce}
% %\fixme{single-phase module}

% The RCE \dword{daq} in \dword{protodune} demonstrates part of the readout scheme for
% the  \dword{felix} +FPGA readout approach for the TPC. In particular, it
% shows that real-time TPC data processing for lossy
% and lossless compression can be facilitated in FPGA, achieving
% compression factors consistent with those expected based on observed
% \dword{protodune} noise levels. In the future, the system will be used
% to demonstrate how \dwords{trigprimitive} are generated in FPGA as
% described in Section~\ref{sec:sp-daq:validation-pd-demonstrator}.

% \subsubsubsection{Validation of Trigger Primitives in Software}
% \label{sec:sp-daq:validation-software-trigger-primitives}
% %\fixme{single-phase module}

% Generating \dwords{trigprimitive} in CPU or GPU software has not
% yet been demonstrated \textit{in situ} in \dword{protodune}, but it has been
% demonstrated in simulations, using real data from \dword{protodune}, on a
% server with specs similar to those 
% of the  \dword{felix}  host server at \dword{protodune}.

% The algorithm is described in detail in \cite{docid-11236}; a Monte Carlo test has demonstrated the algorithm can do real-time processing of one APAs worth
% of collection wire data. This test was performed on a Xeon Gold 6140
% system, where four (4) threads (cores) were sufficient to keep
% up with the detector data rate. The algorithm begins with pedestal removal by
% finding the mean of any given wire waveform baseline using a frugal
% streaming approach. A look-ahead approach was used to stop
% updating the pedestal when it became clear that a potential signal
% had been found. Noise filtering in the form of a 7-tap low-pass FIR
% filter will be applied as an intermediate step
% between pedestal subtraction and hit finding. The hit finding
% algorithm is a simple threshold-discriminator. Using Monte Carlo, a hit
% threshold of 10 \dword{adc} counts, corresponding to 1\/4 MIP for deposits originating at the cathode, yields hit primitives dominated by
% $^{39}$Ar and maintains 100\% efficiency to MIP hits. This is robust against noise level 
% increases of 50\% above the default \dword{dune} Monte Carlo settings \cite{docid-11275}. 

% Tests of \dword{trigprimitive} generation on very early \dword{protodune} data have also been
% performed. Because of its surface
% location and the known noisy channels included in this study,
% \dword{protodune} \dword{trigprimitive} generation rates level off to a floor rate higher than a threshold
% of approximately 16 \dword{adc} counts. Although the total rates are much higher than
% anticipated for DUNE, the algorithm performance is promising and should improve as \dword{protodune} continues
% to run and improve its noise understanding.

% \subsubsubsection{Validation of Trigger Primitives in Firmware}
% \label{sec:sp-daq:validation-firmware-trigger-primitives}
% %\fixme{single-phase module}

% While \dword{trigprimitive} generation in firmware has not yet been
% demonstrated in the RCE system in \dword{protodune}, candidate
% algorithms are being developed and will be deployed in an \dword{fpga}
% for both \dword{protodune} and other demonstrators. 

% On the other hand, \microboone has been able to
% successfully implement dynamic baseline estimates and subtraction
% for region-of-interest hit finding in an \dword{fpga} \cite{NNN18}.
% %\metainfo{Succinctly describe algorithm, include physics and computing
% %  performance numbers.}


% \subsubsubsection{Validation of Hierarchical Trigger Layers}

% As described in Section~\ref{sec:sp-daq:design-selection-algs}, the \dfirst{daqdss} is structured as a layered hierarchy. 
% It must take in constant, high-rate data and output a minuscule rate that provides summary of activity in the data (trigger commands).
% To do this, it is structured in layers (primitive, candidate, command) and each layer requires certain multiplicity that spans multiple host computers communicating over the local high-speed but commodity network. 
% The algorithms, host computer multiplicity, the \dfirst{ipc} system and its network traffic must be demonstrated at scale.
% This will require development of the following elements:

% \begin{itemize}
% \item A model of \dword{trigprimitive} rates and will be developed that is parameterized by signal, background and noise.  This model will be informed by \dword{protodune} and simulation studies.  The model will include the data density per unit of front-end computing as informed by the \dword{trigprimitive} validation work described above.
% \item Likewise an initial model of trigger candidate rates will be developed and concurrently prototype trigger candidate algorithms will be developed and applied to \dwords{trigprimitive} produced by their validation work (again, described above).
% \item Trigger candidate message schema and IPC application-level protocol (described more below) for their transfer will be developed.
% \item Validation will involve implementing these models, prototype algorithms, schema, etc,  initially at a small-scale on a single high-core computer and using throttled localhost connectivity.
% \item The full-scope system will require \bigo{100} host computers, but contiguous branches of the hierarchy can and will be fully tested with \bigo{20} host computers. 
%   These hosts will support 10 Gbps networking (actual or throttled) to match the expected networking. 
%   In some cases higher bandwidth connectivity may be investigated if bottlenecks are discovered. 
% \end{itemize}

% \subsubsubsection{Validation of High Level Filter in Software}

\subsubsection{Prototype Inter-process Communication System}

A prototype of the \dword{ipc} mechanism described in Section~\ref{sec:daq:design-ipc} is currently under development and testing at \dword{protodune}.  Goals of this prototype include evaluating throughput and rate limitations, understanding message schema and application level protocols, prototyping CCM functionality including \dword{daqdispre}, investigating scaling and software complexity management as well as providing functional support for other tests of the DUNE FD DAQ design at \dword{protodune}.

Initial offline prototype tests have demonstrated ZeroMQ inter-thread transport is more than sufficient for use in the highest rate \dword{daq} context (input to \dword{trigprimitive} production).
Tests with small packets have demonstrated ZeroMQ's ability to handle the high packet rate of \dword{trigprimitive} or trigger candidate data over all three of the ZeroMQ transport mechanisms that are being considered for use (inter-thread, inter-process and network).  

ZeroMQ IPC has been successfully exercised at \dword{protodune} as part of a practical solution to transfer the full stream of trigger primitives out of one process and in to another. 
This tests the eventual leg between producers of trigger primitives and their consumers (trigger candidate processors). 
The merging of multiple, asynchronous trigger primitive streams into a single ordered stream which is properly prepared for input to trigger candidate processing has been developed and tested offline.  Performance testing and integration into the self-triggering prototype at \dword{protodune} is in progress.



% \subsubsection{Future \dword{protodune} \dword{daq} Demonstrator}
% \label{sec:sp-daq:validation-pd-demonstrator}

% A demonstration of the \dword{daq} system design based in \dword{protodune} is
% planned for 2020. The key components are the front-end readout, timing
% system, and \dword{daqdsn} systems. A full vertical slice through the
% timing and readout system will be constructed, with
% TPC and PDS detectors, as well as calibration systems. Although a full-scale
% demonstration of the \dword{daqdsn} system cannot be achieved with
% \dword{protodune}, the basic infrastructure will be demonstrated and
% used to self-trigger \dword{protodune}.

% The high-level goals will be to demonstrate and validate
% integration of \dword{fe} with TPC and PDS electronics, 
% data throughput for a single \dword{fe} instance 
% readout data processing for a single \dword{fe} instance 
% \dword{trigprimitive} generation for a single front-end instance 
% distribution of timing and control signals to detector \dwords{fe} 
% data selection infrastructure and self-triggering  and 
% integration with calibration systems.

% The components used in the \dword{protodune}
% demonstrator will be assembled in 2019, and their operation will
% be verified in standalone tests. For \dword{fpga} processing, the
% initial version of firmware will be demonstrated in \dword{fpga} development
% cards, and an integration test will be conducted in 2019 with a
% prototype \dword{fpga} processing module and the  \dword{felix} \dword{fe}.


\subsection{Additional Teststands}
\label{sec:sp-daq:validation-demonstrators}
%\fixme{single-phase module}
%\metainfo{Describe VST demonstrators and why we must build them.}

Concurrently with protodune operation and development, a number of
``vertical slice'' teststands will be built to allow 
development and testing of individual parts of the \dword{daq} system
as well as testing of key aspects of the design and overall
scalability. A data selection subsystem vertical slice teststand will be
constructed and operated on fake-generated data, to assist in the
development of data selection, exercise the system for a variety of
configurations, perform small-scale tests that stress the critical
parts of the corresponding infrastructure, 
and identify likely failure points and/or bottlenecks. The subsystem
will also be deployed and exercised on existing HPC clusters of
comparable resources and specifications as planned for the final
production system for ``horizontal slice'' tests of similar nature. The
back-end DAQ subsystem will be developed and tested in a similar way.

In addition to dedicated vertical and horizontal slice teststands, a number of
\dword{daq} development kits will be available for the consortium for
specific component testing, as well as to other detector and
calibration consortia to support their own development, production, and quality assurance programs. The \dword{daq} 
kit will also form the basis for testing at APA Construction sites beginning in 2020. 

\section{Production, Assembly, Installation and Integration}
\label{sec:sp-daq:production}

\fixme{This will be completed at a later time.}

\subsection{Production and Assembly}
\metainfo{Describe how hardware, firmware and software will produced. }

\subsubsection{Computing Hardware}

\subsubsection{Custom Hardware Fabrication}

\subsubsection{Software and Firmware Development}
\metainfo{Processes and practices.}

\subsection{Installation and Integration}

%\metainfo{Describe how we get stuff in place underground, how we will put it all together and make sure it works. 
%  What can we do to minimize the effort needed underground both in
 % terms of physical work but also in working out the bugs both in
 % individual processes and in emergent behavior of the system as a
 % whole?}

\section{Organization and Project Management}
\label{sec:sp-daq:organization}

\subsection{Consortium Organization}

The DAQ Consortium was formed in 2017 as a joint single and
dual phase consortium, with a Consortium Leader and a Technical
Leader. The organization of the consortium is shown in
Figure~\ref{fig:daq-org}. The DAQ consortium board currently comprises
institutional representatives from 30 institutes as shown in Table~\ref{tab:daq-ib}. The consortium leader is the spokesperson for the consortium and
responsible for the overall scientific program and management of the
group. The technical leader of the consortium is responsible for
managing the project for the group.

The consortium's initial mandate has been the design, construction,
and commissioning 
of the DUNE FD DAQ system. To realize this, the consortium was
initially organized in the form of five working groups: (1)
Architecture, (2) Hardware, (3) Data Selection, (4) Back-end DAQ, and (5)
Installation and Infrastructure. This organization has seen the
project through the conceptual design phase.  

A new organizational
structure has been adopted to see the project through engineering
design and construction, and this structure is expected to evolve in
order to meet the needs of the consortium. This is shown in Figure~\ref{fig:daq-org}. Each working group has a designated working group
leader. In addition to the
working group leads, technical design report editors are responsible for the
overall editing and delivery of the TDR document.

\fixme{This is tentative, from leadership. Needs consortium vetting. }
\begin{dunefigure}{fig:daq-org}{Organizational chart for the \dword{daq} Consortium
 }
  \includegraphics[width=0.9\textwidth]{daq-org.pdf}
\end{dunefigure}

\fixme{This is tentative. }
\begin{dunetable}
[DAQ Consortium Board institutional members and countries]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-ib}
{DAQ Consortium Board institutional members and countries.}   
Member Institute & Country  \\ \toprowrule
CERN & CERN     \\ \colhline
Universidad Sergio Arboleda (USA) & Colombia     \\ \colhline
Czech Technical University & Czech Republic \\ \colhline
Lyon & France \\ \colhline
INFN Bologna & Italy \\ \colhline
Iwate & Japan     \\ \colhline
KEK & Japan     \\ \colhline
NIT Kure & Japan     \\ \colhline
NIKHEF & Netherlands    \\ \colhline
University of Birmingham & UK     \\ \colhline
Bristol University & UK     \\ \colhline
University of Edinburgh & UK     \\ \colhline
Imperial College London & UK     \\ \colhline
University of Liverpool & UK     \\ \colhline
Oxford University & UK     \\ \colhline
Rutherford Appleton Lab (RAL) & UK     \\ \colhline
University of Sussex Sussex & UK     \\ \colhline
University College London (UCL) & UK     \\ \colhline
University of Warwick & UK     \\ \colhline
Brookhaven National Lab (BNL) & USA     \\ \colhline
Colorado State University (CSU) & USA     \\ \colhline
Columbia University  & USA     \\ \colhline
University of California, Davis (UCD) & USA     \\ \colhline
Duke University & USA     \\ \colhline
University of California, Irvine (UCI) & USA     \\ \colhline
Fermi National Lab (FNAL) & USA     \\ \colhline
Iowa State University & USA     \\ \colhline
University of Minnesota, Duluth (UMD) & USA     \\ \colhline
University of Notre Dame & USA     \\ \colhline
University of Pennsylvania (Penn) & USA     \\ \colhline
South Dakota School of Mines and Technology (SDSMT) & USA     \\ \colhline
Stanford Linear Accelerator Lab (SLAC) & USA     \\ \colhline
\end{dunetable}

\subsection{Cost and Labor}
\label{sec:sp-daq:cost}

Table~\ref{tab:daq-cost} shows the current cost estimates for the DAQ
subsystems major components necessary to serve the first DUNE FD
module. Costs are expected to be reduced for subsequent modules, since
multiple components are common across modules. %When appropriate, the quantities of
%components are shown, along with the total cost and a brief description of
%what is included in the cost estimate. 
The cost estimates include
materials and supplies (M\&S) for production, and packing and
shipping to SURF, but not 
spares. 

Labor depends on personnel category (e.g., faculty, student,
technician, post-doc, engineer), and vary by region and
institution. As such, costs are quantified using labor hours needed to
fulfil a given task. Table~\ref{tab:daq-cost} provides estimates of
labor hours for construction. Signficant physics and simulation
effort is needed in particular for data selection related studies; those
labor resources are listed separately.

\begin{dunetable}
[DAQ System Cost Summary]
{p{0.5\textwidth}p{0.2\textwidth}p{0.2\textwidth}}
{tab:daq-cost}
{DAQ System Cost Summary}
Cost Item & M\&S (k\$ US) & Labor Hours \\ \toprowrule
\rowcolor{dunepeach} Design, Engineering and R\&D & & \\ \colhline
Facility & & \\ \colhline
Timing & & \\ \colhline
Detector Readout & & \\ \colhline
Trigger & & \\ \colhline
Data Flow & & \\ \colhline
Event Filter & & \\ \colhline
DAQ Control & & \\ \colhline
\rowcolor{dunepeach} Production Setup & & \\ \colhline
\rowcolor{dunepeach} Production & & \\ \colhline
Facility & & \\ \colhline
TIming & & \\ \colhline
Detector Readout & & \\ \colhline
Trigger & & \\ \colhline
Data Flow & & \\ \colhline
Event Filter & & \\ \colhline
DAQ Control & & \\ \colhline
\rowcolor{dunepeach} DUNE FD Integration \& Installation & & \\ \colhline
Facility & & \\ \colhline
TIming & & \\ \colhline
Detector Readout & & \\ \colhline
Trigger & & \\ \colhline
Data Flow & & \\ \colhline
Event Filter & & \\ \colhline
DAQ Control & & \\ 
\end{dunetable}
% \begin{dunetable}
% [DAQ System Cost Summary]
% {p{0.15\textwidth}p{0.1\textwidth}p{0.15\textwidth}p{0.4\textwidth}}
% {tab:daq-cost}
% {Cost estimates for different DAQ subsystems. All cost estimates
%   include M\&S for construction only. Packing and shipping costs are
%   included; spares are not included. }   
% System & Quantity & Cost (under development) (k\$ US) & Description \\ \toprowrule
% TPC Front-end Readout & 150 & - & Detector links  \\ \colhline
% TPC Front-end Readout & 75 & - & Felix, host server (CPU), and networking  \\ \colhline
% TPC Front-end Readout & 150 & - & Co-processor  \\ \colhline
% PDS Front-end Readout & 6-8 & - & Felix, host server (CPU), and networking  \\ \colhline
% Low level TPC Data Selection & 75 & - & Server (CPU), and networking \\ \colhline
% Low level PDS Data Selection & 6-8 & - & Server (CPU), and networking \\ \colhline
% Trigger Primitive local storage & 1 & - & 1PB hard drive storage \\ \colhline
% Module Level Trigger & 1+1 & - & Server (CPU), networking \\ \colhline
% External Trigger Interface & 1+1 & - & Server (CPU), networking, and
% interface boards \\ \colhline
% High-Level Filter & 5 & - &  Server (GPU), and networking \\ \colhline
% Back-end DAQ & 30 & - & Event Builder servers, and networking \\
% \colhline 
% Back-end DAQ Buffer & 1 & - & 1PB hard drive storage \\
% \colhline 
% CCM configuration, run control, databases & 4 & - & Server (CPU), and networking \\
% \colhline 
% CCM data management and transfer & 4 & - & Server (CPU), and networking \\
% \colhline 
% Timing \& Synchronization & 1 & - & Timing system, GPS \\ \colhline
% Facilities & 16 & - & Racks, professional installation \\ \colhline
% Infrastructure & - & - & IT \\ \colhline
% \end{dunetable} 


% \begin{dunetable}
% [DAQ System Labor]
% {p{0.25\textwidth}p{0.15\textwidth}p{0.1\textwidth}p{0.08\textwidth}p{0.08\textwidth}p{0.1\textwidth}p{0.08\textwidth}}
% {tab:daq-labor}
% {Estimate of labor hours for each category of personnel for different DAQ subsystems.}
% System  & Faculty/Scientist & Post-doc & Student & Engineer & Technician  &  \textbf{Total}\\ \toprowrule
% & (hours) & (hours)& (hours)& (hours)& (hours)& (hours)\\ \toprowrule
% Upstream DAQ & - & -& -& -& - & - \\ \colhline
% Data Selection & - & -& -& -& - & - \\ \colhline
% Back-end DAQ & - & -& -& -& - & - \\ \colhline
% Software Infrastructure & -& -& -& -& - & - \\ \colhline
% CCM & -& -& -& -& - & - \\ 
% Physics \& Simulation & -& -& -& -& - & - \\ \colhline
% \end{dunetable}

Following the funding model envisioned for the consortium, various
responsibilities have been distributed across institutions within the
consortium. At this stage of the project, these should be considered
as ``aspirational'' responsibilities until firm funding decisions are
made. Table~\ref{tab:daq-inst-resp} shows the current institutional
responsibilities for primary DAQ subsystems. Only lead institutes are
listed in the table for a given effort. For physics and simulation
studies, and validation efforts at ProtoDUNE, wider institutional effort is
involved. A detailed list of tasks and institutional responsibilities
are presented in \cite{WBS}.

\begin{dunetable}
[DAQ System Institutional Responsibilities]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-inst-resp}
{Institutional responsibilities in the DAQ Consortium}
DAQ Subsystem  & Institutional Responsibility\\ \toprowrule
Upstream DAQ &  \\ \colhline
Data Selection &  \\ \colhline
Back-end DAQ &   \\ \colhline
Software Infrastructure &  \\ \colhline
CCM &   \\ \colhline
Infrastructure and Facilities &   \\ \colhline
Physics \& Simulation &   \\ \colhline
\end{dunetable}

\subsection{Schedule and Milestones}
\label{sec:fd-daq:schedule}

\begin{dunetable}
[DAQ Consortium Schedule]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-sched}
{DAQ Consortium Schedule}   
Milestone & Date (Month YYYY)   \\ \toprowrule
Upstream DAQ Architecture Technology Decision & June 2020 \\ \colhline
Engineering Design Review for Timing System &  June 2020   \\ \colhline
Production Readiness Review for Timing System & June 2021 \\ \colhline
Preliminary Software Design Review & January 2022 \\ \colhline
Engineering Design Review for Hardware/Firmware & March 2022 \\  \colhline
\rowcolor{dunepeach} Start of \dword{pdsp}-II installation& \startpduneiispinstall      \\ \colhline
\rowcolor{dunepeach} Start of \dword{pddp}-II installation& \startpduneiidpinstall      \\ \colhline
%\dword{prr} dates & December 2021 \\ \colhline
%Production Readiness Review Dates &  January 2022   \\ \colhline
Start of Racks Procurement & July 2022  \\ \colhline
End of Racks Procurement & March 2023  \\ \colhline
Start of DAQ Server Procurement & September 2022  \\ \colhline
End of DAQ Server Procurement & May 2023  \\ \colhline
Production Readiness Review for Readout Hardware/Firmware & December
2022  \\ \colhline
Start of  DAQ Custom Hardware Production &  March 2023    \\ \colhline
End of  DAQ Custom Hardware Production &  December 2023    \\ \colhline
DAQ Software Final Design Review & June 2023  \\ \colhline
\rowcolor{dunepeach}South Dakota Logistics Warehouse available& \sdlwavailable      \\ \colhline
\rowcolor{dunepeach}Beneficial occupancy of cavern 1 and \dword{cuc}&
\cucbenocc      \\ \colhline 
\rowcolor{dunepeach} \dword{cuc} counting room accessible& \accesscuccountrm      \\ \colhline
\rowcolor{dunepeach}Top of \dword{detmodule} \#1 cryostat accessible& \accesstopfirstcryo      \\ \colhline
Start of DAQ Installation & May 2023 \\ \colhline
Start of DAQ Server Procurement  (II) & September 2024  \\ \colhline
End of DAQ Server Procurement  (II) & May 2025  \\ \colhline
End of DAQ installation & May 2025 \\ \colhline
End of DAQ Standalone Commissioning & December 2025 \\ \colhline
DAQ Server Procurement  (III) & July 2026  \\ \colhline
End of DAQ Commissioning & December 2026  \\ \colhline
\rowcolor{dunepeach}Start of \dword{detmodule} \#1 TPC installation& \startfirsttpcinstall      \\ \colhline
\rowcolor{dunepeach}End of \dword{detmodule} \#1 TPC installation& \firsttpcinstallend      \\ \colhline
\rowcolor{dunepeach}Top of \dword{detmodule} \#2 accessible& \accesstopsecondcryo      \\ \colhline
 \rowcolor{dunepeach}Start of \dword{detmodule} \#2 TPC installation& \startsecondtpcinstall      \\ \colhline
\rowcolor{dunepeach}End of \dword{detmodule} \#2 TPC installation& \secondtpcinstallend      \\ \colhline
\end{dunetable}

\begin{dunefigure}{fig:daq-schedule}{DAQ schedule for first \SI{10}{\kilo\tonne} module. \label{sec:fd-daq:schedule}}
  \includegraphics[width=0.95\textwidth,clip,trim=1cm 2cm 1cm 2cm]{daq-schedule.pdf}
\end{dunefigure}


\subsection{Safety and Risks}

%\fixme{The auto-generated risk table below should be uncommented once the risk xls spreadsheet is processed. Already sent to David.}
\input{generated/risks-longtable-SP-FD-DAQ}
