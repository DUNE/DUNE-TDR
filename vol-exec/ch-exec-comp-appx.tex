

\chapter{Computing Roles and Collaborative  Projects} 

\section{Roles}
\label{appx:comp-roles}

This appendix lists \dword{dune} computing roles derived from a comparison with existing similar roles on the \dword{lhcb} experiment at \dword{cern}.  \dword{lhcb} is similar in size and data volumes to \dword{dune}. 

\begin{description}


\item {Distributed Computing Development and Maintenance - 5.0 FTE}
This task includes all software engineering and development activities for packages needed to operate on distributed computing resources. The task requires a good understanding of the distributed computing infrastructure used by DUNE as well as the DUNE computing model.%5.0 FTE

\item {Software and Computing Infrastructure Development and Maintenance - 6.0 FTE}
This task includes software engineering, development and maintenance activities related to central services operated by DUNE to support software and computing activities of the project.   %6.0 FTE

\item {Database Design and Maintenance - 0.5 FTE}
Provides expert assistance to tasks within DUNE which require databases, and helps in addressing performance issues as databases scale. %0.2 FTE

\item {Data Preservation Development - 0.5 FTE}
This task includes activities related to analysis reproducibility and preservation as well as data preservation. The task requires expert knowledge of analysis work and the computing model. %0.5 FTE

\item {Application Managers and Librarians - 2.0 FTE}
Application Managers handle software applications needed for data processing, simulation and analysis. The role includes tasks such as coordination of development activities, release preparation, and the correct deployment of software package releases on software areas needed by DUNE. Librarians organize the overall setup of software packages needed for releases. %The total amount for these roles account for 2 FTE.

\item {Central Services Manager and Operators - 1.5 FTE}
The site manager and operators are responsible for the central infrastructure and services of the DUNE distributed computing infrastructure. This includes liaison with the host laboratory with respect to services they provide for DUNE. %The Site Manager and Operator roles accounts for 1.5 FTE.

\item {Distributed Production Manager - 0.5 FTE}
Distributed Production Managers are responsible for the setup, launch, monitoring and finishing of processing campaigns executed on distributed computing resources of the experiment. Production management is done for data processing, Monte Carlo simulation and working group productions.% The data processing production manager role for all production types accounts for 0.5 FTE.

\item {Distributed Data Manager - 0.5 FTE}
The distributed data manager is responsible for operational interactions with distributed computing disk and tape resources. This role includes activities such as supporting the onboarding of new storage areas, data replication, deletion, movement etc. % The distributed computing data manager role accounts for 0.5 FTE

\item {Distributed Workload Manager - 0.5 FTE}
The distributed workload manager is responsible for operational interactions with distributed computing resources. This role includes activities such as supporting the onboarding of new grid and cloud sites. %The distributed computing workload manager role accounts for 0.5 FTE



\item {Computing Shift Leaders - 1.4 FTE}
The Shift Leader is the main responsible role for distributed computing operations of the experiment. The role is covered by shifts (Monday to Sunday) done by a single person at a time.  They chair the regular operations meetings during their week and attend general DUNE operations meetings as appropriate. %1.4 FTE as it also includes week-ends.

\item {Distributed Computing Resource Contacts - 0.5 FTE}
Distributed computing resource contacts are primary contacts for the DUNE distributed computing operations team and the operators of large (Tier-1) sites and regional federations. They interact directly with the Computing Shift Leaders at operations meetings. %The total of all site contacts accounts for 0.5 FTE and increases with the number of large sites requiring contacts.

\item {User Support - 1.0 FTE}
User support underpins all user activities of the computing project such as software infrastructure, applications and distributed computing, and includes shifts to respond to questions from users on mailing lists and/or Slack-style chat systems and/or ticketing systems, and documenting solutions in knowledge bases and Wikis.% The total FTE count for this role is 1 FTE.

\item {Resource Board chair - 0.1 FTE}
Chairs quarterly meetings of the Computing Resource Board with representatives from the national DUNE collaborations to discuss the level of funding and delivery of the computing resources required for successful processing and exploitation of DUNE data. %0.1 FTE

\item {Computing Coordination - 2.0 FTE}
Overall management of the computing project. %The total amount for this role accounts to 2 FTE.

\todo{Delineate which of these must be dedicated position and which are potentially "shift" duties that can be taken on by individual scientists.   Divide into those two groups.  

May wish to make this an appendix and say that we need around N dedicated full time experts + Y effort from the broader collaboration via shifts or shorter term contributions.
}
\end{description}



%%%%%%%%%%%%%%%%%%%%
\section{Specific Collaborative Computing Projects}
\label{ch:exec-comp-gov-coop}

The \dword{hep} computing community has come together to form a HEP Software Foundation (HSF)\cite{Alves:2017she} that, through working groups, workshops, and white-papers is guiding the next generation of shared \dword{hep} software.  \dword{dune}'s time scale, where we are in the planning and evaluation phase, is almost perfect for us to contribute to and benefit from these efforts.  Our overall strategy for computing infrastructure is to carefully evaluate existing and proposed field-wide solutions, to participate in useful designs, and to build our own solutions only where common solutions do not fit and additional joint development is not feasible.   This section describes some of these common activities. 



\subsection{LArSoft for Event Reconstruction}

The \dword{larsoft}\cite{Snider:2017wjd} reconstruction package is shared by several neutrino experiments using the \dword{lartpc} technology.  \dword{microboone}, \dword{sbnd}, \dword{dune}, and others share in developing a common core software framework customized for each experiment. This software suite and earlier efforts by other experiments made the rapid reconstruction of the \dword{pdsp} data possible.  \dword{dune} will be a major contributor to  the future evolution of this package, in particular, introducing full multi-threading to allow parallel reconstruction of parts of large events, thus anticipating the very large events expected from the full detector. 

\subsection{WLCG/OSG and the HEP Software Foundation}
The  \dword{wlcg}\cite{Bird:2014ctt} organization, which currently combines the resource and infrastructure missions of the \dword{lhc} experiments, has proposed a governance structure that splits dedicated resources for \dword{lhc} experiments from the general middleware infrastructure used to access those resources.  This \dword{sci} is already used by many other experiments worldwide.  In a white paper submitted to the European Strategy Group in December 2018\cite{bib:BirdEUStrategy}, a formal \dword{sci} organization is proposed. As part of the transition to that structure, the \dword{dune} collaboration was provisionally invited to join the \dword{wlcg} Management Board as observers and to participate in the Grid Deployment Board and task forces. The goal of our participation is to contribute to the technical decisions on global computing infrastructure while also contributing to that infrastructure. 
Many of these projecta also involve contributions to and from the broader HEP Software Foundation efforts. 

Areas of collaboration are described in the following. 

\subsubsection{Rucio Development and Extension}

 \dword{rucio}\cite{Barisits:2019fyl}
is a data management system originally developed by the \dword{atlas} collaboration and is now an open-source project.  \dword{dune} has chosen to use \dword{rucio} for large scale data movement.  In the short term, it is combined with the \dword{sam} data catalog used by \dword{fermilab} experiments.  \dword{dune} collaborators at \dword{fermilab} and in the UK are actively collaborating on the \dword{rucio} project, adding value for \dword{dune} but also for the wider effort.


The global \dword{rucio} team now includes \dword{fermilab} and \dword{bnl} staff, \dword{dune}, and \dword{cms} collaborators  in addition to the core developers on \dword{atlas} who initially wrote \dword{rucio}.  Consortium members have started collaborating on several projects:  (1) making object stores (such as Amazon S3 and compatible utilities) work with \dword{rucio} (a large object store in the United Kingdom exists for which \dword{dune} has a sizable allocation);  (2) monitoring  and administering the \dword{rucio} system, leveraging the Landscape system at \dword{fermilab}, and  (3) designing a  data description engine that can be used to replace the \dword{sam} system we currently use.



\dword{rucio} has already proved a powerful and useful tool for moving defined datasets from point A to point B.  Our initial observation is that \dword{rucio} is a good solution for file localization but is missing the detailed tools for data description and granular dataset definition available in the current \dword{sam} system.  The rapidly varying conditions in the test beam have highlighted a need for a sophisticated data description database interfaced to \dword{rucio}'s location functions. 

In addition,   \dword{lhc} experiments such as \dword{atlas} and \dword{cms} work with disk stores and tape stores that are independent of each other.  This is different from the dCache model used in most \dword{fermilab} experiments where most of dCache is a caching frontend for a tape store.  Efficient integration of caching into the \dword{rucio} model will be an important component for \dword{dune} unless  we can afford to have most data on disk to avoid staging.


%\todo{ Comment on metadata project}

\subsubsection{Testing New Storage Technologies and Interfaces}

The larger \dword{hep} community\cite{Berzano:2018xaa} currently has a \dword{doma} task force
 with which several \dword{dune} collaborators are involved. There are task forces for authorization, caching, third party copy, hierarchical storage, and quality of service. All are of interest to \dword{dune} because they will determine the long term standards for common computing infrastructure in the field. 
In particular, the authorization issues should significantly affect \dword{dune}; they are covered in Section~\ref{ch-comp-auth}.


\subsubsection{Data Management and Retention Policy Development}



A data life cycle is built into the \dword{dune} data model.  Obsolete samples (old simulations and histograms and old commissioning data) need not be maintained indefinitely.  
We are organizing the structure of lower storage, so the various retention types are stored separately for easy deletion when necessary.  

\subsubsection{Authentication and Authorization Security and Interoperability}\label{ch-comp-auth}

Within the next 2-3 years, we expect the global \dword{hep} community to change significantly the methods of authentication and authorization of computing and storage. 
Over that period, \dword{dune} must collaborate with the USA and European \dword{hep} computing communities on improved authentication methods  that will allow secure but transparent access to storage and other resources such as logbooks and code repositories.  The current model, where individuals must be authenticated through different mechanisms for access to USA and European resources, is already a bottleneck to efficiently integrating personnel and storage. 
Current efforts to expand the trust realm between \dword{cern} and \dword{fermilab} should allow a single sign-on for each to access the other laboratory.


\subsection{Evaluations of Other Important Infrastructure}

The \dword{dune} \dword{csc} is still evaluating major infrastructure components, notably databases and workflow management systems.

For databases\cite{Laycock:2019ynk}, the \dword{fermilab} conditions database is used for the first run of \dword{protodune} but the Belle II\cite{Ritter:2018jxh} system supported by \dword{bnl} is being considered for subsequent runs. 

For workflow management, we are evaluating \dword{dirac}\cite{Falabella:2016waj} and plan to investigate PANDA\cite{Megino:2017ywl} to compare with the current GlideInWMS, HT Condor, and POMS solution that has been successfully used for the 2018 \dword{protodune} campaigns.
Both \dword{dirac} and PANDA are used by several \dword{lhc} and non-\dword{lhc} experiments and are already being integrated with \dword{rucio}. 
