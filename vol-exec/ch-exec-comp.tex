%\ifdefined\isfinal\documentclass[final]{dune}\else\documentclass{dune}\fi
%\pdfoutput=1            % must be in first 5 lines so arXiv finds it
%\graphicspath{ {graphics/} {executive-summary/graphics/}{generated/} }
%% * <aheavey@fnal.gov> 2018-04-06T17:26:26.700Z:
%%
% ^.
%\input{common/preamble}

% define a local dword for later incorporation

\newcommand{\ldword}[1]{{\bf{[#1]}}\todo{define word#1}}
\newcommand{\ldshort}[1]{{\bf{[#1]}}\todo{define abbr #1}}
\newcommand{\ignore}[1]{}
\newcommand{\lcite}[1]{\cite{#1}}
\renewcommand\thedoctitle{\voltitleexec} % defined in common/defs.tex
%newcommand\thevolumenumber{1}
%\def\titleextra{\includegraphics[width=0.55\textwidth]{energy_nu_no}} -- change image file

%\begin{document}

%%%%%%%%%%%%%%%%%%%%  REMOVE ABOVE %%%%%%%%%%%%%%%%%
%\newduneword{rucio}{Rucio}{Data management system originally developed by ATLAS but now open-source and shared across HEP}
%\newduneabbrev{doma}{DOMA}{Data Organization, Management, and Access}{Data Organization, Management, and Access efforts through the HEP Software Foundation}
%\newduneabbrev{hsf}{HSC}{High Energy Physics Software Foundation}{High Energy Physics Software Foundation}
%\newduneabbrev{wlcg}{WLCG}{Worldwide LHC Computing Grid}{Worldwide LHC Computing Grid}
%\newduneabbrev{osg}{OSG}{Open Science Grid}{Open Science Grid}
%\newduneabbrev{sci}{SCI}{Scientific Computing Infrastructure}{Proposed extension of the infrastructure component of \dword{wlcg} to other experiments}
%\newduneabbrev{csc}{CSC}{Computing and Software Consortium}{DUNE Computing and Software Consortium}


\chapter{Computing in DUNE}
\label{ch:exec-comp}
%
%\fixme{Heidi, this outline may be overkill for the exec summary; it may be a good structure for the computing CDR volume, then pared down for inclusion here. My 2 cents! -Anne}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{To remove: just examples}
%\label{sec:exec-comp-1}
%
%Sample figure to copy and edit, Figure~\ref{fig:map}:
%
%\begin{dunefigure}[DUNE collaboration global map]{fig:mhexec}{The international DUNE
%collaboration. Countries with DUNE membership are shown in orange.}
%\includegraphics[width=0.9\textwidth]{global-retouched.jpg}  
%\label{fig:map}
%\end{dunefigure}
%
%Sample table to copy and edit, Table~\ref{tab:execosctable}:
%
%\begin{dunetable}[Required exposures to reach oscillation physics
%  milestones]{lcc}{tab:execosctable}{The exposure in mass (kt) $\times$ proton beam power
%    (MW) $\times$ time (years) and calendar years assuming the staging plan described in this chapter needed to reach certain oscillation physics
%    milestones. The numbers are for normal hierarchy using the NuFit 2016 best fit values of the known oscillation parameters.  }
%Physics milestone & Exposure  & Exposure \\ \rowtitlestyle
%  & (\ktMWyr{}) & (years)  \\ \toprowrule 
%  $1^\circ$ $\theta_{23}$ resolution ($\theta_{23} = 42^\circ$) & 29  &  1\\ \colhline
%  CPV at $3\sigma$ ($\delta_{\rm CP} = -\pi/2$)  & 77 &  3\\ \colhline
%  \dword{mh} at  $5\sigma$ (worst point) & 209 & 6 \\ \colhline
%  $10^\circ$ $\delta_{\rm CP}$ resolution ($\delta_{\rm CP} = 0$) & 252 & %5 
%  6.5 \\ \colhline
%  ($\sin^2 2 \theta_{13} = 0.084 \pm 0.003$) &  &  \\  
%\end{dunetable}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Executive Summary}
\label{ch:exec-comp-es}

The DUNE  collaboration consists of 178 institutions from 32 countries, including 15 European nations and CERN. The experiment is in preparation now with commissioning of the first 17kT Liquid Argon TPC  module expected over the period 2024-2026 and a long data taking run growing to 4 modules from 2026-2036 and beyond.  An active prototyping program is already in place with a short test beam run in 2018 at CERN.  These tests used  a 700T, 15,360 channel prototype TPC with single-phase readout.  Tests of a similar sized dual-phase detector are scheduled for mid-2019.   The DUNE experiment has already  benefited greatly from these initial tests.  The collaboration has recently formed a formal Computing Consortium, with significant participation by European Institutions and interest from groups in Asia to work on common software and computing development and to formalize resource contributions.

The consortium resource model benefits from existing Open Science Grid\dshort(OSG)  and \dword{wlcg} infrastructure developed for the LHC and broader HEP community.  DUNE, through  the ProtoDUNE-SP effort, is already using global resources for simulation and the analysis of ProtoDUNE-SP data.  Multiple European sites are part of this resource pool and are making significant contributions to the ProtoDUNE single and dual phase programs.  We expect this global computing consortium to grow and evolve as we move towards data from the full DUNE detectors in the middle of the next decade.

The DUNE science program is expected to produce raw data volumes similar in scale to the data volumes that current LHC Run-2 experiments have already recorded.  Baseline predictions for the DUNE data, dependent on actual detector performance and noise levels, are $\ge 30$ PB of raw data per year.  These data, with simulations and derived analysis samples, need to be available to all collaborating institutions.  We anticipate that institutions worldwide will play an important role both as contributors and end-users of storage and CPU resources for DUNE.

To enable these resource contributions in cooperation with the LHC and other communities, we plan to utilize common computing layers for infrastructure access and use common tools to ease integration of facilities with both the DUNE and LHC computing ecosystems.  We will use common data storage methodologies to establish large high-availability data lakes worldwide  and to collaborate with the broader HEP community in developing other common tools.


HEP has considerable infrastructure in place for international computing collaboration thanks to the LHC program.  Additional large non-LHC experiments including LSST, SKA, DUNE and HyperK will be entering operation over the next decade and will need to utilize and expand this model for international cooperation.  This work on organizing the broader HEP community, is being formalized through the HEP Software Foundation(\dshort{hsf})\lcite{Alves:2017she}.  The HSF is an organization of interested parties working to use the extensive knowledge we have gained over the past two decades and the needs that experiments will have over the next two decades, to develop a sustainable computing landscape for the HEP community.  The HSF white papers and roadmaps place emphasis on common tools and infrastructure as the underpinnings of this landscape.

DUNE's computing strategy leverages heavily this model of common tools and infrastructure and features efforts in data movement and storage, job control and monitoring, accounting and authentication which both utilized and contribute to this global community.   DUNE recognizes that other large-scale experiments have similar needs and will encounter similar issues which drive worldwide cooperation on common tools as the most cost-effective path fulfilling the scientific missions of the experiments.  Already in the R\&D phases of DUNE computing there pilot programs that use this model.  Most recently in the realm of data management and storage, a collaboration between Fermilab, CERN, Rutherford Appleton Laboratory and other academic institutions in the United Kingdom's, is investigating the adaptation and use of the {\it \dword{rucio}}\cite{Barisits:2019fyl} data management systems to serve as the core data management system for DUNE.

Examples of this proto-culture of international collaboration within DUNE were demonstrated during the 2018 test beam run of the ProtoDUNE Single-Phase (SP) detector.  The ProtoDUNE run was a valuable live test of this model.  During this run the Single Phase detector situated at CERN produced raw data at rates exceeding 2GB/s.  These data were transferred and stored to the archive facilities at CERN and Fermilab and replicated at sites in the UK and Czech Republic.

In total, 1.8 PB of raw data were produced during the 10 week test beam run, mimicking within a factor of two, actual expected data rates and volumes from the initial running of the far detector complex .  The prototype run was used to examine and test the scalability of existing and proposed computing infrastructure and to establish operational experience within the institutions which have expressed interest in the development and construction of the DUNE computing environment.  The technical design presented here builds heavily on the measurements and information gained from the ProtoDUNE experience.   These measurements are treated as proofs of concept for many of the systems and their behavior is extrapolated to the projected levels needed for the full DUNE experiment. 

In addition to traditional HEP computational strategies which have been prototyped with the ProtoDUNE experience, the format and organization of DUNE's data as large multi-dimensional arrays, open the possibility of treating the data with computing paradigms similar to those developed for astrophysical image data.  These image processing techniques heavily leverage advances in machine learning and pattern recognition.  More over these techniques benefit from the computing resources that are available at High Performance Computing facilities (Supercomputers) where massively parallel and computation heavy algorithms can be mapped onto and scale better with the hardware toplogies than they do with traditional batch farms.  This field machine learning and HPC oriented analysis is currently very active with active advances coming from the current generation of liquid argon experiments such as ArgoNeut, MicroBooNE, SBND and ICARUS.  DUNE is poised to benefited greatly from this work and to expand on it as part of its computing and analysis model.

In summary, DUNE's computing strategy is to be {\bf global}, working with partners worldwide, and {\bf collaborative}, as almost all of the computational challenges we face are faced by similar experiments. 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
\label{ch:exec-comp-ovr}
The mission of the computing consortium is to facilitate the acquisition, processing and analysis of both detector data and supporting simulations for the DUNE experiment.  This mission must extend over all of primary physics drivers for the experiment, and must do so in a cost effective and secure manner. The computing and software consortium \dshort{csc} provides the bridge between the multiple online \dword{daq} and monitoring systems and the different physics groups who develop high-level algorithms and analysis techniques to perform measurements with the DUNE data and simulation. The S\&C works with collaborating institutions to identify and provide computational and storage resources.  They provide the software and computing infrastructure, in the form of analysis frameworks, data catalogs, data transport systems, databases infrastructure, code distributions mechanisms and other supporting services that are essential for recording and analyzing the data and simulation. 

The computing consortium works with national agencies and major laboratories to negotiate use and allocation of computing resources.  This work includes support for near term and R\&D efforts such as the ProtoDUNE runs and extends to the design, development and deployment of the DUNE computing model and its requisite systems.
These designs include the evaluation of major software infrastructure systems, including workload management and data management system, to determine their suitability for satisfying the DUNE physics requirements.   These evaluations are aimed at identifying opportunities for adopting or adapting existing technologies and engaging in collaborative ventures with HEP experiments outside of DUNE. 

In this context, the DUNE computing needs are modest in the context of the projected rates and needs for the high luminosity LHC experiments.  Initial calculation and extrapolations place the data volumes for the DUNE far detector program on par with the volumes produced during LHC Run II.  However the  beam structure, event sizes and analysis methodologies make DUNE very unlike the collider experiments in terms of event processing needs and projected computational budgets.  In addition, the large DUNE event sizes present a novel technical challenge when data processing and analysis are mapped onto  current and planned computing facilities.  Neutrino oscillation analysis and parameter extraction also present novel computational challenges.   Some of these challenges may require significant effort to adapt to the global computing resources that will be available to the experiment.  These global resources are projected to be both heterogenenous with regards to computational capabilities (featuring CPU, GPU and other advanced technologies) and diverse in terms of topological architectures and provisioning models.  The DUNE computing consortium will need to address these issues of diversity and architectural inclusion to fully exploit the global resources that will be available in the 2026+ era, and to enable all collaborators to access the data and perform the scientific mission of the experiment.  

\section{Data types and volumes}

The maximum data rate that the DUNE far detector modules can produce for a single readout is computed from the detector channel counts, base sampling rates, and digitization resolution.  This is then scaled by the expected triggering rates for beam induced activity along with other sources of trigger initiation (i.e. cosmic ray activity, radiological signals, calibration information, etc...) to compute the expected upper limit on the data rates and volumes that can be expected from the detector.  Separately estimates of the effects of lossless data compression and zero suppression schemes have been estimated and in some cases demonstrated with the ProtoDUNE single phase data.  These techniques are highly dependent  on the structure of the data and in particular the structure of the noise or other sources of background that may be observed in the final DUNE detector modules.  As a result the compression/suppression estimates are given as ranges based on detector performance assumptions.

The parameters described in Table~\ref{tab:exec-comp-bigpicture} display the key characteristics of the DUNE far-detector data stream.  A bandwidth specification has been established which provides an operational envelope within which the data acquisition, networking, and software and computing tasks should  operate. This bandwidth envelope has been set to  30~PB of data/yr.  This corresponds to a time-averaged egress bandwidth  of 950 MB/s (7.6 Gbit/s).  As an uncompressed event stream this would equate to 0.15 evt/s, rising to 0.6 evt/s under a target compression of 4:1 of single phase module readout.    The reduction from full streaming data rates to these rates will be implemented in the DAQ/trigger level of the experiment. 
Those decisions are the purview of the collaboration scientists and the data acquisition design with feedback from computing on what is possible.  The ProtoDUNE experience has provided invaluable information to feed back to the experiment design. 

\subsection{Far detector}

The computing model needs to be able to handle a wide range of data inputs from the far detectors, as documented in more detail in docdb-9240\lcite{bib:docdb9240}.

\begin{itemize}
\item Supernova triggers which would have an uncompressed size of 138 TB for a 30 second readout of all channels in a 4-module single-phase detector at a likely rate of 1/month.  
\item Beam neutrino interactions within a single detector module with an uncompressed size of $\approx$ 6.2 GB.  Beam neutrinos arrive at a rate of up to 1 Hz but most do not result in measurable interactions.
\item Atmospheric neutrino interactions, nucleon decay and other lower energy processes confined to a subset of a detector module with a low threshold largely driven by radiological backgrounds.
\item Cosmic ray and rock muons at a rate of around 4,500/day/module.
\item Other calibration systems 
\end{itemize}

The estimates in docdb-9240, with conservative estimates for increased needs for low level data taking during commissioning, have led to a negotiated upper limit of 30 PB/year data volume as a standard for both the trigger and data acquisition and computing groups to work towards. 


\begin{dunetable}[Useful quantities for computing estimates]{lrr}{tab:exec-comp-bigpicture}{Useful quantities for computing estimates}%\rowtitlestyle
Quantity&Value&Explanation\\ 
\hline
{\bf Far Detector Beam:}\\
Single APA readout &41.5 MB& Uncompressed 5.4 ms\\
APAs per module& 150&\\
Full module readout &6.22  GB& Uncompressed 5.4 ms\\
Beam rep. rate&\beamreprate&Untriggered\\
CPU time/event&600-1,200 sec&from MC/ProtoDUNE\\
Memory footprint&2-4 GB&ProtoDUNE experience\\
\hline
{\bf Supernova:}\\
Single channel readout &90.0 MB& Uncompressed 30 s\\
Four module readout&138.2 TB& Uncompressed 30 s\\
Trigger rate&1  per month&(assumption)\\
%Yearly rates nd
%Reduction with roi.  
%CPU time/ event for reconstruction
%Reduction for analysis
%Users 
\end{dunetable}

\subsection{Near Detector}
In addition, a near detector of reasonable size will have multiple neutrino interactions/beam spill leading to a need to read out at the full beam rate of 0.8-1.2 Hz.
The near detector will have fewer channels and better signal/noise discrimination but much higher readout rates.  While the details of the detector design are still unknown, we assume data volumes of similar size to the far detector (30PB/year) in our planning.

\subsection{Simulation}
The bulk of data collected is likely to be backgrounds, with real beam interaction events in the far detector numbering in the thousands/year, not millions. Thus the size of simulation samples is likely to be less than that of the unprocessed raw data considered above.  Lower energy events are either very rare or can be simulated in sub-volumes of the whole detector.  As a result, while simulation will be an important part of the experiment, it is not expected to dominate data volumes as it does in many experiments.  

However, simulation inputs such as flux files, overlay samples and shower libraries pose a special problem as they must be distributed to simulation jobs carefully.  Proper simulation requires that these inputs be delivered in an unbiased fashion. This can be technically difficult in a widely distributed environment and will require thoughtful design. 

\subsection{Analysis}

Analysis formats have not yet been fully defined.  We anticipate that most analysis samples will be orders of magnitude smaller than the raw data.  However, as they are idiosyncratic to particular analyses and in fact particular users,  producing and cataloging them will be sociologically difficult. 
Likely there will be a mix of official samples -  produced by physics groups and distributed through a common catalog and file transfer mechanisms - and small user samples on local disk. 


\section{ProtoDUNE-SP as an example}
\label{ch:exec-comp-proto-SP}
The first ProtoDUNE single phase run at CERN in late 2018 has already led to a small-scale test of a global computing model.  In the following we will describe the ProtoDUNE data design and the lessons learned from our experience. Much of this carries over into planning for full far-detector operations. 

\subsection{Introduction}

The ProtoDUNE Single Phase detector ran at CERN in the np04 beamline from September to November of 2018. Since then, studies with cosmic rays have continued. Prior to that run there were several data challenges at high rate to validate the data transfer mechanisms. 

\subsection{Data Challenges}

ProtoDUNE performed a series of data challenges, starting in late 2017.  Simulated data were passed through the full chain from the event builder machines to tape store at CERN and Fermilab at rates of up to 2 GB/s.  These studies allowed optimization of the network and storage elements well before the start of data taking.
We note that the full DUNE far detector writing 30 PB/year would produce data at rates similar to, or less than, those demonstrated in the 2018 data challenges. 

\subsection{Commissioning and Physics Operations}

The first phase of operations was commissioning of the detector readout systems while the argon reached full purity.  Data were taken with cosmic rays and beam during the commissioning period. Once high Argon purity had been achieved, physics data were  taken with beam through October and half of November. Normal trigger rates were approximately 25 Hz but tests were done at rates of up to 100 Hz. Since the beam run ended, cosmic-ray data continues to be taken with varying detector conditions, such as modified high voltage and purity, and new readout schemes. 

%\subsection{Data Quality Monitoring}

%\todo{DQM}

\subsection{Data volumes}
The single-phase ProtoDUNE detector consists of a \dshort{tpc} with  6 Anode Plane Assemblies (\dshort{apa}), photon detectors (\dshort{pd}s) and a Cosmic Ray Tagger (\dshort{crt}). In addition, the np04 beamline is instrumented with hodoscopes and Cerenkov counters to generate beam triggers. Random triggers  were generated at lower rates to collect unbiased cosmic ray information. The data volume from the test beam run was dominated by readout of the \dshort{TPC}.  Each \dshort{apa} has 2,560 channels and reads out 12 bit ADC values at 2 MHz.   The nominal readout window during beam running was  3 ms to match the drift time at the full voltage of 180 kV that was maintained for most of the run.  The size of the \dshort{tpc} data without compression was thus 138 MB/event, not including headres.  The uncompressed event size including all TPC information and \dword{crt} and \dword{pd} data was 170-180 MB. Compression was implemented before the October beam physics run, lowering the total size per event from around 180 MB to 75 MB.  

\begin{dunetable}[Data volumes]{lrr}{tab:exec-comp-pd-volumes}{Data volumes  recorded by ProtoDUNE-SP as of December 2018.}
Type  & Events & Size\\ %\rowtitlestyle
Raw Beam&8.08 M& 520 TB \\
Raw Cosmics&3.46 M& 271 TB\\
Commissioning&3.86 M& 388 TB\\
Pre-commissioning&13.89 M&641 TB\\
\end{dunetable}

Events were written out in raw files of size 8 GB with each containing of order 100 events. The beam was live for two 4.5 s spills every 32 s beam cycle and data were taken at  rates of up to 50 Hz (typically 25 Hz) leading to compressed DC rates out of the detector of 400-800MB/sec.  Each beam cycle could therefore produce 1-4  8 GB output files.  In earlier running with uncompressed data, and during an April data challenge, transfer rates of up to 2GB/s were demonstrated over substantial periods. 

Beam stopped on November 12 but cosmic ray studies of the detector continue, some with an increased time window of 7.5 ms to collect more complete tracks each readout.  This raises the compressed event size to around 170 MB.


\subsection{ProtoDUNE-SP data streams}
The ProtoDUNE-SP data consist of multiple sources in addition to the TPC data. One of the major challenges for the offline computing systems is merging of these multiple streams into a coherent whole for analysis.  Table \ref{tab:exec-comp-pd-sources} lists the data sources used and their granularity. 

\begin{dunetable}[Data sources]{lrr}{tab:exec-comp-pd-sources}{Data sources  }
Type & indexed by & destination\\
TPC  & run/event & event data\\
Photon Detector data & run/event & event data\\
Cosmic Ray Tagger & run/event & event data\\
Beamline devices & timestamp & beam database\\
Detector conditions & timestamp & slow controls database\\
DAQ configuration & run & files/elisa logbook\\
Run quality & run & human generated spreadsheets\\
Data quality & run/event/time & Data Quality web application\\
File metadata & file & \dword{sam} file database\\
\end{dunetable}

Information about the detector conditions, \dword{daq} configuration and run quality is spread across a number of sources and must be collected and then boiled down into the quantities relevant for offline data analysis.  For example, the Slow Controls system logs detector conditions continually.  Offline analysis needs to know about these data with coarser granularity and then have algorithms capable of using that information. A full conditions database transfer mechanism is being developed but was not available during the run.  As a result, with the exception of beamline information, coarse information is currently added to the \dword{sam} file catalog run by run to allow files with given operating conditions to be easily identified and retrieved. Beam data is stored in the \dword{ifbeam}
database and connected to event data via time stamps.

\subsection{Reconstruction of ProtoDUNE-SP data}
Thanks to substantial previous effort by the 35T prototype, \dword{microboone} and the Liquid Argon TPC community, high quality algorithms were already in place to reconstruct the TPC  data.  As a result, a first pass reconstruction of the ProtoDUNE-SP data with beam triggers was completed by early December, less than a month after the end of data taking.



\subsection{Data preparation}

Before pattern recognition, data from the ProtoDUNE detector is
unpacked and copied to a standard format within the art framework based on ROOT derived objects. 
The format is also used in detector simulation events.
This reformatted raw data includes the waveform for each channel, consisting of 6,000-15,000,  12-bit
, 0.5 $\mu$sec samples. 

The first step in reconstruction is data preparation with the goal of
converting each ADC waveform into a calibrated charge waveform with
signals proportional to charge. At the end of data preparation, regions of interest (ROIs), i.e. blocks of contiguous samples where
useful signals appear, are identified and the data outside these regions are discarded.

%Perhaps most important, the bipolar signals in induction wires are made unipolar.
%Also the electronics shaping is replaced with the Gaussian shaping expected in the
%next stage of processing.
%Relative channel-to-channel and absolute calibration is applied to account for the
%responses of the amplifiers and ADCs.
%And attempts are made to remove noise and mitigate ADC distortions.

The sequence is described more fully in docdb-12349\lcite{bib:docdb12349} and in the Methods section of the Physics Volume but is summarized here:

\begin{enumerate}
\item Each waveform is unpacked into integers.
\item Pedestals are determined per event/per channel from the most common ADC value. 
\item Pedestals and calibrations are applied. %\label{local:ped}
\item Bad channels, sticky bits and other know hardware problems are corrected or removed.
\item Signal undershoot that creates a long negative tail is removed. 
\item The waveforms  are deconvoluted.  In the first processing this was done with simple 1-D  convolution for a single wire.  A 2-D method of deconvolving a detailed detector electrostatic field map, originally developed for MicroBooNE\lcite{Adams:2018dra}, is in now available for ProtoDUNE use and will be used in the future reconstruction passes.  It properly undoes the long-range induction effects while keeping efficiency high and bias low.  The deconvolution Fourier transforms the waveform, replaces the  bipolar field response function with a unipolar function, applies a low pass filter to remove high frequency noise and then transforms back.




\item Finally regions of interest are defined where the signal exceeds a given threshold and time slices well outside the \dshort{roi} are dropped, leading to significant reduction in the size of the remaining data. These data feed into the reconstruction algorithms for further pattern recognition. %\label{local:roi}
\end{enumerate}




%\subsection{Signal processing}
Figures~\ref{fig:ch-exec-comp-chtraw}-\ref{fig:ch-exec-comp-chtroi} illustrate the transformation of TPC data  during data
preparation.

\begin{figure}[t]
\includegraphics[width=\textwidth,angle=0]{comp-evd_twq-proj_5449_20926_raw.png}
\caption{
Example of pedestal-subtracted data for one of the ProtoDUNE  wire planes.  The top pane shows the ADC values in a V (induction) plane with the x-axis being channel number and the y-axis, time slice. The bottom pane shows the bipolar pulses induced on one channel. 
}
\label{fig:ch-exec-comp-chtraw}
\end{figure}




%\begin{figure}[t]
%  \includegraphics[width=\textwidth]{ccomp-evd.twq-proj.5449.20926.recon..png}
%\caption{
%Same as Fig.~\ref{fig:ch-exec-comp-chtraw} except after hit correction, tail removal and deconvolution.
%}
%\label{fig:ch-exec-comp-chtdco}
%\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{comp-evd_twq-proj_5449_20926_decon.png}
\caption{
Same as Fig.~\ref{fig:ch-exec-comp-chtraw} except after calibration, cleanup, deconvolution and ROI finding. 
}
\label{fig:ch-exec-comp-chtroi}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\todo{Statement about timing and memory for this phase}

\subsection{Computational characteristics of data preparation and deconvolution }
Decoding for ProtoDUNE-SP is currently done with all 6 APAs in memory. As each 3 ms APA readout consists of over 15M 16-bit values, decompression and conversion to floating point results in substantial memory expansion.  Decoding and deconvolution of 6 APAs with 3 ms readout fits within a normal 2 GB memory/core footprint but the 7.5 ms readout window used in some cosmic ray studies requires a correspondingly larger memory footprint. As electrical signals are correlated between channels within an APA wire plane, but not between planes, processing each wire plane (3/APA) independently reduces the memory footprint.  These changes are being implemented.


However,  while subdividing the detector into wire planes solves the memory problems for short readouts it is  not a viable solution for the long readouts expected for supernova events. We are still exploring the best strategy for dealing with these much larger ($\times 10,000$)time windows. The DAQ group is already testing 1 second ($300 \times$ longer time window) readouts of small numbers of channels.  These are being used as tests of optimal models for data segmentation.  Section \ref{ch:exec-comp-mod} describes the start of a bottoms-up collaboration with the \dshort{daq} consortium on an optimal data model for the full DUNE detectors. 

\subsection{Further reconstruction}
The downstream pattern recognition steps starting with \dword{roi} are described further in the Tools and Methods chapter of the Physics Volume.  
Full reconstruction of ProtoDUNE-SP interactions, with beam particles and of order 20-40 cosmic rays per readout window took 600-1200 sec/event.
Table  \ref{tab:comp-raw-data-size} shows the input datasize for a typical beam event, dominated by around 71 MB of TPC waveform information. Table  \ref{tab:comp-reco-data-size} shows the size of different reconstructed objects, still dominated by around 10 MB of reduced TPC hit information,  while \ref{tab:comp-reco-data-time} shows the reconstruction time breakdown.  This event had a 3 ms readout window.  The input size and reconstruction time scale reasonably linearly with the readout window.  

\subsection{Reconstruction characteristics}

The data preparation phase can be segmented by detector component, for example into wire planes within a APA.  The operations performed in signal processing require few decisions to be made but do include operations such as fast-Fourier transforms and deconvolution.  These operations are well suited for GPU and parallel processing. There is active exploration into multi-threading processing for all data preparation algorithms.


Once ROI's have been identified, several 3-D reconstruction packages are used. For the first reconstruction pass in November, the  \dword{pandora}\cite{Acciarri:2017hat}, \dword{wirecell}\cite{wirecell} and \dword{pma}\cite{ref:PMA}  frameworks were used with results described in the Physics volume.   Table \ref{tab:comp-reco-data-time} indicates that they are comparable in terms of CPU time used.   Deep Learning techniques based on image pattern recognition algorithms are also being developed. Many of these algorithms can be adapted to run on HPC's, but probably different architectures that would be optimal for the data preparation phase. 

All of these algorithms are currently being run on conventional unix CPU's using \dword{osg}/\dword{wlcg} grid computing  infrastructure. 



\begin{dunetable}[Compressed size/event for Raw data - 7 GeV beam data with a 3 ms time window]{rrl}{tab:comp-raw-data-size}{Compressed size/event for Raw data - 7 GeV beam data with a 3 ms time window}
  Size in Bytes&Fraction&Data Product Name\\
\hline
44,155.47&0.576&RCE DAQ Fragments\\
27,952.64&0.364&FELIX DAQ Fragments\\
4,586.82&0.06&Photon Detector DAQ Fragments\\
5.72&0&CTB DAQ Fragments\\
0.17&0&DAQ Timing Fragments\\
0.09&0&Trigger Results\\
\hline
76,703.25 & 1.0 & Total\\
\end{dunetable}

\begin{dunetable}
[Compressed size/event for Reconstructed data - 7 GeV beam data]
{rrl}
{tab:comp-reco-data-size}
{Compressed size for Reconstructed data - 7 GeV beam events}
Size in kBytes&Fraction&Data Product Name\\
4,218,536&0.185&recob::Wires\\
2,236,432&0.098&recob::Hits(hitpdune) \\
2,102,520&0.092&recob::Hits(gaushit)\\
2,052,796&0.090&recob::Hits(linecluster)\\
2,020,575&0.089&artdaq::Photon Detector DAQ Fragments\\
1,532,502&0.067&raw::OpDetWaveforms\\
1,018,088&0.045&anab::Calorimetrys(pandoracalo)\\
873,797&0.038&recob::Tracks(pandoraTrack)\\
806,513&0.035&anab::Calorimetrys(pmtrackcalo)\\
555,775&0.024&recob::SpacePoints(pandora)\\
479,599&0.021&recob::Tracks(pmtrack)\\
414,824&0.018&raw::OpDetWaveforms\\
391,791&0.017&recob::Hitrecob::Trackrecob::TrackHitMetaart::Assns\_pmtrack.\\
379,553&0.017&recob::SpacePoints\_pmtrack\_\_DecoderandReco.\\
310,021&0.014&recob::SpacePoints\_reco3d\_pre\_DecoderandReco.\\
260,143&0.011&recob::Hitrecob::SpacePointvoidart::Assns\_hitpdune\\
250,175&0.011&recob::Hitrecob::SpacePointvoidart::Assns\_reco3d\\
229,711&0.01&recob::SpacePoints\_reco3d\_noreg\\
218,874&0.01&recob::SpacePoints\_reco3d\\
200,618&0.009&recob::Hitrecob::SpacePointvoidart::Assns\_pandora\\
2,407,376&0.106&Smaller Objects\\
\hline
22,759,597&1.000&Total\\
\end{dunetable}

\begin{dunetable}[Algorithm timing for 7 GeV beam event]{lrr}{tab:comp-reco-data-time}{Algorithm timing for 7 GeV beam events.  Smaller processes not shown for clarity. A 10 event job used 2.7 GB of memory to do this reconstruction.}
  Processing step&Average CPU time, sec\\
  RootInput(read)&0.2\\
  %decode:timingrawdecoder:TimingRawDecoder&0.0\\
  %decode:ssprawdecoder:SSPRawDecoder&0.3\\
  PDSPTPCRawDecoder&15.9\\
  %decode:crtrawdecoder:CRTRawDecoder&0.0\\
  %decode:ctbrawdecoder:PDSPCTBRawDecoder&0.0\\
  BeamEvent&0.7\\
  DataPrepModule&89.1\\
  Deconvolution&113.3\\
  %decode:digitwire:EventButcher&0.8\\
  GausHitFinder&20.7\\
  SpacePointSolver&18.0\\
  DisambigFromSpacePoints&23.0\\
  LineCluster&3.9\\
  StandardPandora&93.2\\
  LArPandoraTrackCreation&12.3\\
  LArPandoraShowerCreation&5.2\\
  Calorimetry&5.8\\
  %decode:pandorapid:Chi2ParticleID&0.0\\
  PMAlgTrackMaker&142.0\\
  PMCalorimetry&6.1\\
  %decode:pmtrackpid:Chi2ParticleID&0.0\\
  %decode:ophitInternal:OpHitFinder&0.0\\
  %decode:ophitExternal:OpHitFinder&0.0\\
  %decode:opflashInternal:OpFlashFinder&0.0\\
  %decode:opflashExternal:OpFlashFinder&0.0\\
  %[art]:TriggerResults:TriggerResultInserter&0.0\\
  %end_path:out1:RootOutput&0.0\\
  RootOutput(write)&3.3\\
  Total&503.7\\
\end{dunetable}





 




\subsection{Processing Infrastructure for Reconstruction and Simulation}
\label{ch-comp-processing}
DUNE makes use of computing resources internationally through the Open Science Grid and the parallel infrastructure set up for WLCG in Europe.  In 2018, significant effort was put into integrating European sites into the DUNE reconstruction and simulation processing with very positive results.  
Figure \ref{fig:ch-exec-comp-cpupie} shows the distribution of production jobs worldwide in November and December 2018 during the main reconstruction pass.  FNAL and CERN as the host laboratories made the largest contributions but significant resources were also made available from the UK through integration with GridPP and in the Czech republic through FZU and at IN2P3 in France. 

\begin{figure}[htp]
\centering
%\subfloat[]{
%\includegraphics[height=2.5in]{comp-dunepro_pdsp_keepup.png}%
\includegraphics[height=4in]{graphics/comp-vo-summary.png}
%}
%\vspace{1cm}
%\subfloat[]{
%\includegraphics[height=2.3in]{comp-dunepro_mcc11.png}%
%\includegraphics[height=2.3in]{comp-dunepro_mcc11_legend.png}
%}
\caption{CPU wall-time for November 2018 during ProtoDUNE-SP reconstruction showing multiple site contributions.  The major contributions were from FNAL, CERN, many UK institutions and FZU.}
\label{fig:ch-exec-comp-cpupie}
\end{figure}

\begin{dunetable}
[Data storage  and CPU needs for reconstruction of ProtoDUNE test beam data]
{llrrrr}{tab:exec-comp-needs}{Data storage and CPU needs for reconstruction of ProtoDUNE-SP test beam data taken in 2018 and projections for 2019-2021.  We assume two copies of raw data are stored and that each event is reconstructed twice.  Analysis and simulation are estimated to be of order the same CPU use as reconstruction based on the 2018 experience.}%\rowtitlestyle
Detector& value &
2018&
2019&
2020&
2021\\
&&As built\\
\hline
SP&
Events, M&
15.1&
13.0&
6.5&
40.5\\
&
Raw data, TB&
1047&
2239&
1120&
2799\\
&
Reco data, TB&
2094&
4479&
2239&
5599\\
&
CPU, MH&
5.0&
4.3&
2.2&
13.5\\
\hline
DP&
Events, M&
0.0&
101.1&
56.2&
119.9\\
&
Raw data, TB&
0&
809&
449&
1799\\
&
Reco data, TB&
0&
1617&
899&
3598\\
&
CPU, MH&
0.0&
33.7&
18.7&
40.0\\
\hline
total&
Events, M&
15.1&
114.0&
62.6&
160.4\\
&2x
Raw data, TB&
2094&
6096&
3138&
9197\\
&
Reco data, TB&
2094&
6096&
3138&
9197\\
total&Storage, TB
4188&
12193&
6276&
18394\\
&
Reco CPU, MH&
5.0&
38.0&
20.9&
53.5\\
&
Analysis CPU, MH&
5.0&
40.0&
40.0&
40.0\\
Total&CPU, MH&
10.0&
78.0&
60.9&
93.5\\
\end{dunetable}

\subsection{Lessons learned}

\begin{itemize}
    \item Data and simulation challenges led to a reasonably mature and robust model for acquiring, storing and cataloging the main data stream. 
    \item The experiment was able to integrate multiple existing grid sites and make use of substantial opportunistic resources.  This allowed initial processing of data within one month of the end of the run.
    \item Substantial but successful effort went into signal processing. 
    \item Prototype infrastructure was in place for provisioning, authentication and authorization, Data Management, networking, file catalog, and workflow management. 
    \item Reconstruction algorithms are not perfect but sufficient for studies of detector performance and calibration. 
    \item Beam information was successfully integrated into the processing through the \dword{ifbeam} database.
    \item Auxiliary information from, for example, slow controls was not integrated into processing due to lack of personpower.  This led to dependence on hand input of running conditions by shift personnel and offline incorporation of that information into the data catalog. 
\end{itemize}

Overall, the ProtoDUNE-SP data taking and processing was a success but overly dependent on doing things ``by hand'' as automated processes were not always in place. Considerable effort will be needed needed for integration of detector conditions, data migration, workflow systems and integration of HPCs with multi-threaded and vectorized software.

\subsection{Near future}

Table \ref{tab:exec-comp-needs} summarizes the known resource usage for 2018 and projections for 2019-2021.  The collaboration has requested a substantial test beam run for both single and dual phase detectors in 2021.  The \dshort{csc} views this run as a first production test for the full DUNE computing infrastructure. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Model for the Far and Near Detectors}
\label{ch:exec-comp-mod}

%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}
\label{ch:exec-comp-mod-int}
In parallel with the ProtoDUNE-SP data, a joint Data Model task force was formed by the DAQ and Computing Consortia to lay a framework for the full near and far DUNE detectors. 
The Data Model task force grappled with the problems of efficiently triggering, reading out and storing data from an enormous detector on multiple time scales.

They defined major concepts.

\begin{description}

\item{Configuration:} set of parameters that define the persisted, expected detector state. Globally, this corresponds to a desirable state for the detector, capable of providing data of physics or calibration quality. Each component of the detector may have its own configuration.
 
\item{Run:} Period of time over which data has been collected across some set of desired components in a consistent configuration.
 
\item{Subrun:} Period of time within a run over which data has been collected across some set of desired components in a consistent configuration. The set of desired components in a subrun must be a subset of the desired components for a run, and is the set of components over which data is expected.
 
(Time-based rollovers of runs and subruns may be automatic. Differences of subrun and run due to configuration or changes in the desired components will be tracked by the DAQ, and may be either manual or automatic.)
 
\item{Trigger:} data from the desired components in that subrun over a readout window. This would typically be centered around a trigger time, and is what is recorded by the DAQ. The readout window may be subdivided into frames as determined by the DAQ.
 
\item{Event:} subset of a trigger isolated in time and space containing an independent interaction in the detector. Events may overlap in space or time, within the same trigger. This is generally determined by the offline, based on reconstruction of data in a frame.

\end{description}

These definitions are intended to allow triggering, recording and reconstruction of interactions in subsets of the detector. While the whole detector (or time window) can produce enormous amounts of data, any individual interaction is expected to span a reasonably short time and spatial volume. A data model that can isolate individual interactions  allows efficient storage and reconstruction of interactions. 


The main data stream will be augmented by beam, slow controls, \dword{daq} configuration and calibration information. 

This work continues and informs  the  joint  calibration, \dshort{csc} and \dshort{daq} designs.


\section{Consortium Organization}

%The DUNE computing and software consortium was formed in October 2018.  
%%%%%%%%%%%%%%%%%%%%
%\subsection{Requirements}
%\label{ch:exec-comp-mod-req}


%%%%%%%%%%%
%\subsubsection{How Physics Drives}
%\label{ch:exec-comp-mod-req-phys-drv}


%%%%%%%%%%%
%\subsubsection{Oscillation Analyses}
%\label{ch:exec-comp-mod-req-osc}


%%%%%%%%%%%
%\subsubsection{Cross Sections}
%\label{ch:exec-comp-mod-req-xsec}


%%%%%%%%%%%
%\subsubsection{Other Drivers}
%\label{ch:exec-comp-mod-req-oth-drv}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Interfaces with Other Projects}
%\label{ch:exec-comp-mod-intfc}


%%%%%%%%%%%
%\subsubsection{DAQ}
%\label{ch:exec-comp-mod-intfc-daq}


%%%%%%%%%%%
%\subsubsection{Calibration}
%\label{ch:exec-comp-mod-intfc-calib}

%Why is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{Physics}
%\label{ch:exec-comp-mod-intfc-phys}

%Why is this cutting off here on the page?

%%%%%%%%%%%%%%%%%%%%
%\subsection{Use cases}
%\label{ch:exec-comp-mod-use}


%%%%%%%%%%%
%\subsubsection{Discussion of detector options (SP,DP,ND? )}
%\label{ch:exec-comp-mod-use-opt}


%%%%%%%%%%%
%\subsubsection{Data acquisition}
%\label{ch:exec-comp-mod-use-daq}


%%%%%%%%%%%
%\subsubsection{Data Quality}
%\label{ch:exec-comp-mod-use-dq}


%%%%%%%%%%%
%\subsubsection{Reconstruction}
%\label{ch:exec-comp-mod-use-reco}


%%%%%%%%%%%
%\subsubsection{Calibration}
%\label{ch:exec-comp-mod-use-calib}


%%%%%%%%%%%
%\section{Simulation}
%\label{ch:exec-comp-mod-use-sim}


%%%%%%%%%%%
%\section{Analysis}
%\label{ch:exec-comp-mod-use-anls}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Existing Infrastructure}
%\label{ch:exec-comp-mod-infr}


%%%%%%%%%%%
%\subsubsection{sam/enstore/eos/castor}
%\label{ch:exec-comp-mod-infr-stor}


%%%%%%%%%%%
%\subsubsection{Grid}
%\label{ch:exec-comp-mod-infr-gr}


%%%%%%%%%%%
%\subsubsection{Databases}
%\label{ch:exec-comp-mod-infr-db}


%%%%%%%%%%%%%%%%%%%%
%\subsection{ProtoDUNE Experience}
%\label{ch:exec-comp-mod-pdune}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Evolving Infrastructure}
%%\label{ch:exec-comp-mod-evlv}
%Why is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{rucio}
%\label{ch:exec-comp-mod-evlv-ruc}
%Why is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{Load Management}
%\label{ch:exec-comp-mod-evlv-load}


%%%%%%%%%%%
%\subsubsection{?.}
%\label{ch:exec-comp-mod-evlv-}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Novel Architectures}
%\label{ch:exec-comp-mod-nov}

%%%%%%%%%%%
%\subsubsection{HPC}
%\label{ch:exec-comp-mod-nov-hpc}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Authentication}
%\label{ch:exec-comp-mod-auth}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Software}
%\label{ch:exec-comp-sw}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Introduction}
%\label{ch:exec-comp-sw-int}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Existing Packages}
%\label{ch:exec-comp-sw-int-pkg}


%%%%%%%%%%%
%\subsubsection{GEANT4}
%\label{ch:exec-comp-sw-int-gnt}


%%%%%%%%%%%
%\subsubsection{ROOT}
%\label{ch:exec-comp-sw-int-root}
%hy is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{art}
%\label{ch:exec-comp-sw-int-art}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Evolving Packages}
%\label{ch:exec-comp-sw-evpkg}


%%%%%%%%%%%
%\subsubsection{LArSoft}
%\label{ch:exec-comp-sw-evpkg-larsoft}


%%%%%%%%%%%
%\subsubsection{Wirecell}
%\label{ch:exec-comp-sw-evpkg-wcell}


%%%%%%%%%%%
%\subsubsection{GENIE}
%\label{ch:exec-comp-sw-evpkg-genie}


%%%%%%%%%%%%%%%%%%%%
%\subsection{DUNE-specific Software}
%\label{ch:exec-comp-sw-evpkg-spec}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Novel Architectures}
%\label{ch:exec-comp-sw-novarch}

%%%%%%%%%%%
%\subsubsection{Machine Learning}
%\label{ch:exec-comp-sw-novarch-mach}
%Why is this cutting off here on the page?
%%%%%%%%%%%
%\subsubsection{Vectorization}
%\label{ch:exec-comp-sw-novarch-vec}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Development Environment}
%\label{ch:exec-comp-sw-devenv}

%%%%%%%%%%%
%\subsubsection{Environment Specifications}
%\label{ch:exec-comp-sw-devenv-spec}


%%%%%%%%%%%
%\subsubsection{Code and Configuration Management}
%\label{ch:exec-comp-sw-devenv-mgmt}


%%%%%%%%%%%
%\subsubsection{Validation}
%\label{ch:exec-comp-sw-devenv-val}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Training and Communication}
%\label{ch:exec-comp-sw-train}
%Why is this cutting off here on the page?

%%%%%%%%%%%%%%%%%%%%
%\subsection{Lessons from ProtoDUNE}
%\label{ch:exec-comp-sw-lessons}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Resources and Governance}
\label{ch:exec-comp-gov}

The Computing and Software effort is now a DUNE Consortium.  Docdb 12751 \lcite{bib:docdb12751} describes the governance structure for the Consortium.  The Consortium coordinates effort across the collaboration but funding comes from collaborating institutions, laboratories and national funding agencies. 

The consortium has an overall Consortium Leader. The Consortium Leader is responsible for the sub-system deliverables and represents the consortium to the overall DUNE collaboration.

In addition there  are Technical Leads to act as the overall project managers for the consortium. The Technical Leads report to the overall Consortium Leader.
Computing has both a Host Laboratory Technical Project Lead, responsible for coordination with the DUNE Project and host lab and an External Technical Lead responsible for coordination with other entities.
It is anticipated that at least one of the three leadership roles will be held by a non-US scientist. 
As with other DUNE consortia, the consortium is responsible for assigning a provisional division of institutional
responsibilities for computing resources, deliverables and operations, amongst the participating institutions. This division of responsibilities must account for the resources that are likely to be available. The internally agreed division of responsibilities needs to be presented to the Technical Board, which will then make a recommendation to the collaboration management for approval.



\subsection{Scope of the Consortium}
The Computing and Software Consortium (\dword{csc}) is mainly concerned with the infrastructure and resources for offline computing.  Algorithm development resides within the Physics groups while online systems at experimental sites are governed by the Data Acquisition and Cryogenics Instrumentation and Slow Controls Consortia. These groups coordinate closely to assure that the full chain of data acquisition, processing and analysis works. Formal interfaces with these groups are described in Docdb 7123 (DAQ)\lcite{bib:docdb7123} and Docdb 7126 (CISC)\lcite{bib:docdb7126}.

The consortium operates at two levels: at the hardware level, where generic resources can be provided as in kind contributions to the collaboration, and at the human level, where individuals and groups contribute to the development of common software infrastructure. 

\subsection{Hardware}
As noted above, the collaboration has already made use of substantial global resources through the \dword{wlcg} and \dword{osg} grid mechanisms. As the Consortium evolves, institutions and collaborating nations will be asked to make formal pledges of resources (both CPU and storage) and those resources will be accounted for and considered in-kind contributions to the collaboration.
As illustrated above, several international partners are already making substantial contributions. We are currently integrating additional large national facilities. Most resources are currently opportunistic but Fermilab and CERN have committed several thousand cores and several PB of disk and the UK reserves 10\% of GridPP resources for non-LHC experiments, an allocation that DUNE has already benefited from.

\begin{dunetable}
[DUNE S+C Consortium Members as of February 20 19]{lll}{tab:exec-comp-consortium}{DUNE S+C Consortium Members as of February 2019, -- indicates sites not yet integrated into production computing. }%\rowtitlestyle
Institution& Country \\%& Integrated\\
KISTI&Korea\\%&--\\
TIFR  & India \\%& in process \\
Nikhef&NL\\%&Yes\\
Edinburgh&UK\\%&Yes\\
GridPP&UK\\%&\\%Yes \\
Manchester&UK\\%&Yes\\
RAL/STFC&UK\\%&Yes\\
Argonne&USA\\%&--\\
BNL&USA\\%&Yes\\
Cincinnati&USA\\%& Yes\\
Colorado State&USA\\%& Yes\\
CU Boulder&USA\\%&Yes\\
Fermilab&USA\\%& Yes \\
Florida &USA\\%& Yes\\
LBNL&USA\\%&Yes\\
Minnesota&USA\\%&Yes\\
Northern Illinois Univ.\\%&USA& --\\
Notre Dame&USA\\%&Yes\\
Oregon State University&USA\\%&Yes\\
Tennessee&USA\\%&--\\
Texas, Austin&USA\\%&--\\
\end{dunetable}

\subsection{People}

The \ldshort{csc} has (or will have) subgroups for the following areas.  Highlights of some of the ongoing projects are detailed in subsequent sections. 

\begin{itemize}
    \item 
Collaborative Tools
\item Data Storage and Management
\item Databases 
\item Production and Processing 
\item Workflow Management
\item Data Quality Monitoring 
\item Software Release Management 
\item Core Software led by a Software Architect
\item Advanced Architectures
\item Algorithm liaisons
\item Networking
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
\subsection{Cooperative Work with Other Collaborations}
\label{ch:exec-comp-gov-coop}

The HEP computing community has come together to form an HEP Software Foundation (HSF)\lcite{Alves:2017she} which, through working groups, workshops and white-papers is guiding the next generation of shared HEP software.  DUNE's time-scale, where we are in the planning and evaluation phase, is almost perfect for us to contribute to and benefit from these efforts.  Our overall strategy for computing infrastructure is to carefully evaluate existing and proposed field-wide solutions, to participate in their design where they are useful and to only build our own solutions where the common solutions do not fit and additional joint development is not feasible.   This section describes some of these common activities. 



\subsection{\dword{larsoft} for event reconstruction}

The \dword{larsoft} \lcite{Snider:2017wjd} reconstruction package is shared by a collaboration of LAr neutrino experiments.  MicroBooNE, SBND, DUNE and others share in the development of a common core software framework with customization for each experiment. The existence of this software suite and prior efforts by other experiments is what made the rapid reconstruction of the ProtoDUNE-SP data possible.  DUNE will be a major contributor to  the future evolution of this package, in particular in introducing full multi-threading to allow parallel reconstruction of parts of large events in anticipation of the very large events expected from the full detector. 



\subsection{Relation to WLCG/OSG}
The Worldwide LHC Computing Grid organization (\dshort{wlcg})\lcite{Bird:2014ctt}, which currently combines the resource and infrastructure missions for the LHC experiments, has proposed a future governance structure that splits the dedicated resource provision for LHC experiments from the general middleware infrastructure used to access those resources.  This Scientific Computing Infrastructure (\dshort{sci}) is already used by many other experiments worldwide.  In a white paper submitted to the European Strategy Group in December 2018\lcite{bib:BirdEUStrategy}, a formal Scientific Computing Infrastructure organization is proposed. As part of the transition to that structure the DUNE collaboration has been provisionally invited to join the WLCG with observer status and participate in the Grid Management Board. The goal of our participation is to have input into the technical decisions on global computing infrastructure while contributing to that infrastructure. 

Areas of collaboration are described in the following subsections. 



\subsubsection{Rucio Development and Extension}

 \dword{rucio}
\cite{Barisits:2019fyl}
is a data management system originally developed by the ATLAS collaboration and now an open-source project.  DUNE has chosen to use \dword{rucio} for our large scale data movement.  In the short term it is being combined with the \dword{sam} data catalog used by Fermilab experiments.  DUNE collaborators at FNAL and in the UK are actively collaborating in the \dword{rucio} project, adding value for DUNE but also the wider effort.


There is a global \dword{rucio} team which now includes Fermilab and BNL staff, DUNE and CMS collaborators,  in addition to the core developers on ATLAS who wrote it initially.  Consortium members have started collaborative work on several projects.   These include (a) making object stores (such as Amazon S3 and compatible utilities) work with Rucio.  There is a large object store in the UK on which DUNE has a sizable allocation.  (b) Monitoring  and administration of the \dword{rucio} system, leveraging the Landscape system at Fermilab.  (c) Designing a  data description engine that can be used as a replacement for the SAM system we currently use.

Rucio has already been shown to be a powerful and useful tool for moving defined datasets from point A to point B.  Our initial observation is that \dword{rucio} is a good solution for file localization but is missing the detailed tools for data description and granular dataset definition available in the current \dword{sam} system.  The rapidly varying conditions in the test beam have highlighted a need for a sophisticated data description database interfaced to \dword{rucio}'s location functions. 

In addition,   LHC experiments such as ATLAS and CMS work with disk stores and tape stores that are independent of each other.  This is different than the dCache model used by most Fermilab experiments in which most of dCache is a caching frontend for a tape store.  Efficient integration of caching into the \dword{rucio} model will be an important component for DUNE unless  we can afford to have most data on disk to avoid staging.

\subsubsection{Testing of New Storage Technologies and Interfaces}

There is currently a Data Organization, Management, and Access (\dword{doma}) taskforce working in the larger HEP community\lcite{Berzano:2018xaa}
 in which several DUNE collaborators are involved. There are task forces for authorization, caching, third party copy, hierarchical storage, and quality of service. All of these are of interest to DUNE as they will determine the long-term standards for common computing infrastructure in the field. 
In particular, the authorization issues have significant impact on DUNE and are covered in subsection \ref{ch-comp-auth}.


\subsubsection{Data Management and Retention Policy Development}



There is a data life cycle built into the DUNE data model.  Obsolete samples such as old simulations and histograms and old commissioning data do not have to be maintained indefinitely.  
We are organizing the structure of lower storage so that the various retention types are stored separately for easy deletion when necessary.  

\subsubsection{Authentication and Authorization Security and Interoperability}\label{ch-comp-auth}

Within the next 2-3 years we expect the global HEP community to make significant changes in the methods of authentication and authorization of computing and storage. 
Over that period, DUNE needs to collaborate with the US and European HEP computing communities on improved authentication methods  that will allow secure but transparent access to storage and other resources such as logbooks and code repositories.  The current model where individuals need to authenticate through different mechanisms for access to US and European resources is already a roadblock to efficient integration  of personnel and storage. 
Current efforts to expand the trust realm between CERN and Fermilab should allow single sign on for each to provide access to the other lab.





%\todo{\verbatim{Add reference to http://wlcg-docs.web.cern.ch/wlcg-docs/technical_documents/HEP-Computing-Evolution.pdf}}



\subsubsection{Evaluations of other important infrastructure}

The DUNE S+C effort is still evaluating major infrastructure components, notably databases and workflow management systems.

For databases\cite{Laycock:2019ynk}, the Fermilab Conditions Database is being used for the first run of ProtoDUNE but the Belle II\cite{Ritter:2018jxh} system supported by BNL is also being considered for subsequent runs. 

For workflow management, we are evaluating \dword{dirac}\cite{Falabella:2016waj} and plan to investigate PANDA \cite{Megino:2017ywl} in comparison with the current GlideInWMS, HT Condor, and POMS solution that was successfully used for the 2018 ProtoDUNE campaigns.
Both of \dword{dirac} and PANDA are used by multiple LHC and non-LHC experiments and are already being integrated with \dword{rucio}. 


\section{Conclusion}

The DUNE Software and Computing efforts have already undergone a substantial test with the successful run of ProtoDUNE-SP, including demonstration of data movement to storage at 2GB/s, reconstruction with high quality algorithms of the full test beam sample and the start of analysis of the multiple PB of reconstructed data. 

The Consortium is now working with the global HEP computing community to evaluate and specify modern infrastructure that will serve the needs of DUNE and the rest of the community.  We plan to collaborate wherever possible with other experiments where we have common technical challenges. However, the extremely large but simple events generated by Liquid Argon TPC's, even with short readouts, present a unique challenge. 

Over the next two years our major activities  will be  thorough reviews of available and potential tools, continued growth of collaborations and acquisition of the resources necessary to launch this large suite of ambitious projects. 
%%%%%%%%%%%%%%%%%%%%
%\subsection{Resource Needs}
%\label{ch:exec-comp-gov-res}

%%%%%%%%%%%
%\subsubsection{Hardware}
%\label{ch:exec-comp-gov-res-hw}

%%%%%%%%%%%
%\subsubsection{Personnel}
%\label{ch:exec-comp-gov-res-hum}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Contribution Models}
%\label{ch:exec-comp-gov-contrib}
%Why is this cutting off here on the page?

%%%%%%%%%%%%%%%%%%%%
%\subsection{Technical Decision Governance}
%\label{ch:exec-comp-gov-tech}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Resource Decision Governance}
%\label{ch:exec-comp-gov-resdec}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Project Management}
%\label{ch:exec-comp-gov-pm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  REMOVE THESE %%%%%%%%%%%%%
%\printglossaries
%\end{document}

%\ignore{
%  REFERENCES 
%
%@article{Snider:2017wjd,
%      author         = ``Snider, E. L. and Petrillo, G.'',
%      title          = ``{LArSoft: Toolkit for Simulation, Reconstruction and
%                        Analysis of Liquid Argon TPC Neutrino Detectors}'',
%      booktitle      = ``{Proceedings, 22nd International Conference on Computing
%                        in High Energy and Nuclear Physics (CHEP2016): San
%                        Francisco, CA, October 14-16, 2016}'',
%      journal        = ``J. Phys. Conf. Ser.'',
%      volume         = ``898'',
%      year           = ``2017'',
%      number         = ``4'',
%      pages          = ``042057'',
%      doi            = ``10.1088/1742-6596/898/4/042057'',
%      reportNumber   = ``FERMILAB-CONF-17-052-CD'',
%      SLACcitation   = ``%%CITATION = 00462,898,042057;%%''
%}
%
%REFERENCES
%
%@article{Snider:2017wjd,
%      author         = ``Snider, E. L. and Petrillo, G.'',
%      title          = ``{LArSoft: Toolkit for Simulation, Reconstruction and
%                        Analysis of Liquid Argon TPC Neutrino Detectors}'',
%      booktitle      = ``{Proceedings, 22nd International Conference on Computing
%                        in High Energy and Nuclear Physics (CHEP2016): San
%                        Francisco, CA, October 14-16, 2016}'',
%      journal        = ``J. Phys. Conf. Ser.'',
%      volume         = ``898'',
%      year           = ``2017'',
%      number         = ``4'',
%      pages          = ``042057'',
%      doi            = ``10.1088/1742-6596/898/4/042057'',
%      reportNumber   = ``FERMILAB-CONF-17-052-CD'',
%      SLACcitation   = ``%%CITATION = 00462,898,042057;%%''
%}
%DQM
%
%https://docs.dunescience.org/cgi-bin/private/ShowDocument?docid=10551
%
%DOMA
%
%@article{Berzano:2018xaa,
%      author         = ``Berzano, Dario and others'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group -- Data Organization, Management and Access (DOMA)}'',
%      year           = ``2018'',
%      eprint         = ``1812.00761'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-04, FERMILAB-PUB-18-671-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1812.00761;%%''
%}
%
%
%%%% contains utf-8, see: https://inspirehep.net/info/faq/general#utf8
%%%% add \usepackage[utf8]{inputenc} to your latex preamble
%
%@article{Laycock:2019ynk,
%      author         = ``Bracko, Marko and Clemencic, Marco and Dykstra, Dave and
%                        Formica, Andrea and Govi, Giacomo and Jouvin, Michel and
%                        Lange, David and Laycock, Paul and Wood, Lynn'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group Ð Conditions Data}'',
%      year           = ``2019'',
%      eprint         = ``1901.05429'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``FERMILAB-PUB-19-044-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1901.05429;%%''
%}
%
%@article{Calafiura:2018rwe,
%      author         = ``Calafiura, Paolo and others'',
%      editor         = ``Hegner, Benedikt and Kowalkowski, Jim and Sexton-Kennedy,
%                        Elizabeth'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group - Data Processing Frameworks}'',
%      year           = ``2018'',
%      eprint         = ``1812.07861'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-08, FERMILAB-PUB-18-693-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1812.07861;%%''
%}
%
%@article{Berzano:2018xaa,
%      author         = ``Berzano, Dario and others'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group -- Data Organization, Management and Access (DOMA)}'',
%      year           = ``2018'',
%      eprint         = ``1812.00761'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-04, FERMILAB-PUB-18-671-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1812.00761;%%''
%}
%
%
%%%% contains utf-8, see: https://inspirehep.net/info/faq/general#utf8
%%%% add \usepackage[utf8]{inputenc} to your latex preamble
%
%@article{Bellis:2018hej,
%      author         = ``Bellis, Matthew and others'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group Ð Visualization}'',
%      year           = ``2018'',
%      eprint         = ``1811.10309'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-15, FERMILAB-PUB-18-710-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1811.10309;%%''
%}
%
%@article{Hildreth:2018tsn,
%      author         = ``Hildreth, M. D. and others'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group - Data and Software Preservation to Enable Reuse}'',
%      year           = ``2018'',
%      eprint         = ``1810.01191'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-06, FERMILAB-FN-1060-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1810.01191;%%''
%}
%
%@article{Albertsson:2018maf,
%      author         = ``Albertsson, Kim and others'',
%      title          = ``{Machine Learning in High Energy Physics Community White
%                        Paper}'',
%      booktitle      = ``{Proceedings, 18th International Workshop on Advanced
%                        Computing and Analysis Techniques in Physics Research
%                        (ACAT 2017): Seattle, WA, USA, August 21-25, 2017}'',
%      journal        = ``J. Phys. Conf. Ser.'',
%      volume         = ``1085'',
%      year           = ``2018'',
%      number         = ``2'',
%      pages          = ``022008'',
%      doi            = ``10.1088/1742-6596/1085/2/022008'',
%      eprint         = ``1807.02876'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``FERMILAB-PUB-18-318-CD-DI-PPD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1807.02876;%%''
%}
%
%@article{Berzano:2018krv,
%      author         = ``Berzano, Dario and others'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group - Training, Staffing and Careers}'',
%      collaboration  = ``HEP Software Foundation'',
%      year           = ``2018'',
%      eprint         = ``1807.02875'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.ed-ph'',
%      reportNumber   = ``HSF-CWP-2017-02'',
%      SLACcitation   = ``%%CITATION = ARXIV:1807.02875;%%''
%}
%
%@article{Bauerdick:2018qjx,
%      author         = ``Bauerdick, Lothar and others'',
%      editor         = ``Neubauer, Mark S.'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group - Data Analysis and Interpretation}'',
%      collaboration  = ``HEP Software Foundation'',
%      year           = ``2018'',
%      eprint         = ``1804.03983'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-05, FERMILAB-FN-1057-CD-PPD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1804.03983;%%''
%}
%
%@article{Apostolakis:2018ieg,
%      author         = ``Apostolakis, J and others'',
%      editor         = ``Elvira, V and Harvey, J'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group - Detector Simulation}'',
%      collaboration  = ``HEP Software Foundation'',
%      year           = ``2018'',
%      eprint         = ``1803.04165'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-07, FERMILAB-FN-1054-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1803.04165;%%''
%}
%
%@article{Albrecht:2018iur,
%      author         = ``Albrecht, Johannes and others'',
%      title          = ``{HEP Community White Paper on Software Trigger and Event
%                        Reconstruction}'',
%      year           = ``2018'',
%      eprint         = ``1802.08638'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``FERMILAB-PUB-18-071-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1802.08638;%%''
%}
%
%@article{Albrecht:2018zgl,
%      author         = ``Albrecht, Johannes and others'',
%      title          = ``{HEP Community White Paper on Software Trigger and Event
%                        Reconstruction: Executive Summary}'',
%      year           = ``2018'',
%      eprint         = ``1802.08640'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``FERMILAB-PUB-18-072-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1802.08640;%%''
%}
%
%@article{Couturier:2017cgq,
%      author         = ``Couturier, Benjamin and others'',
%      title          = ``{HEP Software Foundation Community White Paper Working
%                        Group - Software Development, Deployment and Validation}'',
%      year           = ``2017'',
%      eprint         = ``1712.07959'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-13'',
%      SLACcitation   = ``%%CITATION = ARXIV:1712.07959;%%''
%}
%
%@article{Alves:2017she,
%      author         = ``Albrecht, Johannes and others'',
%      title          = ``{A Roadmap for HEP Software and Computing R\&D for the
%                        2020s}'',
%      collaboration  = ``HEP Software Foundation'',
%      journal        = ``Comput. Softw. Big Sci.'',
%      volume         = ``3'',
%      year           = ``2019'',
%      number         = ``1'',
%      pages          = ``7'',
%      doi            = ``10.1007/s41781-018-0018-8'',
%      eprint         = ``1712.06982'',
%      archivePrefix  = ``arXiv'',
%      primaryClass   = ``physics.comp-ph'',
%      reportNumber   = ``HSF-CWP-2017-01, HSF-CWP-2017-001,
%                        FERMILAB-PUB-17-607-CD'',
%      SLACcitation   = ``%%CITATION = ARXIV:1712.06982;%%''
%}
%
%
%
%
%
%
%
%
%
%
%
%
%}% end ignore
