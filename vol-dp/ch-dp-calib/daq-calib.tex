%\section{DAQ Requirements}
%\label{sec:sp-calib-daqreq}
%\fixme{JM/SG made edits. Josh K. to verify and sign off, especially the updated rates and assumptions there.}

The calibration systems must interface with the \dword{dune} \dword{daq} system, discussed in detail in Chapter~\ref{ch:dp-daq}. 
%%KMTDRREADME: This was directly repetitive with next paragraph
%Trigger decisions for physics events are done hierarchically: trigger primitives (TPs) are generated from\dword{crp} and \dword{pds} ``hits'', and these TPs are then used to create Trigger Candidates which are collections of TPs satisfying selection criteria such as exceeding a threshold number of adjacent \dword{crp} hits, or total  charge recorded, etc. These Trigger Candidates are passed on to a Module Level Trigger which then makes decisions about whether a given Trigger Candidate is accepted as a detector-wide trigger.  If so, the Module Level Trigger sends trigger commands to the Data Flow Orchestrator (DFO) which in turn passes them to an available Event Builder that then requests data from the Front-End Readout of the DAQ (servers that host FELIX cards). The management of trigger decisions---whether they are generated by candidates from the \dword{tpc}, \dword{pds}, calibrations, or other systems---is done in the \dword{mlt}.
%\fixme{add def to MLT in glossary}

Trigger decisions for physics events are made %done 
hierarchically: \dwords{trigprimitive} are generated from %\dword{tpc} 
\dword{crp} and \dword{pds} hits, and these \dwords{trigprimitive} are then used to create \dwords{trigcandidate}, which are collections of \dwords{trigprimitive} satisfying selection criteria such as exceeding a threshold number of adjacent 
%collection wire 
\dword{crp} hits, total 
%collection wire 
charge recorded, as well as other criteria. These \dwords{trigcandidate} are passed on to a \dword{mlt}, which then makes decisions about whether a given \dword{trigcandidate} is accepted as a detector-wide trigger.  If so, the \dword{mlt} sends trigger commands to the \dword{daqdfo}, which in turn passes them to an available \dword{eb} that then requests data from the \dword{fe} readout of the \dword{daq} (servers that host \dword{felix} cards). Managing \dwords{trigdecision}, whether they are generated by candidates from the \dword{tpc}, \dword{pds}, calibrations, or other systems, is done in the \dword{mlt}. 

The trigger commands are in the form of absolute time stamps that are used to extract snapshots of the data stored in the \dword{fe} readout buffers. For physics triggers, all \dword{tpc} information for a snapshot of time
%(roughly twice the drift time, or \SI{5.4}{\milli\s})
(\SI{16.4}{\milli\s}) %\todo{SG: readout time updated to 16.4 ms for DP. JK to sign off} 
are read out, without any additional zero suppression or localization. For calibration events, this approach would create an unmanageable amount of data and, in any case, is unnecessary because calibration events create interactions or tracks at known positions or times or both.

%The primary interface with calibrations will be through the DUNE timing system, which is responsible for synchronization across all subsystems and for providing absolute time stamps t. 

    
    To reduce data volume from calibrations, therefore, calibration systems that can be triggered externally are desirable. Like the distribution of trigger commands to the \dword{fe} readout buffers, the external trigger for a calibration system will take the form of an absolute time stamp. The time stamp is generated by the \dword{mlt}, thus ensuring that (for example) a calibration event does not occur during a candidate supernova burst.  These time stamps are distributed through the \dword{daq}'s timing and synchronization system. Thus, triggerable calibration systems (like the laser or \dword{pns}) must be synchronized to the rest of the \dword{daq} system and be capable of accepting time stamps.

There will be differences in the details of how different calibration systems are handled, as discussed below. \fixme{Tables (and figures) must be discussed in the text. This table is not discussed anywhere that I can see.}
           
\begin{dunetable}
[Calibration DAQ summary]
{p{0.2\textwidth}p{0.15\textwidth}p{0.5\textwidth}}
{tab:calib-daq}
{Estimated \dword{daq} rates per year per \SI{10}{\kt} for various calibration systems.}   
System & Uncompressed Data Volume (TB/year) & Assumptions  \\ \toprowrule
Ionization Laser System & \num{37} & \num{400}k laser pulses, \num{10}$\times$\num{10}$\times$\SI{10}{\cubic\cm} voxel sizes, a \SI{100}{\micro\s} zero suppression window (lossy readout), \num{2} times/year  \\ \colhline
Neutron Source System & \num{102} & \num{e6}~neutrons/pulse, \num{1000} neutron captures/m$^{3}$, \num{1300} observed neutron captures per pulse, \num{6}~times/year  \\ \colhline
%Proposed Radioactive Source System & \num{200} & Source rate < \SI{10}{\hertz}; single \dword{apa} readout,  lossless readout; \num{4} times/year   \\ \colhline
\end{dunetable}           
           
\subsubsection{Laser System}

%The proposed laser source is the only practical way to unambiguously measure the electric field vectors within the detector. 
The \efield vector from ionization laser calibration is determined by looking at the deflection of crossing laser tracks within detector voxels. The voxels are currently estimated at \num{10}$\times$\num{10}$\times$\SI{10}{\cubic\cm}. Because any given laser track
illuminates many such voxels, one laser pulse can be used for several
measurements; essentially, the number that matters is the area of each voxel.
The number of total laser events are estimated to be \num{400000}
%: about half the rate of cosmic rays 
and thus nominally produces a substantial total volume of data .

Fortunately, unlike every other event type in the detector, the laser track has both a reasonably well known position and time; thus the trigger command issued to the \dword{fe} buffers can be much narrower than the window used for physics triggers. A \SI{100}{\micro\s} zero suppression window should be wide enough to avoid windowing problems in the 
%induction plane wire \todo{SG:this sentence needs to be updated for DP}
signal deconvolution process. To ensure that the interesting part of each waveform is recorded, the \dword{daq} must know the current position  of the laser, which will be transmitted from the laser system to the \dword{mlt} via the \dword{daqccm}.

%\fixme{This estimate is dependent of assumptions that still need to be checked, namely that a zero suppression window \SI{100}{\micro\s} is still possible in DP, that the number of bytes per sample and the sampling rate are the same as in SP.} 

From the standpoint of data volume, therefore, the total, assuming the \SI{100}{\micro\s} zero-suppression window, is

%SP
%\begin{equation}
%\num{800000}/{\rm scan}/\SI{10}{\kt} \times \SI{100}{\micro\s} \times \num{1.5}{\rm Bytes/sample}\times \SI{2}{\mega\hertz}\times \num{384000}~{\rm channels}   = \num{92}~{\rm TB/scan/\SI{10}{\kt}.}   
%\end{equation}

%(\dpnumcrpch = 153 600 instead of \spnumch
% Scaling 92 * 400k/800k *153600/384k = 92*0.2 = 18.4
\begin{equation}
\num{400000}/{\rm scan}/\SI{10}{\kt} \times \SI{100}{\micro\s} \times \num{1.5}{\rm Bytes/sample}\times \SI{2}{\mega\hertz}\times \dpnumcrpch~{\rm channels}   = \num{18.4}~{\rm TB/scan/\SI{10}{\kt}.}   
\end{equation}


If such a calibration scan were done twice a year, then the total annual data volume for the laser is approximately \num{37}~TB/year/\SI{10}{\kt}.

\subsubsection{Pulsed Neutron Source}
%\todo{SG: JW to update this text and the number in the DAQ table 1.3.}
%There are two radioactive sources suggested to provide low-energy calibration data for DUNE: a neutron generator source, and a $\gamma$ source. 

The \dword{pns} system creates a burst of neutrons that
%, because of the interesting neutron cross section of argon, 
are captured throughout a large fraction of the total cryostat volume. For triggering and data volume, this is convenient: the existing scheme of taking \SI{16.4}{\milli\s} of data for each trigger means all these neutrons will be collected in a single \dword{dune} event.%\todo{SG: this requires updating since DP readout is not 5.4 ms} 
Thus, the data volume is simply \num{6.22}~GB times the total number of such pulses, but these are likely to be few: a single burst can produce thousands of neutrons whose $t_0$ is known up to the neutron capture time of \SI{200}{\micro\s} or so.

To trigger the \dword{pns}, the \dword{mlt} will provide a time stamp for the source to fire and then send a trigger command to the \dword{fe} readout buffers (via the \dword{daqdfo} and \dword{eb}) that will look like a physics trigger command.  The \dword{mlt} itself then tags that trigger command with the expected trigger type (in this case, \dword{pns}).

Typically, a commercial $DD$ neutron generator produces \num{e5} - \num{e8} neutrons/pulse, depending on the adjustable pulse width. The current assumption for neutron yield from the $DD$ generator is \num{e6} neutrons per pulse\footnote{Ideal assumption based on $DD$ generators that produce the highest neutron yield with a pulse width less than \SI{100}{\micro\s}. Such $DD$ generators are being developed in laboratories; commercial devices may require further development to reach this level of performance.}. With the current deployment designs in Figure~\ref{fig:PNS_Two_Designs}, approximately \num{1300} neutron captures per $DD$ generator pulse should be observed inside a \SI{10}{\kt} module. The suggested number for localized energy calibration is \num{1000} neutron captures per \si{\cubic\m}, so a total number of \num{4600} pulses would be needed to calibrate a \SI{10}{\kt} module. Assuming two identical \dwords{pns} operating in synchronization mode, \num{2300} pulses are needed for each calibration run. Therefore, the total data volume per run would be

%SP
%\begin{equation}
%\num{2300}~{\rm Pulses} \times \num{1.5}~{\rm Bytes}\times
%\SI{2}{\mega\hertz}\times \SI{5.4}{\milli\s}\times \num{384000}~{\rm channels} = %\num{14}~{\rm TB/run}.
%\end{equation}

%scale by 14 * 16.4/5.4 * 153600/384000 = 17
\begin{equation}
\num{2300}~{\rm Pulses} \times \num{1.5}~{\rm Bytes}\times
\SI{2}{\mega\hertz}\times \SI{16.4}{\milli\s}\times \dpnumcrpch~{\rm channels} = \num{17}~{\rm TB/run}.
\end{equation}

Running the \dword{pns} calibration system every two months would result in a total data volume of \num{102}~TB per \SI{10}{\kt} per year and running \num{12} times/year would result in \num{204}~TB/year per \SI{10}{\kt}. 

%\subsubsection{Proposed Radioactive Source System}

%The radioactive source will not be triggerable by the Module Level Trigger.  Rather, it will deliver a tag to the Module Level Trigger and that tag will include a time stamp that can be used by the Module Level Trigger to issue a trigger command to the Front-End Readout.  The trigger command will have a standard readout window size of \SI{5.4}{\milli\s}, but to keep data rates manageable, the command will only be send to Front-End Readout buffers that are expected to be illuminated by the source. The localization of trigger commands thus reduces the data volume by \num{150}, if only one \dword{apa} is read out.

 %Nevertheless, if the rate of such a source is anywhere close to one per \SI{5.4}{\milli\s}, the detector would be running  continuously in the current scheme. Therefore we assume that theinteraction rate in the detector is \SI{10}{\hertz} or less. The tag from the source will likely be much higher than this, because not all $\gamma$s interact in the active \dword{tpc} volume. Thus the radioactive source trigger will be a coincidence in the Module-Level Trigger between a low-energy trigger candidate from the illuminated \dword{apa}, and a source tag with a relevant time stamp.  With this rate, and with localization of events to one \dword{apa}, the total data volume would be

%\begin{equation}
%\num{8}~{\rm hours} \times \num{4}~{\rm FTs} \times %\SI{10}{\hertz} \times \num{1.5}~{\rm Bytes}\times %\SI{2}{\mega\hertz}\times \SI{5.4}{\milli\s}\times \num{2560}~{\rm %channels} = \num{50}~{\rm TB/scan}.
%\end{equation}

%Running this calibration four times/year would yield \num{200}~TB of data in \SI{10}{\kt} per year.


\begin{comment}
%SG: This is not under the scope of this chapter. Needs to be moved to physics. 
\fixme{I did go ahead and edit this, but apparently it goes elsewhere in the manuscript.}
\subsubsection{Intrinsic Radioactivity}

Mike Mooney has suggested using the intrinsic \Ar39 as a calibration source. This has many advantages over either of the radioactive
source calibrations, in particular the known level of \Ar39, its uniform distribution in the detector, and the fact that it is always there and therefore integrates correctly over the detector lifetime. The difficulty is
that, because any individual \Ar39 event's $x$ position is not known
because there is no $t_0$, the distribution of these events must be used to
make measurements, thus requiring fairly high statistics.

Mooney's proposal is that roughly \num{250000} \Ar39 can provide a \SI{1}{\%} measurement of electron lifetime. (Note that \SI{1}{\%} is a reasonable goal;
if the lifetime and maximum drift time are the same, this results in a \SI{2}{\%} uncertainty on energy scale, which would begin to compromise \dword{dune}'s physics program). This number of events is easily obtained with the existing random triggers as well as every other trigger source excluding laser pulses and front-end calibrations.

Like all other parameters that must be calibrated, however, what is not
clear is what the spatial and temporal variations will be in the detector.
Other \dword{lar} \dword{tpc}s have performed lifetime calibrations daily (using cosmic rays
primarily), and a pixelization of \SI{1}{\square\m} is not unreasonable, leading to a
need for \num{250000} events for every \si{\square\m} in the detector each day, or about a \SI{1}{\hertz} trigger rate.

In the existing scheme, this would be overwhelmingly the dominant source of data. Thus either the pixelization must be reduced (say, to each of the \dword{tpc} volumes) or a zero-suppression scheme must be used.
Such a zero-suppression scheme would happen post-trigger (for example, running
random triggers at \SI{1}{\hertz} and based upon that trigger type, zero suppressing
signals). In the current scheme, this would happen in the event builder, but at \SI{1}{\hertz}, the data rate would be too high. To do zero suppression upstream (perhaps in the \dword{apa}-level readout) based on the trigger type will likely require more
hardware resources.
\end{comment}

%\subsection{External Muon Tracker}

%        An External Muon Tracker (EMT) has also been proposed, likely as a scintillator-bar telescope at the front face of the detector. The EMT would be intended to trigger on rock muons and provide a known entry position and direction for these. It is thus the only way to test reconstruction in the DUNE FD for a sample of events in the same energy regime as the beam events.

%        Because the EMT is measuring events that will already be triggered by the \dword{tpc}, the additional data volume comes only from the scintillator counters themselves. Because the only information needed for these events is the time of a hit in each counter, and because only four counters are likely to be hit by each muon (two planes of $x$ and $y$), the additional data rate from the EMT is very small.  If we limit ourselves to just the rock muons and assume that four counters are hit resulting in 4 12-bit words/counter (one charge and one time each, plus the counter ID and a local timestamp, then we get a yearly total data volume of
%\begin{equation}
%735{\rm year/10 ktonne} \times 24~{\rm B/event} = 17.6~{\rm kB/year}   
%\end{equation}